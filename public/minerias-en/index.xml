<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Mining | Valentin Barriere</title>
    <link>http://localhost:1313/minerias-en/</link>
      <atom:link href="http://localhost:1313/minerias-en/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Mining</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu6033596820838205990.png</url>
      <title>Data Mining</title>
      <link>http://localhost:1313/minerias-en/</link>
    </image>
    
    <item>
      <title>Intro Aprendizaje Supervisado</title>
      <link>http://localhost:1313/minerias-en/1_intro/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias-en/1_intro/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-heredm_intro_slpdf&#34;&gt;The slides are available &lt;a href=&#34;DM_Intro_SL.pdf&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;p&gt;This class is pretty cool as you will discover the basics of the knowledge used to build Machine Learning, Deep Learning and Artifical Intelligence in general: Supervised Learning! This is a simple setting where a &lt;strong&gt;model will learn its own parameters using examples and associated labels&lt;/strong&gt;. We will revise all the important concepts of supervised learning, which are very useful for anybody who wants to become data scientist.&lt;/p&gt;
&lt;p&gt;Esta clase es bastante bacan ya que descubriran las bases del conocimiento utilizado para construir Machine Learning, Deep Learning e Inteligencia Artificial en general: Aprendizaje Supervisado! Se trata de un entorno sencillo en el que un &lt;strong&gt;modelo aprenderá sus propios parámetros utilizando ejemplos y etiquetas asociadas&lt;/strong&gt;. Revisaremos todos los conceptos importantes del aprendizaje supervisado, que son muy útiles para cualquiera que quiera trabajar como data scientist.&lt;/p&gt;
&lt;h2 id=&#34;generalidades&#34;&gt;Generalidades&lt;/h2&gt;
&lt;p&gt;El aprendizaje supervisado utiliza datos y etiquetas para aprender a un modelo a reconocer padrones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es necesario de transformar los documentos en vectores, para poder hacer la optimizacion&lt;/li&gt;
&lt;li&gt;El modelo va a apprender sus parametros sobre un conjunto de entrenamiento&lt;/li&gt;
&lt;li&gt;El modelo entrenado puede ser usado para predicir la etiquetas de nuevos datos que nunca ha visto antes&lt;/li&gt;
&lt;li&gt;El documento puede ser cualquier dato: audio, texto, imagen, video, usuario, red,&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/predictive_modeling_data_flow.png&#34; alt=&#34;predictive_modeling_data_flow&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;features-y-etiquetas&#34;&gt;Features y etiquetas&lt;/h3&gt;
&lt;p&gt;Se puede representar en un espacio los documentos como vectores. Aca cada punto es una cancion, que esta representando con su intensidad y tempo promedios. La etiqueta es la color del punto. Aca tenemos una tarea de &lt;strong&gt;clasificacion binaria de musica&lt;/strong&gt;, sengundo las preferencias de un usuario.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/classif.jpg&#34; alt=&#34;classif&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del juego, va a ser de encontrar &lt;strong&gt;una funcion que separa el espacio en dos partes&lt;/strong&gt;. Una donde hay las canciones que le gustan a la persona, y la otra parte que no le gustan. De este manera, cuando vamos a tener un nuevo punto en este espacio, podemos decir si la persona va a gustar o no esta cancion, lo que sea &lt;strong&gt;predicir su etiqueta&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;en-resumen&#34;&gt;En resumen&lt;/h3&gt;
&lt;p&gt;Se necesitan varias cosas para un entrenamiento&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tener datos etiquetados&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Conjunto de datos de tamaño $n$
, $\mathcal{D}_n = \{(\text{Doc}_i, Y_i), i=1..n\}$
&lt;/li&gt;
&lt;li&gt;$\text{Doc}$ es una muestra (por ejemplo: una persona)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Extraer los descriptores&lt;/strong&gt; = transformar documentos en vectores&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{X}$ es un vector de observaciones (por ejemplo: edad, sexo, salario)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Crear un modelo matemático $f_\theta$&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Modelo $f_\theta$ tal que  $f_\theta(\mathbf{X})$
 esté cerca de $Y$ (para regresión)&lt;/li&gt;
&lt;li&gt;$\theta$ es el conjunto de parámetros del modelo matemático&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Implementar una función de costo (error) $\ell$
 a minimizar&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Cuanto más se equivoque el modelo, mayor será el costo&lt;/li&gt;
&lt;li&gt;En general, se desea tener un costo pequeño&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Encontrar los parámetros  $\theta^*$ 
 de manera que  $\ell(f_{\theta^*}(\mathbf{X}_i), Y_i)$ 
 sea pequeño&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
$$\theta^* = \underset{\theta}{\arg\min}\sum_{i}\ell(f_{\theta}(\mathbf{X}_i), Y_i)$$&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Probar $f_{\theta^*}$
 en nuevos datos con una métrica de evaluación adecuada&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;aprendizaje&#34;&gt;Aprendizaje&lt;/h2&gt;
&lt;p&gt;Hay varios conceptos en el aprendizaje:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Datos etiquetados&lt;/strong&gt;: Regresión o Clasificación&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extracción de características&lt;/strong&gt;: Tono, Intensidad, Tempo o Edad, Salario, Género, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelo $f_\theta$&lt;/strong&gt;: SVM, Regresión Logística, Bosque Aleatorio, CNN&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Función de costo&lt;/strong&gt; a optimizar: Pérdida de Bisagra, Pérdida de Entropía Cruzada, Pérdida Logística, Pérdida Cuadrada, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algoritmo de optimización&lt;/strong&gt;: Adam, SGD, BFGS, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Métrica de evaluación&lt;/strong&gt;: Recall, Precisión, Mínimos Cuadrados, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funcion-de-costo&#34;&gt;Funcion de costo&lt;/h3&gt;
&lt;p&gt;Para cuantificar las errores del modelo en la optimizacion, se necesita una funcion de costo que llamamos  $\ell$ 
. Ella representa si el modelo esta dando las buenas respuestas $y$
 segundo una entrada $X$
. Este funcion penaliza el modelo cuando comete errores, y lo que queremos hacer es optimizar los pesos del modelo, para obtener una valor minimum de este costo, lo que significaria menos errores, entonces mejor modelo.&lt;/p&gt;
&lt;p&gt;Hay que minimizar esta función sobre el conjunto de entrenamiento (riesgo empírico) para encontrar parámetros del modelo satisfactorios:&lt;/p&gt;
$$ f_{\hat{\theta}} =\underset{f_\theta, \theta \in \Theta}{\arg\min} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$$&lt;p&gt;Los parametros van a cambiar para tener un valor minimum de costo:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/convergence_algo_optim.png&#34; alt=&#34;convergence_algo_optim&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La funcion de costo expresa el error desde una perspectiva &lt;strong&gt;numérica&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Transmite al algoritmo de aprendizaje lo que es importante y tiene sentido para la tarea&lt;/li&gt;
&lt;li&gt;Debe ser una función que se pueda optimizar eficientemente (convexa). &lt;strong&gt;La función  $\ell^{0/1} = \mathds{1}_{f(\mathbf{X}) = Y}$ 
 no es utilizable&lt;/strong&gt; (ni siquiera continua).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;complejidad-de-los-modelos-y-sobresoto-aprendizaje&#34;&gt;Complejidad de los modelos y sobre/soto-aprendizaje&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; $\mathcal{F} = \{ f: \text{ funciones medibles } \mathcal{X} \text{&amp;rarr;} \mathcal{Y}\}$ 
&lt;/li&gt;
&lt;li&gt;Mejor solución  $f^* = \arg\min_{f \in \mathcal{F}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Clase de funciones  $\mathcal{S} \subset \mathcal{F}$ 
 utilizadas como modelos&lt;/li&gt;
&lt;li&gt;Objetivo ideal en $\mathcal{S}$:  $f^*_\mathcal{S} = \arg\min_{f \in \mathcal{S}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Estimación obtenida en  $\mathcal{S}$ 
: se obtiene  $f_\mathcal{S}$ 
 tras un entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Se pueden encontrar dos maneras de no tener el riesgo minimum optimum:&lt;/p&gt;
$$ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f^*) = \textcolor{red}{\underbrace{ \mathcal{R}(f_\mathcal{S}^*) - \mathcal{R}(f^*) }_{\text{error de aproximacion}}} +  \textcolor{blue}{\underbrace{ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f_\mathcal{S}^*) }_{\text{error de estimacion}}}$$&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/approx_estim_errors.png&#34; alt=&#34;approx_estim_errors&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El error de aproximación puede ser grande si el modelo $\mathcal{S}$ no es adaptado, y el error de estimación puede ser grande si el modelo es complejo.&lt;/p&gt;
&lt;p&gt;Un ejemplo simple seria un polinomio de grado P que quiere estimar un polinomio de grado N con ruido:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/ex_over-underfitting.png&#34; alt=&#34;underfitting&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Soto-aprentizaje&lt;/strong&gt;: Si no hay demasiado parametros, es imposible de estimar bien la curva,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sobre-aprentizaje&lt;/strong&gt;: Si hay demasiado parametros va a enfocar en memorizar el ruido del ensemble de entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;regularizacion-y-parsimonia&#34;&gt;Regularizacion y parsimonia&lt;/h3&gt;
&lt;p&gt;Una solucion para combatir el problema de no generalizacion es la regularizacion, que permite de agregar una penalización en relación con la complejidad del modelo:&lt;/p&gt;

$$\arg\min_{f_\theta, \theta \in \Theta} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) ) + pen(\theta)$$


&lt;p&gt;Hay varias posibilidades de penalizacion, generalmente se usa la norma de los pesos del modelo. La intuicion detras de eso es que disminuir la norma del modelo o su número de coeficientes, número de ramas del grafo (poda).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AIC:

$pen(\theta) = \lambda ||\theta||_0$


&lt;em&gt;(no convexa, parsimoniosa, poco utilizada)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ridge:

$pen(\theta) = \lambda ||\theta||_2$ 


&lt;em&gt;(convexa, no parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Lasso:

$pen(\theta) = \lambda ||\theta||_1$ 


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Elastic Net:

$pen(\theta) = \lambda_1 ||\theta||_1 + \lambda_2 ||\theta||_2$


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El  $\lambda$ 
 es un nuevo hiperparametro del modelo.&lt;/p&gt;
&lt;p&gt;El lasso induce la parcimonia. Aca se pueden ver para  $n=\{0,1,2\}$ 
, las bolas  $$\mathcal{B}^n = \{x / x \in \mathbb{R}^d \text{ and } ||x||_n &lt; 1\}$$ 
:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/norms.png&#34; alt=&#34;norms&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En dimensiones grandes, la mayoría de  $\mathcal{B}^1$ 
 se concentra en los ejes: &lt;strong&gt;esto equivale a tener valores nulos para otros ejes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/Sparsityl1.png&#34; alt=&#34;Sparsityl1&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;optimizacion-loss-landscape&#34;&gt;Optimizacion, &lt;em&gt;loss landscape&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;La optimizacion de la funcion de costo sobre el ensemble de entrenamiento ( $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
) se puede hacer de manera analitica en casos simples, o de manera iterativa. La fase de optimizacion va a &lt;strong&gt;hacer converger los parametros&lt;/strong&gt; para encontrar los que van a dar un &lt;strong&gt;costo minimum en el conjunto de datos de entrenamiento&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/convergence_algo_optim.png&#34; alt=&#34;convergence_algo_optim&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En este ejemplo se puede ver los parametros $a,b$
 del modelo  $y = a\mathbf{X}+b$ 
 cambiar por cada iteracion, para tener un valor del error (sse; suma residual de cuadrados) que esta diminuando:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/regression_gif.gif&#34; alt=&#34;regression_gif&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Porque la valor del costo empirico  $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
 es un nombre real positivo, podemos representarlo en un eje, y los parametros con unos otros ejes. Eso se llama el &lt;em&gt;loss landscape&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/LossAlps.png&#34; alt=&#34;LossAlps&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del algoritmo de optimizacion es de encontrar la &amp;ldquo;ruta&amp;rdquo; para conducir en una &amp;ldquo;valle&amp;rdquo;, que representa un minimum local o global. Este ollo significa que los parametros sean los que dan un error pequeña.&lt;/p&gt;
&lt;h3 id=&#34;gradiente-deciendente&#34;&gt;Gradiente deciendente&lt;/h3&gt;
&lt;p&gt;El gradiente El gradiente de una función  $\nabla_xf(x)=(\frac{\partial f}{\partial\x_i})_{i=1..n}$ 
 es su derivativa según cada dimensión. Es una &lt;strong&gt;aproximación lineal de la función al nivel local&lt;/strong&gt;. Este indica la direccion donde aumenta una funcion:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/gradient_curve1D.png&#34; alt=&#34;gradient_curve1D&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Por eso, se puede utilizar el gradiente de la funccion de costo para minimizar el costo. Después de cada cálculo de la función de costo  $\ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$ 
, se calcula el gradiente de esta función para actualizar los parámetros $\theta$
:&lt;/p&gt;
$$	\theta \leftarrow \theta - \alpha*\nabla_\theta \ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$$&lt;p&gt;La tasa de aprendizaje  $\alpha$ 
 en la ecuacion precedente representa la cantidad de acutalizacion de los parametros. Es importante porque va a influir sobre la convergencia.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/learningrates.jpeg&#34; alt=&#34;learningrates&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En en &lt;em&gt;loss landscape&lt;/em&gt;, se puede representar el modelo durante la optimizacion como un vector moviendo en cada iteracion. Con este vision, la tasa de aprendizaje define mas o menos la &amp;ldquo;velocidad&amp;rdquo; de como se mueve este punto. Por eso, es simple de entender que a veces tiene que ser mas grande y otra veces mas pequeño, por ejemplo para pasar topografias particular del &lt;em&gt;landscape&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hay varios algoritmos de tipo gradiente decendiente para converger, con una mejora aproximacion de la tasa de aprendizaje, o la utilizacion de un momentum para ayudar el modelo
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/OtherOptimizers.gif&#34; alt=&#34;OtherOptimizers&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;metricas&#34;&gt;Metricas&lt;/h2&gt;
&lt;h3 id=&#34;tipos-de-errores&#34;&gt;Tipos de errores&lt;/h3&gt;
&lt;p&gt;Un clasificador binario debe detectar un evento. A cada prediccion puede tener una prediciccion verdadera o falsa: eso son los True/False Positives/Negatives: True Positive (TP), False Postiive (FP), False Negative (FN), True Negative (TN). Se pueden encontrar dos tipos de errores:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/error_types.jpg&#34; alt=&#34;error&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Con eso se puede crear una matriz de confusion.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/confusion_matrix.png&#34; alt=&#34;confusion_matrix&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Las matrices de confusion pueden abarcar mas de 2 clases:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Se puede cada vez volver a una binaria:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/conf_mat_multi.png&#34; alt=&#34;conf_mat_multi&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-metricas-y-costo&#34;&gt;Tipos de metricas y costo&lt;/h3&gt;
&lt;p&gt;Usando los True/False Positives/Negatives, se puede calcular varias metricas segundo el tipo de applicacion. Tenemos: el accuracy al nivel general, y el recall, la precision y el F-score al nivel de las clases. Mas informacion &lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;por alla&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cada tipo de error puede tener un costo differente segundo si es importante o no en la applicacion del sistema. Por eso se puede utilizar un matriz de costo, y calcular un costo global del sistema.&lt;/p&gt;
&lt;h3 id=&#34;aggregacion&#34;&gt;Aggregacion&lt;/h3&gt;
&lt;p&gt;Se puede agregar las metricas que son al nivel de clase para obtener un valor general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Macro-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Micro-averaging&lt;/strong&gt;: crear matriz de confusion binaria para cada clase, combinar las matrices y luego evaluar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weighted-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar usando pesos segun el importancia de la clase (nombre de ejemplos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a simple example in python obtained by the &lt;code&gt;sklearn.metrics.classification_report&lt;/code&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;class 0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 2&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;roc--auc&#34;&gt;ROC &amp;amp; AUC&lt;/h3&gt;
&lt;p&gt;Al momento de la inferencia, nuestro clasificador binario va a dar una probabilidad que un ejemplo sea de la clase positiva. Generalmente, si es superior a $\tau = 0.5$
, significa que el ejemplo es de la clase positiva. Sin embargo, se puede jugar con este umbral $\tau$
.&lt;/p&gt;
&lt;p&gt;El ROC (Receiver Operating Characteristic) es une curva representando la performancia de un clasificador en varias situaciones y que se crea variando el umbral. Para varios valores de el umbra, se calcula la fraccion de verdaderos positivos de los positivos frente a la fraccion de falsos positivos de los negativos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/ROC.png&#34; alt=&#34;ROC&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El Area Under Curve (AUC) permite de obtener una unica valor representando la calidad de la curve. Mas grande significa mejor.&lt;/p&gt;
&lt;h3 id=&#34;regresion&#34;&gt;Regresion&lt;/h3&gt;
&lt;p&gt;Para una regresion, se utilizan metricas que que evaluan las distancias, y si el modelo representa bien la varianza de los datos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Varios calculos de errores&lt;/li&gt;
&lt;li&gt;Coeficiente de determinacion $R^2$
 representa la proporcion de la varianza explicada&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tecnicas-de-evaluacion&#34;&gt;Tecnicas de evaluacion&lt;/h2&gt;
&lt;h3 id=&#34;validacion-cruzada-cross-validation&#34;&gt;Validacion Cruzada (Cross-Validation)&lt;/h3&gt;
&lt;p&gt;Para obtener una mejora estimacion de las performancias del modelo. Para estar seguro de testear sobre cada datos, se puede hacer $V$
 experiencias, cortando el dataset en $V$
 partes, entrenar sobre $V-1$
 y testear sobre $1$
. Es un tipo de bootstrapping con los datos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/cross-val_final.png&#34; alt=&#34;cross-val_final&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Eso sirve para obtener los hiperparametros optimum, antes de entrenar el model final sobre todos los datos, y estimar las performancias sobre el ensemble de test.&lt;/p&gt;
&lt;h3 id=&#34;conjuntos-de-validacion-y-prueba-holdout&#34;&gt;Conjuntos de validacion y prueba (Holdout)&lt;/h3&gt;
&lt;p&gt;Si es imposible de hacer una validacion cruzada (porque el entrenaimento es largo), se puede crear un set de entrenamiento, de validacion, y de test.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/train_val_test.png&#34; alt=&#34;train_val_test&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tamaño-de-la-particion&#34;&gt;Tamaño de la particion&lt;/h3&gt;
&lt;p&gt;Un modelo que es buena usando pocos datos es interesante porque a veces obtener etiquetas puede ser costozo. Generalmente las &lt;strong&gt;perfomancias son mas variables y mas bajas con pocos datos de entrenamiento&lt;/strong&gt;, pero la &lt;strong&gt;evaluacion es mas confiable con mas datos de test&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/learning_curve.png&#34; alt=&#34;learning_curve&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
  </channel>
</rss>
