<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course | Valentin Barriere</title>
    <link>http://localhost:1313/tags/course/</link>
      <atom:link href="http://localhost:1313/tags/course/index.xml" rel="self" type="application/rss+xml" />
    <description>Course</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 30 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu6033596820838205990.png</url>
      <title>Course</title>
      <link>http://localhost:1313/tags/course/</link>
    </image>
    
    <item>
      <title>Introduccion</title>
      <link>http://localhost:1313/deep/1_introduction/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/1_introduction/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslides1_introductionpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/1_Introduction.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Introduccion</title>
      <link>http://localhost:1313/minerias/1_intro/</link>
      <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/1_intro/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_intro_generalpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Intro_general.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;h2 id=&#34;introducción-a-la-ciencia-de-datos-ia-y-machine-learning&#34;&gt;Introducción a la Ciencia de Datos, IA y Machine Learning&lt;/h2&gt;
&lt;p&gt;La &lt;strong&gt;Minería de Datos&lt;/strong&gt; (o Data Mining) es un campo que busca la &lt;strong&gt;extracción de conocimiento a partir de grandes cantidades de datos&lt;/strong&gt; mediante métodos automáticos o semiautomáticos. Se nutre de diversas disciplinas —como estadística, inteligencia artificial o informática— para &lt;strong&gt;encontrar patrones&lt;/strong&gt; y &lt;strong&gt;estructuras relevantes&lt;/strong&gt; en esos datos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Campos&#34; srcset=&#34;
               /minerias/1_intro/figures/DS_AI_ML_hu9447518532780869997.webp 400w,
               /minerias/1_intro/figures/DS_AI_ML_hu3965967255674636004.webp 760w,
               /minerias/1_intro/figures/DS_AI_ML_hu8785539472510587232.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/DS_AI_ML_hu9447518532780869997.webp&#34;
               width=&#34;760&#34;
               height=&#34;522&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sin embargo, dentro del panorama general, es útil diferenciar algunos conceptos clave:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Science (Ciencia de Datos)&lt;/strong&gt; se centra en el &lt;strong&gt;análisis de datos&lt;/strong&gt; para &lt;strong&gt;extraer conocimiento&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning (Aprendizaje Automático)&lt;/strong&gt; utiliza &lt;strong&gt;algoritmos&lt;/strong&gt; para &lt;strong&gt;predecir&lt;/strong&gt; y tomar decisiones basadas en los datos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Artificial Intelligence (Inteligencia Artificial)&lt;/strong&gt; va un paso más allá y busca &lt;strong&gt;sistemas que puedan realizar tareas “inteligentes” de manera autónoma&lt;/strong&gt;, a veces usando ML como herramienta fundamental.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;De forma simplificada:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Data mining genera entendimiento&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Machine learning genera predicciones&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Artificial intelligence genera acciones&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;material-necesario&#34;&gt;Material Necesario&lt;/h2&gt;
&lt;h3 id=&#34;python-y-anaconda&#34;&gt;Python y Anaconda&lt;/h3&gt;
&lt;p&gt;Para trabajar con análisis de datos y Machine Learning, se recomienda utilizar:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Computadora&lt;/strong&gt; con Python instalado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anaconda&lt;/strong&gt; (versión con Python 3.x)
&lt;ul&gt;
&lt;li&gt;Descarga desde &lt;a href=&#34;https://www.anaconda.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.anaconda.com/download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Incluye la distribución de Python y diversas bibliotecas útiles.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Notebook&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Un entorno interactivo para escribir y ejecutar código Python en celdas, visualizar gráficos, explicar y anotar pasos.&lt;/li&gt;
&lt;li&gt;Permite prototipar y analizar datos de forma ordenada.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pandas&#34;&gt;Pandas&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pandas&lt;/a&gt; es una &lt;strong&gt;biblioteca de Python&lt;/strong&gt; especializada en la &lt;strong&gt;manipulación y el análisis de datos&lt;/strong&gt;. Ofrece estructuras de datos como:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DataFrame&lt;/strong&gt;: tablas con filas y columnas, parecidas a las hojas de cálculo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Series&lt;/strong&gt;: columnas o vectores unidimensionales.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con pandas podemos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leer&lt;/strong&gt; datos (csv, Excel, bases de datos SQL).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtrar, agrupar y transformar&lt;/strong&gt; datos rápidamente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crear&lt;/strong&gt; resúmenes estadísticos y visualizaciones sencillas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En muchas tareas de minería de datos, &lt;code&gt;pandas&lt;/code&gt; es la base para cargar y preprocesar el dataset antes de aplicar modelos de Machine Learning.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pandas-biblioteca-de-análisis-de-datos-en-python&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pandas library&#34; srcset=&#34;
               /minerias/1_intro/figures/pandas_hu6045762415544197018.webp 400w,
               /minerias/1_intro/figures/pandas_hu7643437430166501985.webp 760w,
               /minerias/1_intro/figures/pandas_hu888891917289806406.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/pandas_hu6045762415544197018.webp&#34;
               width=&#34;760&#34;
               height=&#34;475&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      pandas: biblioteca de análisis de datos en Python
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;cheatsheets&#34;&gt;Cheatsheets&lt;/h3&gt;
&lt;p&gt;Además, existen &lt;strong&gt;cheatsheets&lt;/strong&gt; muy útiles para repasar rápidamente las funciones principales:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat NumPy&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_np_hu14375244523488473349.webp 400w,
               /minerias/1_intro/figures/cheat_np_hu14411272965934784444.webp 760w,
               /minerias/1_intro/figures/cheat_np_hu13597904967969990868.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_np_hu14375244523488473349.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat Matplotlib&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_matplot_hu15331046240580983923.webp 400w,
               /minerias/1_intro/figures/cheat_matplot_hu3135365546132595272.webp 760w,
               /minerias/1_intro/figures/cheat_matplot_hu10236542872121609322.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_matplot_hu15331046240580983923.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat Scikit-learn&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_skl_hu13780008353465452812.webp 400w,
               /minerias/1_intro/figures/cheat_skl_hu7231920706300985347.webp 760w,
               /minerias/1_intro/figures/cheat_skl_hu10880287153828955388.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_skl_hu13780008353465452812.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat pandas&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_pd_hu10999684890599352937.webp 400w,
               /minerias/1_intro/figures/cheat_pd_hu17601242950209789534.webp 760w,
               /minerias/1_intro/figures/cheat_pd_hu4889890984785326674.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_pd_hu10999684890599352937.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat Jupyter&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_jup_hu13542443862476004036.webp 400w,
               /minerias/1_intro/figures/cheat_jup_hu1785265908805913062.webp 760w,
               /minerias/1_intro/figures/cheat_jup_hu2839386195936346783.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_jup_hu13542443862476004036.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cheat Keras&#34; srcset=&#34;
               /minerias/1_intro/figures/cheat_keras_hu15372201235457536347.webp 400w,
               /minerias/1_intro/figures/cheat_keras_hu5611213694181866590.webp 760w,
               /minerias/1_intro/figures/cheat_keras_hu787666906409877520.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/cheat_keras_hu15372201235457536347.webp&#34;
               width=&#34;708&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Les pueden encontrar &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/tree/main/CheatSheets/Code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aca&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;diferentes-métodos-en-minería-de-datos--machine-learning&#34;&gt;Diferentes Métodos en Minería de Datos / Machine Learning&lt;/h2&gt;
&lt;p&gt;Hay varias tareas principales dentro del &lt;strong&gt;aprendizaje a partir de datos&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Clasificación&lt;/strong&gt;: predecir &lt;strong&gt;etiquetas&lt;/strong&gt; (clases discretas).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regresión&lt;/strong&gt;: predecir un &lt;strong&gt;valor&lt;/strong&gt; (continuo).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: &lt;strong&gt;agrupar&lt;/strong&gt; elementos según su similitud (sin etiquetas dadas).&lt;/li&gt;
&lt;li&gt;(Otros) Reducción de dimensión, detección de anomalías, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;visión-general&#34;&gt;Visión general&lt;/h3&gt;
&lt;p&gt;Un diagrama popular de &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-learn&lt;/a&gt; muestra el &lt;strong&gt;mapa&lt;/strong&gt; de estos métodos:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-diferentes-áreas-y-algoritmos-de-aprendizaje&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Mapa de ML de scikit-learn&#34; srcset=&#34;
               /minerias/1_intro/figures/ml_map_hu3659606064345962454.webp 400w,
               /minerias/1_intro/figures/ml_map_hu16404596823941526495.webp 760w,
               /minerias/1_intro/figures/ml_map_hu9329016589100192170.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/ml_map_hu3659606064345962454.webp&#34;
               width=&#34;760&#34;
               height=&#34;474&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diferentes áreas y algoritmos de aprendizaje
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A continuación, describimos algunos ejemplos de clasificaciones, regresiones y clusterings comunes.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;clasificación&#34;&gt;Clasificación&lt;/h3&gt;
&lt;p&gt;La &lt;strong&gt;clasificación&lt;/strong&gt; consiste en asignar una etiqueta a cada dato de un conjunto de posibles clases. Ejemplos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reconocimiento de emociones en el habla&lt;/strong&gt;: determinar si alguien está enojado, feliz, triste, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clasificación de especies de animales&lt;/strong&gt;: a partir de características de la imagen, decidir si es un gato, un puma, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Detección de tumores en imágenes médicas&lt;/strong&gt;: clasificar entre “tumor presente” vs “sin tumor”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplo-de-clasificación-con-múltiples-etiquetas&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo multi-label classification&#34; srcset=&#34;
               /minerias/1_intro/figures/multi_label_classif_hu9756133175341905838.webp 400w,
               /minerias/1_intro/figures/multi_label_classif_hu3882038718077223937.webp 760w,
               /minerias/1_intro/figures/multi_label_classif_hu12748719311531710440.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/multi_label_classif_hu9756133175341905838.webp&#34;
               width=&#34;760&#34;
               height=&#34;262&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplo de clasificación con múltiples etiquetas
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;regresión&#34;&gt;Regresión&lt;/h3&gt;
&lt;p&gt;La &lt;strong&gt;regresión&lt;/strong&gt; busca predecir un valor numérico continuo. Ejemplos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reconocer la intensidad de una emoción&lt;/strong&gt;: ¿cuánto enojo muestra la persona?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluar daños tras un terremoto&lt;/strong&gt;: estimar la severidad de daños en una escala cuantitativa.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medir la severidad de Alzheimer&lt;/strong&gt; en la voz: ¿qué tan avanzada está la enfermedad?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-de-un-valor-continuo-a-una-clasificación-basada-en-umbrales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de pasar de regresión a clasificación de edades&#34; srcset=&#34;
               /minerias/1_intro/figures/age_reg_to_classif_hu17690873109572084870.webp 400w,
               /minerias/1_intro/figures/age_reg_to_classif_hu5798053001599690990.webp 760w,
               /minerias/1_intro/figures/age_reg_to_classif_hu7439249746381267252.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/age_reg_to_classif_hu17690873109572084870.webp&#34;
               width=&#34;760&#34;
               height=&#34;649&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      De un valor continuo a una clasificación basada en umbrales
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;clustering&#34;&gt;Clustering&lt;/h3&gt;
&lt;p&gt;El &lt;strong&gt;clustering&lt;/strong&gt; (agrupamiento) agrupa automáticamente los datos según su semejanza, sin etiquetas previas. Ejemplos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Topic mining&lt;/strong&gt; en foros políticos: descubrir de qué hablan los ciudadanos (temas más discutidos).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Detección de desinformación&lt;/strong&gt; en redes sociales: agrupar noticias sospechosas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentación de clientes&lt;/strong&gt;: agrupar usuarios según sus preferencias para campañas de marketing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-los-puntos-se-agrupan-en-clusters-similares-se-puede-descartar-puntos-como-ruido&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Clustering de datos bidimensionales&#34; srcset=&#34;
               /minerias/1_intro/figures/clustering_hu7521456035374489175.webp 400w,
               /minerias/1_intro/figures/clustering_hu4875518410358034455.webp 760w,
               /minerias/1_intro/figures/clustering_hu10503044972186760370.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/clustering_hu7521456035374489175.webp&#34;
               width=&#34;760&#34;
               height=&#34;412&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los puntos se agrupan en clusters similares. Se puede descartar puntos como ruido.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;laboratorio-exploración-de-datos-con-movielens&#34;&gt;Laboratorio: Exploración de Datos con MovieLens&lt;/h2&gt;
&lt;p&gt;Como primer enfoque, estudiaremos un &lt;strong&gt;conjunto de datos de críticas de películas&lt;/strong&gt; (MovieLens):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consta de ~3 millones de puntuaciones (“ratings”).&lt;/li&gt;
&lt;li&gt;Incluye &lt;strong&gt;descriptores sociales&lt;/strong&gt;: edad, sexo, etc.&lt;/li&gt;
&lt;li&gt;Permite aplicar un &lt;strong&gt;análisis básico&lt;/strong&gt; de minería de datos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;MovieLens logo&#34; srcset=&#34;
               /minerias/1_intro/figures/movielens-logo-white_hu5994885034689636590.webp 400w,
               /minerias/1_intro/figures/movielens-logo-white_hu9162017677084157356.webp 760w,
               /minerias/1_intro/figures/movielens-logo-white_hu9786150572690709253.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/1_intro/figures/movielens-logo-white_hu5994885034689636590.webp&#34;
               width=&#34;760&#34;
               height=&#34;267&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En este lab, aprenderemos a:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cargar los datos en &lt;code&gt;pandas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Explorar variables (estadísticas descriptivas).&lt;/li&gt;
&lt;li&gt;Cruzar información de películas y usuarios.&lt;/li&gt;
&lt;li&gt;Visualizar distribuciones y relaciones simples.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Datos I</title>
      <link>http://localhost:1313/minerias/2_datos/</link>
      <pubDate>Fri, 29 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/2_datos/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_datospdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Datos.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;p&gt;Esta clase se centra en el concepto de &lt;strong&gt;Datos&lt;/strong&gt; dentro del contexto de Machine Learning y minería de datos. Veremos de manera general cómo se representan, qué tipos de datos existen, cómo es la calidad de estos datos y finalmente cómo podemos realizar pasos de preprocesamiento para preparar los datos antes de aplicar algoritmos de aprendizaje.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;generalidades&#34;&gt;Generalidades&lt;/h2&gt;
&lt;h3 id=&#34;scikit-learn-biblioteca-de-ml-en-python&#34;&gt;Scikit-learn: biblioteca de ML en Python&lt;/h3&gt;
&lt;p&gt;Para manejar datos y entrenar modelos, &lt;strong&gt;scikit-learn&lt;/strong&gt; proporciona multitud de herramientas:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-scikit-learn&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Logo scikit-learn&#34; srcset=&#34;
               /minerias/2_datos/figures/Scikit_learn_logo_small_svg_hu6327989177387202692.webp 400w,
               /minerias/2_datos/figures/Scikit_learn_logo_small_svg_hu8563963018684445495.webp 760w,
               /minerias/2_datos/figures/Scikit_learn_logo_small_svg_hu1265031393500101177.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/Scikit_learn_logo_small_svg_hu6327989177387202692.webp&#34;
               width=&#34;260&#34;
               height=&#34;140&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scikit-learn
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sitio oficial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/user_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;User guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El &lt;strong&gt;workflow general&lt;/strong&gt; involucra la carga de datos, preprocesamiento, extracción de características, entrenamiento y evaluación, con metodos normalizadas entre las clases:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-etapas-de-un-pipeline-en-scikit-learn&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Flujo de trabajo en scikit-learn&#34; srcset=&#34;
               /minerias/2_datos/figures/supervised_scikit_learn_hu17372726708710164390.webp 400w,
               /minerias/2_datos/figures/supervised_scikit_learn_hu13489451897173136613.webp 760w,
               /minerias/2_datos/figures/supervised_scikit_learn_hu5671300666085423450.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/supervised_scikit_learn_hu17372726708710164390.webp&#34;
               width=&#34;659&#34;
               height=&#34;484&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Etapas de un pipeline en Scikit-learn
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;patrones-en-los-datos-y-vectorización&#34;&gt;Patrones en los datos y vectorización&lt;/h3&gt;
&lt;p&gt;El objetivo de muchos métodos de Machine Learning es &lt;strong&gt;detectar estructuras&lt;/strong&gt; o &lt;strong&gt;patrones&lt;/strong&gt; en los datos. Para ello, generalmente necesitamos que la información esté en forma de &lt;strong&gt;vectores&lt;/strong&gt; numéricos, de modo que cada ejemplo (documento, imagen, usuario, transacción, etc.) esté representado como un conjunto de variables numéricas (una por dimensión).&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-here-is-the-content-of-this-class-in-the-global-framwork-of-scikit-learn&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Scikit&#34; srcset=&#34;
               /minerias/2_datos/figures/supervised_scikit_learn_FExt_hu4892075279246747440.webp 400w,
               /minerias/2_datos/figures/supervised_scikit_learn_FExt_hu14889019013373788.webp 760w,
               /minerias/2_datos/figures/supervised_scikit_learn_FExt_hu3437252132674115788.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/supervised_scikit_learn_FExt_hu4892075279246747440.webp&#34;
               width=&#34;659&#34;
               height=&#34;484&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Here is the content of this class, in the global framwork of scikit-learn
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En la práctica, tendremos que &lt;strong&gt;extraer representaciones cifradas&lt;/strong&gt; (features) que describan lo más relevante posible de cada ejemplo. Por ejemplo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformar un texto en un vector que represente la frecuencia de ciertas palabras.&lt;/li&gt;
&lt;li&gt;Medir el histograma de colores de una imagen.&lt;/li&gt;
&lt;li&gt;Recopilar atributos de una tabla (edad, sexo, país&amp;hellip;) para un usuario.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-un-ejemplo-de-vectorizacion-de-razas-de-animales-el-objetivo-aca-es-de-encontrar-un-modelo-que-puede-reconocer-las-partes-del-espacio-caracteristicas-de-una-clase&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Reconnaissance&#34; srcset=&#34;
               /minerias/2_datos/figures/reconnaissance_dans_espace_petit_hu17874801778558254842.webp 400w,
               /minerias/2_datos/figures/reconnaissance_dans_espace_petit_hu15134139039843177786.webp 760w,
               /minerias/2_datos/figures/reconnaissance_dans_espace_petit_hu5122108817378378540.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/reconnaissance_dans_espace_petit_hu17874801778558254842.webp&#34;
               width=&#34;363&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Un ejemplo de vectorizacion de razas de animales. El objetivo aca es de encontrar un modelo que puede reconocer las partes del espacio caracteristicas de una clase.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;features-y-etiquetas&#34;&gt;Features y etiquetas&lt;/h3&gt;
&lt;p&gt;Cuando hacemos aprendizaje &lt;strong&gt;supervisado&lt;/strong&gt;, además de la representación vectorial (features), necesitamos una &lt;strong&gt;etiqueta&lt;/strong&gt; o valor de salida asociado a cada ejemplo:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-proceso-global-de-un-sistema-supervisado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Esquema de aprendizaje supervisado&#34; srcset=&#34;
               /minerias/2_datos/figures/classif_hu9702192758643128410.webp 400w,
               /minerias/2_datos/figures/classif_hu1940178425719737447.webp 760w,
               /minerias/2_datos/figures/classif_hu1255634141064509397.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/classif_hu9702192758643128410.webp&#34;
               width=&#34;760&#34;
               height=&#34;476&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Proceso global de un sistema supervisado
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Lo que describe al ejemplo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Etiqueta (label)&lt;/strong&gt;: Variable objetivo que se quiere predecir.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelo&lt;/strong&gt;: Aprenderá parámetros para predecir la etiqueta a partir de las features.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tipos-de-datos&#34;&gt;Tipos de datos&lt;/h2&gt;
&lt;h3 id=&#34;cualitativos-vs-cuantitativos&#34;&gt;Cualitativos vs. cuantitativos&lt;/h3&gt;
&lt;p&gt;Los datos pueden ser de tipo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cuantitativos&lt;/strong&gt;: numéricos, mediciones, contajes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cualitativos&lt;/strong&gt;: categóricos, nominales o incluso ordinales (pero no lineales).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-diferentes-tipos-de-características-cantidad-marca-sabor-etc&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de café con datos cuantitativos y cualitativos&#34; srcset=&#34;
               /minerias/2_datos/figures/coffee_hu12238997880869042543.webp 400w,
               /minerias/2_datos/figures/coffee_hu3514629794022085907.webp 760w,
               /minerias/2_datos/figures/coffee_hu5031277750654754841.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/coffee_hu12238997880869042543.webp&#34;
               width=&#34;760&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diferentes tipos de características (cantidad, marca, sabor, etc.)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Los datos cualitativos pueden ser más interpretables, pero a veces pierden detalle.&lt;/li&gt;
&lt;li&gt;Los datos cuantitativos dan más precisión, pero pueden ser más difíciles de interpretar.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datos-estructurados-vs-no-estructurados&#34;&gt;Datos estructurados vs. no estructurados&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Estructurados&lt;/strong&gt;: Se presentan en tablas con filas y columnas, es decir, cada ejemplo/instancia y sus atributos (p. ej., dataset de Titanic).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No estructurados&lt;/strong&gt;: Texto, imágenes, audio, etc. Suelen requerir más trabajo de &lt;strong&gt;vectorización&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-comparación-entre-datos-en-tabla-y-datos-en-bruto&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplos de datos estructurados vs. no estructurados&#34; srcset=&#34;
               /minerias/2_datos/figures/str_vs_unstr_hu16312124660273804279.webp 400w,
               /minerias/2_datos/figures/str_vs_unstr_hu7267090381556476155.webp 760w,
               /minerias/2_datos/figures/str_vs_unstr_hu5542194265417987857.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/str_vs_unstr_hu16312124660273804279.webp&#34;
               width=&#34;760&#34;
               height=&#34;422&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Comparación entre datos en tabla y datos en bruto
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En muchos problemas, tendremos que convertir datos no estructurados a forma vectorial o tabular para poder aplicar algoritmos de ML.&lt;/p&gt;
&lt;h3 id=&#34;distancia-entre-vectores&#34;&gt;Distancia entre vectores&lt;/h3&gt;
&lt;p&gt;Cuando representamos datos como vectores, podemos comparar su &lt;strong&gt;similitud&lt;/strong&gt; o &lt;strong&gt;diferencia&lt;/strong&gt; con métricas como la distancia euclidiana o el &lt;strong&gt;coseno&lt;/strong&gt; (similaridad de coseno):&lt;/p&gt;
\[
\cos(\mathbf{X}, \mathbf{X}&#39;) \;=\; 
\frac{\langle \mathbf{X}, \mathbf{X}&#39;\rangle}{\|\mathbf{X}\|\;\|\mathbf{X}&#39;\|}.
\]&lt;p&gt;















&lt;figure  id=&#34;figure-la-distancia-entre-vectores-se-puede-calcular-de-varias-maneras&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;distance_vectors&#34; srcset=&#34;
               /minerias/2_datos/figures/distance_vectors_hu9643923070251790051.webp 400w,
               /minerias/2_datos/figures/distance_vectors_hu922159784453930484.webp 760w,
               /minerias/2_datos/figures/distance_vectors_hu10091620246869096621.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/distance_vectors_hu9643923070251790051.webp&#34;
               width=&#34;760&#34;
               height=&#34;364&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La distancia entre vectores se puede calcular de varias maneras
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Esto se usa en muchas aplicaciones de clustering, recomendación y clasificación.&lt;/p&gt;
&lt;h3 id=&#34;extraccion-con-sklearn&#34;&gt;Extraccion con sklearn&lt;/h3&gt;
&lt;p&gt;Un ejemplo simple de one-hot encoding con scikit-learn:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;genders&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;locations&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;from Africa&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Asia&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Europe&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from US&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;browsers&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Chrome&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Firefox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses IE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Safari&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;preprocessing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OneHotEncoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;genders&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;locations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;browsers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Note that for there are missing categorical values for the 2nd and 3rd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# feature&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from US&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Safari&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Europe&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Firefox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;OneHotEncoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;from Africa&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Asia&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Europe&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                           &lt;span class=&#34;s1&#34;&gt;&amp;#39;from US&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Chrome&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Firefox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses IE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                           &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Safari&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;from Asia&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;uses Chrome&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;toarray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;calidad-de-los-datos&#34;&gt;Calidad de los datos&lt;/h2&gt;
&lt;p&gt;Los datos reales suelen estar lejos de ser perfectos.&lt;/p&gt;
&lt;h3 id=&#34;ruido&#34;&gt;Ruido&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ruido&lt;/strong&gt;: irregularidad aleatoria en los datos, diferencias no explicadas por el modelo. No tienen ningún patrón. Estos errores suelen ser &lt;strong&gt;inevitables e imprevisibles&lt;/strong&gt;. Puede provenir de:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Errores&lt;/strong&gt;: Errores de medición o muestreo que pueden distorsionar los datos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Residuos&lt;/strong&gt;: Variación intrínseca no capturada en nuestras features. Incluso aunque no haya errores de medición, un modelo no suele capturar el 100% de la variabilidad, por lo que siempre existirán residuos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ruido-blanco-en-una-imagen-las-perturbaciones-que-habia-en-la-television-cuando-habia-mala-señal-el-sonido-de-fondo-cuando-capta-mal-el-telefono&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ruido blanco&#34; srcset=&#34;
               /minerias/2_datos/figures/white_noise_hu579348812110497967.webp 400w,
               /minerias/2_datos/figures/white_noise_hu10361843702748057629.webp 760w,
               /minerias/2_datos/figures/white_noise_hu15236131570812366011.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/white_noise_hu579348812110497967.webp&#34;
               width=&#34;760&#34;
               height=&#34;284&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ruido blanco en una imagen, las perturbaciones que habia en la television, cuando habia mala señal, el sonido de fondo cuando capta mal el telefono&amp;hellip;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es difícil (o imposible) de representar toda la realidad con un ensamble finito de observaciones&lt;/li&gt;
&lt;li&gt;Vamos a representar una cosa con un vector de tamaño finito, lo que puede ser reductible al fenómeno inicial, es una aproximación de la realidad&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Se va a quedar un componente de ruido que no se puede modelizar&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si queremos modelizar \(Y = 3*X_1 - 2*X^2_2 + \epsilon\) con \(X_1\) y \(X_2\), no lo vamos a lograr.
















&lt;figure  id=&#34;figure-modelo-que-intenta-ajustar-datos-con-ruido&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ruido en el ajuste de una función&#34; srcset=&#34;
               /minerias/2_datos/figures/ex_over-underfitting_noise_hu7160750459805610265.webp 400w,
               /minerias/2_datos/figures/ex_over-underfitting_noise_hu1258070293613913168.webp 760w,
               /minerias/2_datos/figures/ex_over-underfitting_noise_hu16428299825422072936.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/ex_over-underfitting_noise_hu7160750459805610265.webp&#34;
               width=&#34;728&#34;
               height=&#34;255&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Modelo que intenta ajustar datos con ruido
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;outliers&#34;&gt;Outliers&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplo-de-outliers-en-2-dimensiones&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Outliers en diferentes distribuciones&#34; srcset=&#34;
               /minerias/2_datos/figures/outlier_gaussians_hu18089628100723988662.webp 400w,
               /minerias/2_datos/figures/outlier_gaussians_hu11729765782774009637.webp 760w,
               /minerias/2_datos/figures/outlier_gaussians_hu4176910214133138174.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/outlier_gaussians_hu18089628100723988662.webp&#34;
               width=&#34;440&#34;
               height=&#34;310&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplo de outliers en 2 dimensiones
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Un &lt;strong&gt;outlier&lt;/strong&gt; o valor atípico es un punto de datos que difiere significativamente de la mayoría. Pueden ser:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outliers ruidosos&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Son datos erróneos (fallos de medición, errores tipográficos, etc.) o extremos por variabilidad natural que no nos interesan.&lt;/li&gt;
&lt;li&gt;Suelen distorsionar estimaciones estadísticas (e.g. la media).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outliers “útiles”&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Responden a eventos raros o anómalos que sí queremos detectar (fraude, crisis, rarezas de inventario).&lt;/li&gt;
&lt;li&gt;Pueden ser el foco de ciertos análisis (detección de anomalías).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplos-del-mvtec-anomaly-detection-dataset&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Outliers en diferentes distribuciones&#34; srcset=&#34;
               /minerias/2_datos/figures/dataset_overview_anomaly_hu12222639050208223478.webp 400w,
               /minerias/2_datos/figures/dataset_overview_anomaly_hu14348380986165943337.webp 760w,
               /minerias/2_datos/figures/dataset_overview_anomaly_hu7518922898301784344.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/dataset_overview_anomaly_hu12222639050208223478.webp&#34;
               width=&#34;760&#34;
               height=&#34;386&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplos del MVTEC Anomaly Detection Dataset
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Para identificarlos, se pueden usar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualización (boxplots, scatter plots).&lt;/li&gt;
&lt;li&gt;Métodos estadísticos (rango intercuartílico, z-score).&lt;/li&gt;
&lt;li&gt;Algoritmos de ML (Isolation Forest, Local Outlier Factor).&lt;/li&gt;
&lt;li&gt;Validación de dominio (comprobar en la realidad si ese punto es auténtico o no).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-outlier-label-detection-cleanlabhttpsgithubcomcleanlabcleanlab-allowed-to-detect-many-of-the-label-error-in-the-imagenet-dataset&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Outliers en diferentes distribuciones&#34; srcset=&#34;
               /minerias/2_datos/figures/CleanLab_hu10917344694898179941.webp 400w,
               /minerias/2_datos/figures/CleanLab_hu1141749507235822239.webp 760w,
               /minerias/2_datos/figures/CleanLab_hu413918498232524429.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/CleanLab_hu10917344694898179941.webp&#34;
               width=&#34;760&#34;
               height=&#34;447&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Outlier Label detection. &lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CleanLab&lt;/a&gt; allowed to detect many of the label error in the Imagenet dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;valores-faltantes&#34;&gt;Valores faltantes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Missing values&lt;/strong&gt;: Es frecuente tener celdas vacías o desconocidas, por ejemplo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleImputer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;imp_mean&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleImputer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strategy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;imp_mean&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Podemos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Eliminar las filas&lt;/strong&gt; (si son pocas y su ausencia no afecta demasiado).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Imputar valores&lt;/strong&gt; usando la media, mediana o algoritmos como &lt;code&gt;KNNImputer&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelos que los manejen directamente&lt;/strong&gt;: algunos estimadores permiten tratar valores faltantes sin preprocesamiento adicional.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-existen-modelos-que-pueden-manejar-los-missing-values-en-scikit&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Modelos que aceptan valores faltantes&#34; srcset=&#34;
               /minerias/2_datos/figures/estimators_missing_values_hu5552150127740906947.webp 400w,
               /minerias/2_datos/figures/estimators_missing_values_hu2966550281670234438.webp 760w,
               /minerias/2_datos/figures/estimators_missing_values_hu1414166785689099899.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/estimators_missing_values_hu5552150127740906947.webp&#34;
               width=&#34;760&#34;
               height=&#34;419&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Existen modelos que pueden manejar los missing values en scikit
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;duplicados&#34;&gt;Duplicados&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Datos duplicados&lt;/strong&gt;: aparecen al combinar fuentes o por errores de recolección. Puede generar &lt;em&gt;sobrerepresentación&lt;/em&gt; de ciertos ejemplos y perjudicar el entrenamiento.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Errores en la concatenación o carga de datos desde fuentes múltiples.&lt;/li&gt;
&lt;li&gt;Recolección repetida de la misma observación.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Los duplicados suelen &lt;strong&gt;sobrerepresentar&lt;/strong&gt; determinados ejemplos, generando un sesgo en el entrenamiento. En casos masivos (p.ej., entrenamiento de grandes modelos de lenguaje), se ha demostrado que duplicar documentos puede perjudicar significativamente la calidad del modelo.&lt;/p&gt;
&lt;p&gt;Para mitigarlos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comparar hashes o firmas&lt;/strong&gt; de los ejemplos (si hablamos de texto, imágenes, etc.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering de similitud&lt;/strong&gt; de ejemplos para detectar duplicaciones leves o parciales.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Herramientas de deduplicación&lt;/strong&gt; específicas (ej.: para nombres de usuarios, direcciones de correo, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Es crucial deduplicar en datasets grandes (por ejemplo, para entrenar grandes modelos de lenguaje).&lt;/p&gt;
&lt;p&gt;En caso simple de datos tabulares, se puede utilizar metodos como &lt;code&gt;pandas.DataFrames.drop_duplicates()&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;use-case-llm&#34;&gt;Use-case: LLM&lt;/h3&gt;
&lt;p&gt;Para entrenar un LLM desde zero, es necesario de colectar una grande cantidad de datos! Colectando datos del web, es imposible de tener datos limpios!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Los datos extraídos de la web tienen mucho ruido y hay que limpiarlos.&lt;/li&gt;
&lt;li&gt;Marcas, roturas de sintaxis, etc&amp;hellip; todo lo que da texto no NL es perjudicial, ¡y puede impedir la convergencia!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Se ha determinado que la deduplicación desempeña un papel importante&lt;/strong&gt; en la mejora de los modelos lingüísticos (&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3359591.3359735?casa_token=AT3LybXtoLQAAAAA:LJLGtclf0beYhmJBuxCxUpAgDe4KspLeZYN2LWG9A3ePEl3Lkh21hsjczzjyyMiSx6dg7MQUbmtlLw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allamanis, 2019&lt;/a&gt;; &lt;a href=&#34;https://arxiv.org/abs/2107.06499&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lee et al., 2022&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Se ha demostrado que la repetición de datos es cada vez más perjudicial para la calidad del modelo a medida que aumenta el número de parámetros (&lt;a href=&#34;https://arxiv.org/abs/2205.10487&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hernandez et al., 2022&lt;/a&gt;):
&lt;ul&gt;
&lt;li&gt;para un modelo de 1B parámetros, cien duplicados son perjudiciales;&lt;/li&gt;
&lt;li&gt;a 175B, &lt;strong&gt;incluso unos pocos duplicados&lt;/strong&gt; podrían tener un efecto desproporcionado.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;limpieza-y-preprocesamiento&#34;&gt;Limpieza y preprocesamiento&lt;/h2&gt;
&lt;h3 id=&#34;estandarización-y-normalización&#34;&gt;Estandarización y normalización&lt;/h3&gt;
&lt;p&gt;Muchos algoritmos de ML (especialmente basados en distancias o gradientes) funcionan mejor cuando las &lt;strong&gt;features&lt;/strong&gt; tienen escalas comparables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Estandarización&lt;/strong&gt; (StandardScaler):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Convierten cada feature a media cero y varianza uno:&lt;/p&gt;
\[
     X_{\mathrm{std}} = \frac{X - \mu_X}{\sigma_X}.
     \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Afecta cada atributo de forma que su distribución resulte centrada en 0 y con desviación estándar 1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normalización&lt;/strong&gt; (Normalizer):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ajusta cada &lt;strong&gt;vector&lt;/strong&gt; para que su norma sea 1.&lt;/li&gt;
&lt;li&gt;Se suele usar en tareas donde la dirección del vector importa más que su magnitud (p.ej. coseno de similaridad).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scaling&lt;/strong&gt; a un rango \([0, 1]\) (MinMaxScaler):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Se “comprime” cada atributo dentro de \([0..1]\):&lt;/p&gt;
\[
     X_{\mathrm{scaled}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}.
     \]&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Útil cuando no se desea asumir forma gaussiana y se quiere mantener la escala finita.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;¿Por qué es importante?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evitar que los atributos con rangos muy grandes dominen sobre otros.&lt;/li&gt;
&lt;li&gt;Favorecer la convergencia de algoritmos de optimización que basan sus pasos en gradientes (como Redes Neuronales).&lt;/li&gt;
&lt;li&gt;Mejorar la calidad de métodos de distancia (k-NN, SVM, clustering) que asumen escalas comparables en las coordenadas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mas info &lt;a href=&#34;https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aca&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;discretización&#34;&gt;Discretización&lt;/h3&gt;
&lt;p&gt;Dividir atributos continuos en bins (categorías).&lt;/p&gt;
&lt;p&gt;La discretizacion (tambien conocida como cuantizacion o binning) permite dividir las caracterısticas continuas en valores discretos (clases). Las caracterısticas discretizadas codificadas de una sola vez pueden &lt;strong&gt;hacer que un modelo sea mas expresivo, manteniendo la interpretabilidad.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-transformar-la-variable-continua-edad-en-clases-discretas&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de discretización&#34; srcset=&#34;
               /minerias/2_datos/figures/age_reg_to_classif_hu17690873109572084870.webp 400w,
               /minerias/2_datos/figures/age_reg_to_classif_hu5798053001599690990.webp 760w,
               /minerias/2_datos/figures/age_reg_to_classif_hu7439249746381267252.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/age_reg_to_classif_hu17690873109572084870.webp&#34;
               width=&#34;760&#34;
               height=&#34;649&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Transformar la variable continua &amp;rsquo;edad&amp;rsquo; en clases discretas
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;polynomial-features&#34;&gt;Polynomial Features&lt;/h3&gt;
&lt;p&gt;Añadir términos polinómicos (no lineales) para incrementar la complejidad de un modelo lineal.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-polynomial-helps-fitiing-more-complex-functions&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de discretización&#34; srcset=&#34;
               /minerias/2_datos/figures/ex_over-underfitting_polynomial_hu12296047467455913660.webp 400w,
               /minerias/2_datos/figures/ex_over-underfitting_polynomial_hu3603271985052957351.webp 760w,
               /minerias/2_datos/figures/ex_over-underfitting_polynomial_hu12325601955647240871.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/ex_over-underfitting_polynomial_hu12296047467455913660.webp&#34;
               width=&#34;732&#34;
               height=&#34;274&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Polynomial helps fitiing more complex functions
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Mas informaciones &lt;a href=&#34;https://scikit-learn.org/stable/modules/preprocessing.html%5c#generating-polynomial-features&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aca&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;interés-del-sampling-muestreo&#34;&gt;Interés del sampling (muestreo)&lt;/h3&gt;
&lt;p&gt;Un mal muestreo puede generar sesgos en nuestros datos y conclusiones. Existen diversas estrategias:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Muestreo estratificado&lt;/strong&gt;: Mantiene proporciones de clases o grupos.&lt;br&gt;
















&lt;figure  id=&#34;figure-muestreo-estratificado-mantiene-proporciones-en-subgrupos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de muestreo estratificado&#34; srcset=&#34;
               /minerias/2_datos/figures/stratified_sampling_hu3739506566429986846.webp 400w,
               /minerias/2_datos/figures/stratified_sampling_hu12963448647892023621.webp 760w,
               /minerias/2_datos/figures/stratified_sampling_hu9627835439692744135.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/stratified_sampling_hu3739506566429986846.webp&#34;
               width=&#34;486&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Muestreo estratificado: mantiene proporciones en subgrupos
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Muestreo aleatorio simple&lt;/strong&gt;: Elegir instancias al azar.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Muestreo sistemático&lt;/strong&gt;: Tomar cada k-ésimo elemento desde un punto inicial aleatorio.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-systematic-random-sampling-selecciona-elementos-de-una-población-a-intervalos-regulares-desde-un-punto-de-partida-aleatorio&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de muestreo estratificado&#34; srcset=&#34;
               /minerias/2_datos/figures/systematic_random_sampling_hu11080984733742039593.webp 400w,
               /minerias/2_datos/figures/systematic_random_sampling_hu15285761124891998038.webp 760w,
               /minerias/2_datos/figures/systematic_random_sampling_hu15458938320889412188.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/systematic_random_sampling_hu11080984733742039593.webp&#34;
               width=&#34;760&#34;
               height=&#34;353&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Systematic Random Sampling: Selecciona elementos de una población a intervalos regulares desde un punto de partida aleatorio
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;agregaciones-de-datos&#34;&gt;Agregaciones de datos&lt;/h3&gt;
&lt;p&gt;Combinar varios valores en uno solo (por ejemplo, la media diaria) puede:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reducir ruido&lt;/strong&gt; y variabilidad.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simplificar&lt;/strong&gt; el conjunto de datos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resumir&lt;/strong&gt; grandes volúmenes de información.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-agregación-en-un-día-para-medir-la-opinión-general&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Agregación diaria de tweets positivos&#34; srcset=&#34;
               /minerias/2_datos/figures/hapiness_twitter_hu869437225968806113.webp 400w,
               /minerias/2_datos/figures/hapiness_twitter_hu9597793583217252604.webp 760w,
               /minerias/2_datos/figures/hapiness_twitter_hu9500706212212640228.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/hapiness_twitter_hu869437225968806113.webp&#34;
               width=&#34;760&#34;
               height=&#34;398&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Agregación en un día para medir la opinión general
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;dimensión-cursa-y-reducción&#34;&gt;Dimensión: &lt;em&gt;Cursa&lt;/em&gt; y reducción&lt;/h3&gt;
&lt;p&gt;En altas dimensiones, los datos se dispersan y pierden significado las distancias (curse of dimensionality). Para mitigar esto:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reducción de dimensión&lt;/strong&gt; (p. ej. PCA, selección de atributos).&lt;/li&gt;
&lt;li&gt;Eliminar o fusionar atributos irrelevantes.&lt;/li&gt;
&lt;li&gt;Acelerar el procesamiento y mejorar la interpretabilidad.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-en-dimensiones-muy-altas-los-datos-se-distribuyen-uniformemente&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ilustración del curse of dimensionality&#34; srcset=&#34;
               /minerias/2_datos/figures/curse_of_dimensionality_hu13015040265525298778.webp 400w,
               /minerias/2_datos/figures/curse_of_dimensionality_hu17025575749590265977.webp 760w,
               /minerias/2_datos/figures/curse_of_dimensionality_hu17010720842900593354.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/2_datos/figures/curse_of_dimensionality_hu13015040265525298778.webp&#34;
               width=&#34;433&#34;
               height=&#34;347&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      En dimensiones muy altas, los datos se distribuyen uniformemente
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>(TODO) Datos II</title>
      <link>http://localhost:1313/minerias/3_datos_exp/</link>
      <pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/3_datos_exp/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-not-available-heretodopdf&#34;&gt;The slides are not available &lt;a href=&#34;todo.pdf&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Intro Aprendizaje Supervisado</title>
      <link>http://localhost:1313/minerias-en/1_intro/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias-en/1_intro/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-heredm_intro_slpdf&#34;&gt;The slides are available &lt;a href=&#34;DM_Intro_SL.pdf&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;p&gt;This class is pretty cool as you will discover the basics of the knowledge used to build Machine Learning, Deep Learning and Artifical Intelligence in general: Supervised Learning! This is a simple setting where a &lt;strong&gt;model will learn its own parameters using examples and associated labels&lt;/strong&gt;. We will revise all the important concepts of supervised learning, which are very useful for anybody who wants to become data scientist.&lt;/p&gt;
&lt;p&gt;Esta clase es bastante bacan ya que descubriran las bases del conocimiento utilizado para construir Machine Learning, Deep Learning e Inteligencia Artificial en general: Aprendizaje Supervisado! Se trata de un entorno sencillo en el que un &lt;strong&gt;modelo aprenderá sus propios parámetros utilizando ejemplos y etiquetas asociadas&lt;/strong&gt;. Revisaremos todos los conceptos importantes del aprendizaje supervisado, que son muy útiles para cualquiera que quiera trabajar como data scientist.&lt;/p&gt;
&lt;h2 id=&#34;generalidades&#34;&gt;Generalidades&lt;/h2&gt;
&lt;p&gt;El aprendizaje supervisado utiliza datos y etiquetas para aprender a un modelo a reconocer padrones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es necesario de transformar los documentos en vectores, para poder hacer la optimizacion&lt;/li&gt;
&lt;li&gt;El modelo va a apprender sus parametros sobre un conjunto de entrenamiento&lt;/li&gt;
&lt;li&gt;El modelo entrenado puede ser usado para predicir la etiquetas de nuevos datos que nunca ha visto antes&lt;/li&gt;
&lt;li&gt;El documento puede ser cualquier dato: audio, texto, imagen, video, usuario, red,&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/predictive_modeling_data_flow.png&#34; alt=&#34;predictive_modeling_data_flow&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;features-y-etiquetas&#34;&gt;Features y etiquetas&lt;/h3&gt;
&lt;p&gt;Se puede representar en un espacio los documentos como vectores. Aca cada punto es una cancion, que esta representando con su intensidad y tempo promedios. La etiqueta es la color del punto. Aca tenemos una tarea de &lt;strong&gt;clasificacion binaria de musica&lt;/strong&gt;, sengundo las preferencias de un usuario.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/classif.jpg&#34; alt=&#34;classif&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del juego, va a ser de encontrar &lt;strong&gt;una funcion que separa el espacio en dos partes&lt;/strong&gt;. Una donde hay las canciones que le gustan a la persona, y la otra parte que no le gustan. De este manera, cuando vamos a tener un nuevo punto en este espacio, podemos decir si la persona va a gustar o no esta cancion, lo que sea &lt;strong&gt;predicir su etiqueta&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;en-resumen&#34;&gt;En resumen&lt;/h3&gt;
&lt;p&gt;Se necesitan varias cosas para un entrenamiento&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tener datos etiquetados&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Conjunto de datos de tamaño $n$
, $\mathcal{D}_n = \{(\text{Doc}_i, Y_i), i=1..n\}$
&lt;/li&gt;
&lt;li&gt;$\text{Doc}$ es una muestra (por ejemplo: una persona)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Extraer los descriptores&lt;/strong&gt; = transformar documentos en vectores&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{X}$ es un vector de observaciones (por ejemplo: edad, sexo, salario)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Crear un modelo matemático $f_\theta$&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Modelo $f_\theta$ tal que  $f_\theta(\mathbf{X})$
 esté cerca de $Y$ (para regresión)&lt;/li&gt;
&lt;li&gt;$\theta$ es el conjunto de parámetros del modelo matemático&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Implementar una función de costo (error) $\ell$
 a minimizar&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Cuanto más se equivoque el modelo, mayor será el costo&lt;/li&gt;
&lt;li&gt;En general, se desea tener un costo pequeño&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Encontrar los parámetros  $\theta^*$ 
 de manera que  $\ell(f_{\theta^*}(\mathbf{X}_i), Y_i)$ 
 sea pequeño&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
$$\theta^* = \underset{\theta}{\arg\min}\sum_{i}\ell(f_{\theta}(\mathbf{X}_i), Y_i)$$&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Probar $f_{\theta^*}$
 en nuevos datos con una métrica de evaluación adecuada&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;aprendizaje&#34;&gt;Aprendizaje&lt;/h2&gt;
&lt;p&gt;Hay varios conceptos en el aprendizaje:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Datos etiquetados&lt;/strong&gt;: Regresión o Clasificación&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extracción de características&lt;/strong&gt;: Tono, Intensidad, Tempo o Edad, Salario, Género, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelo $f_\theta$&lt;/strong&gt;: SVM, Regresión Logística, Bosque Aleatorio, CNN&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Función de costo&lt;/strong&gt; a optimizar: Pérdida de Bisagra, Pérdida de Entropía Cruzada, Pérdida Logística, Pérdida Cuadrada, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algoritmo de optimización&lt;/strong&gt;: Adam, SGD, BFGS, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Métrica de evaluación&lt;/strong&gt;: Recall, Precisión, Mínimos Cuadrados, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funcion-de-costo&#34;&gt;Funcion de costo&lt;/h3&gt;
&lt;p&gt;Para cuantificar las errores del modelo en la optimizacion, se necesita una funcion de costo que llamamos  $\ell$ 
. Ella representa si el modelo esta dando las buenas respuestas $y$
 segundo una entrada $X$
. Este funcion penaliza el modelo cuando comete errores, y lo que queremos hacer es optimizar los pesos del modelo, para obtener una valor minimum de este costo, lo que significaria menos errores, entonces mejor modelo.&lt;/p&gt;
&lt;p&gt;Hay que minimizar esta función sobre el conjunto de entrenamiento (riesgo empírico) para encontrar parámetros del modelo satisfactorios:&lt;/p&gt;
$$ f_{\hat{\theta}} =\underset{f_\theta, \theta \in \Theta}{\arg\min} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$$&lt;p&gt;Los parametros van a cambiar para tener un valor minimum de costo:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/convergence_algo_optim.png&#34; alt=&#34;convergence_algo_optim&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La funcion de costo expresa el error desde una perspectiva &lt;strong&gt;numérica&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Transmite al algoritmo de aprendizaje lo que es importante y tiene sentido para la tarea&lt;/li&gt;
&lt;li&gt;Debe ser una función que se pueda optimizar eficientemente (convexa). &lt;strong&gt;La función  $\ell^{0/1} = \mathds{1}_{f(\mathbf{X}) = Y}$ 
 no es utilizable&lt;/strong&gt; (ni siquiera continua).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;complejidad-de-los-modelos-y-sobresoto-aprendizaje&#34;&gt;Complejidad de los modelos y sobre/soto-aprendizaje&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; $\mathcal{F} = \{ f: \text{ funciones medibles } \mathcal{X} \text{&amp;rarr;} \mathcal{Y}\}$ 
&lt;/li&gt;
&lt;li&gt;Mejor solución  $f^* = \arg\min_{f \in \mathcal{F}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Clase de funciones  $\mathcal{S} \subset \mathcal{F}$ 
 utilizadas como modelos&lt;/li&gt;
&lt;li&gt;Objetivo ideal en $\mathcal{S}$:  $f^*_\mathcal{S} = \arg\min_{f \in \mathcal{S}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Estimación obtenida en  $\mathcal{S}$ 
: se obtiene  $f_\mathcal{S}$ 
 tras un entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Se pueden encontrar dos maneras de no tener el riesgo minimum optimum:&lt;/p&gt;
$$ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f^*) = \textcolor{red}{\underbrace{ \mathcal{R}(f_\mathcal{S}^*) - \mathcal{R}(f^*) }_{\text{error de aproximacion}}} +  \textcolor{blue}{\underbrace{ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f_\mathcal{S}^*) }_{\text{error de estimacion}}}$$&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/approx_estim_errors.png&#34; alt=&#34;approx_estim_errors&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El error de aproximación puede ser grande si el modelo $\mathcal{S}$ no es adaptado, y el error de estimación puede ser grande si el modelo es complejo.&lt;/p&gt;
&lt;p&gt;Un ejemplo simple seria un polinomio de grado P que quiere estimar un polinomio de grado N con ruido:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/ex_over-underfitting.png&#34; alt=&#34;underfitting&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Soto-aprentizaje&lt;/strong&gt;: Si no hay demasiado parametros, es imposible de estimar bien la curva,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sobre-aprentizaje&lt;/strong&gt;: Si hay demasiado parametros va a enfocar en memorizar el ruido del ensemble de entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;regularizacion-y-parsimonia&#34;&gt;Regularizacion y parsimonia&lt;/h3&gt;
&lt;p&gt;Una solucion para combatir el problema de no generalizacion es la regularizacion, que permite de agregar una penalización en relación con la complejidad del modelo:&lt;/p&gt;

$$\arg\min_{f_\theta, \theta \in \Theta} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) ) + pen(\theta)$$


&lt;p&gt;Hay varias posibilidades de penalizacion, generalmente se usa la norma de los pesos del modelo. La intuicion detras de eso es que disminuir la norma del modelo o su número de coeficientes, número de ramas del grafo (poda).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AIC:

$pen(\theta) = \lambda ||\theta||_0$


&lt;em&gt;(no convexa, parsimoniosa, poco utilizada)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ridge:

$pen(\theta) = \lambda ||\theta||_2$ 


&lt;em&gt;(convexa, no parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Lasso:

$pen(\theta) = \lambda ||\theta||_1$ 


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Elastic Net:

$pen(\theta) = \lambda_1 ||\theta||_1 + \lambda_2 ||\theta||_2$


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El  $\lambda$ 
 es un nuevo hiperparametro del modelo.&lt;/p&gt;
&lt;p&gt;El lasso induce la parcimonia. Aca se pueden ver para  $n=\{0,1,2\}$ 
, las bolas  $$\mathcal{B}^n = \{x / x \in \mathbb{R}^d \text{ and } ||x||_n &lt; 1\}$$ 
:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/norms.png&#34; alt=&#34;norms&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En dimensiones grandes, la mayoría de  $\mathcal{B}^1$ 
 se concentra en los ejes: &lt;strong&gt;esto equivale a tener valores nulos para otros ejes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/Sparsityl1.png&#34; alt=&#34;Sparsityl1&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;optimizacion-loss-landscape&#34;&gt;Optimizacion, &lt;em&gt;loss landscape&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;La optimizacion de la funcion de costo sobre el ensemble de entrenamiento ( $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
) se puede hacer de manera analitica en casos simples, o de manera iterativa. La fase de optimizacion va a &lt;strong&gt;hacer converger los parametros&lt;/strong&gt; para encontrar los que van a dar un &lt;strong&gt;costo minimum en el conjunto de datos de entrenamiento&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/convergence_algo_optim.png&#34; alt=&#34;convergence_algo_optim&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En este ejemplo se puede ver los parametros $a,b$
 del modelo  $y = a\mathbf{X}+b$ 
 cambiar por cada iteracion, para tener un valor del error (sse; suma residual de cuadrados) que esta diminuando:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/regression_gif.gif&#34; alt=&#34;regression_gif&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Porque la valor del costo empirico  $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
 es un nombre real positivo, podemos representarlo en un eje, y los parametros con unos otros ejes. Eso se llama el &lt;em&gt;loss landscape&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/LossAlps.png&#34; alt=&#34;LossAlps&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del algoritmo de optimizacion es de encontrar la &amp;ldquo;ruta&amp;rdquo; para conducir en una &amp;ldquo;valle&amp;rdquo;, que representa un minimum local o global. Este ollo significa que los parametros sean los que dan un error pequeña.&lt;/p&gt;
&lt;h3 id=&#34;gradiente-deciendente&#34;&gt;Gradiente deciendente&lt;/h3&gt;
&lt;p&gt;El gradiente El gradiente de una función  $\nabla_xf(x)=(\frac{\partial f}{\partial\x_i})_{i=1..n}$ 
 es su derivativa según cada dimensión. Es una &lt;strong&gt;aproximación lineal de la función al nivel local&lt;/strong&gt;. Este indica la direccion donde aumenta una funcion:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/gradient_curve1D.png&#34; alt=&#34;gradient_curve1D&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Por eso, se puede utilizar el gradiente de la funccion de costo para minimizar el costo. Después de cada cálculo de la función de costo  $\ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$ 
, se calcula el gradiente de esta función para actualizar los parámetros $\theta$
:&lt;/p&gt;
$$	\theta \leftarrow \theta - \alpha*\nabla_\theta \ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$$&lt;p&gt;La tasa de aprendizaje  $\alpha$ 
 en la ecuacion precedente representa la cantidad de acutalizacion de los parametros. Es importante porque va a influir sobre la convergencia.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/learningrates.jpeg&#34; alt=&#34;learningrates&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En en &lt;em&gt;loss landscape&lt;/em&gt;, se puede representar el modelo durante la optimizacion como un vector moviendo en cada iteracion. Con este vision, la tasa de aprendizaje define mas o menos la &amp;ldquo;velocidad&amp;rdquo; de como se mueve este punto. Por eso, es simple de entender que a veces tiene que ser mas grande y otra veces mas pequeño, por ejemplo para pasar topografias particular del &lt;em&gt;landscape&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hay varios algoritmos de tipo gradiente decendiente para converger, con una mejora aproximacion de la tasa de aprendizaje, o la utilizacion de un momentum para ayudar el modelo
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/OtherOptimizers.gif&#34; alt=&#34;OtherOptimizers&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;metricas&#34;&gt;Metricas&lt;/h2&gt;
&lt;h3 id=&#34;tipos-de-errores&#34;&gt;Tipos de errores&lt;/h3&gt;
&lt;p&gt;Un clasificador binario debe detectar un evento. A cada prediccion puede tener una prediciccion verdadera o falsa: eso son los True/False Positives/Negatives: True Positive (TP), False Postiive (FP), False Negative (FN), True Negative (TN). Se pueden encontrar dos tipos de errores:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/error_types.jpg&#34; alt=&#34;error&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Con eso se puede crear una matriz de confusion.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/confusion_matrix.png&#34; alt=&#34;confusion_matrix&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Las matrices de confusion pueden abarcar mas de 2 clases:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Se puede cada vez volver a una binaria:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/conf_mat_multi.png&#34; alt=&#34;conf_mat_multi&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-metricas-y-costo&#34;&gt;Tipos de metricas y costo&lt;/h3&gt;
&lt;p&gt;Usando los True/False Positives/Negatives, se puede calcular varias metricas segundo el tipo de applicacion. Tenemos: el accuracy al nivel general, y el recall, la precision y el F-score al nivel de las clases. Mas informacion &lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;por alla&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cada tipo de error puede tener un costo differente segundo si es importante o no en la applicacion del sistema. Por eso se puede utilizar un matriz de costo, y calcular un costo global del sistema.&lt;/p&gt;
&lt;h3 id=&#34;aggregacion&#34;&gt;Aggregacion&lt;/h3&gt;
&lt;p&gt;Se puede agregar las metricas que son al nivel de clase para obtener un valor general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Macro-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Micro-averaging&lt;/strong&gt;: crear matriz de confusion binaria para cada clase, combinar las matrices y luego evaluar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weighted-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar usando pesos segun el importancia de la clase (nombre de ejemplos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a simple example in python obtained by the &lt;code&gt;sklearn.metrics.classification_report&lt;/code&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;class 0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 2&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;roc--auc&#34;&gt;ROC &amp;amp; AUC&lt;/h3&gt;
&lt;p&gt;Al momento de la inferencia, nuestro clasificador binario va a dar una probabilidad que un ejemplo sea de la clase positiva. Generalmente, si es superior a $\tau = 0.5$
, significa que el ejemplo es de la clase positiva. Sin embargo, se puede jugar con este umbral $\tau$
.&lt;/p&gt;
&lt;p&gt;El ROC (Receiver Operating Characteristic) es une curva representando la performancia de un clasificador en varias situaciones y que se crea variando el umbral. Para varios valores de el umbra, se calcula la fraccion de verdaderos positivos de los positivos frente a la fraccion de falsos positivos de los negativos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/ROC.png&#34; alt=&#34;ROC&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El Area Under Curve (AUC) permite de obtener una unica valor representando la calidad de la curve. Mas grande significa mejor.&lt;/p&gt;
&lt;h3 id=&#34;regresion&#34;&gt;Regresion&lt;/h3&gt;
&lt;p&gt;Para una regresion, se utilizan metricas que que evaluan las distancias, y si el modelo representa bien la varianza de los datos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Varios calculos de errores&lt;/li&gt;
&lt;li&gt;Coeficiente de determinacion $R^2$
 representa la proporcion de la varianza explicada&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tecnicas-de-evaluacion&#34;&gt;Tecnicas de evaluacion&lt;/h2&gt;
&lt;h3 id=&#34;validacion-cruzada-cross-validation&#34;&gt;Validacion Cruzada (Cross-Validation)&lt;/h3&gt;
&lt;p&gt;Para obtener una mejora estimacion de las performancias del modelo. Para estar seguro de testear sobre cada datos, se puede hacer $V$
 experiencias, cortando el dataset en $V$
 partes, entrenar sobre $V-1$
 y testear sobre $1$
. Es un tipo de bootstrapping con los datos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/cross-val_final.png&#34; alt=&#34;cross-val_final&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Eso sirve para obtener los hiperparametros optimum, antes de entrenar el model final sobre todos los datos, y estimar las performancias sobre el ensemble de test.&lt;/p&gt;
&lt;h3 id=&#34;conjuntos-de-validacion-y-prueba-holdout&#34;&gt;Conjuntos de validacion y prueba (Holdout)&lt;/h3&gt;
&lt;p&gt;Si es imposible de hacer una validacion cruzada (porque el entrenaimento es largo), se puede crear un set de entrenamiento, de validacion, y de test.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/train_val_test.png&#34; alt=&#34;train_val_test&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tamaño-de-la-particion&#34;&gt;Tamaño de la particion&lt;/h3&gt;
&lt;p&gt;Un modelo que es buena usando pocos datos es interesante porque a veces obtener etiquetas puede ser costozo. Generalmente las &lt;strong&gt;perfomancias son mas variables y mas bajas con pocos datos de entrenamiento&lt;/strong&gt;, pero la &lt;strong&gt;evaluacion es mas confiable con mas datos de test&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/learning_curve.png&#34; alt=&#34;learning_curve&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Intro Aprendizaje Supervisado</title>
      <link>http://localhost:1313/minerias/4_intro_sl/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/4_intro_sl/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_intro_slpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Intro_SL.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;p&gt;Esta clase es bastante interesante ya que descubriran las bases del conocimiento utilizado para construir Machine Learning, Deep Learning e Inteligencia Artificial en general: Aprendizaje Supervisado!&lt;/p&gt;
&lt;p&gt;Se trata de un entorno sencillo en el que un &lt;strong&gt;modelo aprenderá sus propios parámetros utilizando ejemplos y etiquetas asociadas&lt;/strong&gt;. Revisaremos todos los conceptos importantes del aprendizaje supervisado, que son muy útiles para cualquiera que quiera trabajar como data scientist.&lt;/p&gt;
&lt;h2 id=&#34;generalidades&#34;&gt;Generalidades&lt;/h2&gt;
&lt;p&gt;El aprendizaje supervisado utiliza datos y etiquetas para aprender a un modelo a reconocer padrones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Es necesario de transformar los documentos en vectores, para poder hacer la optimizacion&lt;/li&gt;
&lt;li&gt;El modelo va a apprender sus parametros sobre un conjunto de entrenamiento&lt;/li&gt;
&lt;li&gt;El modelo entrenado puede ser usado para predicir la etiquetas de nuevos datos que nunca ha visto antes&lt;/li&gt;
&lt;li&gt;El documento puede ser cualquier dato: audio, texto, imagen, video, usuario, red,&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-aprendizaje-de-maquinas-predictivo-supervisado-sigue-eso&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;predictive_modeling_data_flow&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/predictive_modeling_data_flow_hu12315308385899460389.webp 400w,
               /minerias/4_intro_sl/figures/predictive_modeling_data_flow_hu9011300678791282958.webp 760w,
               /minerias/4_intro_sl/figures/predictive_modeling_data_flow_hu8943212305677280315.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/predictive_modeling_data_flow_hu12315308385899460389.webp&#34;
               width=&#34;760&#34;
               height=&#34;549&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El aprendizaje de maquinas predictivo supervisado sigue eso
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;features-y-etiquetas&#34;&gt;Features y etiquetas&lt;/h3&gt;
&lt;p&gt;Se puede representar en un espacio los documentos como vectores. Aca cada punto es una cancion, que esta representando con su intensidad y tempo promedios. La etiqueta es la color del punto. Aca tenemos una tarea de &lt;strong&gt;clasificacion binaria de musica&lt;/strong&gt;, segundo las preferencias de un usuario.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-para-las-tareas-de-clasificaciones-los-ejemplos-se-representan-como-puntos-en-un-espacio-de-dimension-de-las-observaciones-y-una-color-o-forma-para-representar-las-diferentes-clases&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;classif&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/classif_hu9702192758643128410.webp 400w,
               /minerias/4_intro_sl/figures/classif_hu1940178425719737447.webp 760w,
               /minerias/4_intro_sl/figures/classif_hu1255634141064509397.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/classif_hu9702192758643128410.webp&#34;
               width=&#34;760&#34;
               height=&#34;476&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Para las tareas de clasificaciones, los ejemplos se representan como puntos en un espacio de dimension de las observaciones, y una color o forma para representar las diferentes clases
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del juego, va a ser de encontrar &lt;strong&gt;una funcion que separa el espacio en dos partes&lt;/strong&gt;. Una donde hay las canciones que le gustan a la persona, y la otra parte que no le gustan. De este manera, cuando vamos a tener un nuevo punto en este espacio, podemos decir si la persona va a gustar o no esta cancion, lo que sea &lt;strong&gt;predicir su etiqueta&lt;/strong&gt;!&lt;/p&gt;
&lt;h3 id=&#34;en-resumen&#34;&gt;En resumen&lt;/h3&gt;
&lt;p&gt;Se necesitan varias cosas para un entrenamiento&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tener datos etiquetados&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Conjunto de datos de tamaño $n$
, $\mathcal{D}_n = \{(\text{Doc}_i, Y_i), i=1..n\}$
&lt;/li&gt;
&lt;li&gt;$\text{Doc}$ es una muestra (por ejemplo: una persona)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Extraer los descriptores&lt;/strong&gt; = transformar documentos en vectores&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{X}$ es un vector de observaciones (por ejemplo: edad, sexo, salario)&lt;/li&gt;
&lt;li&gt;$Y$ son las etiquetas (por ejemplo: monto del préstamo concedido)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Crear un modelo matemático $f_\theta$&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Modelo $f_\theta$ tal que  $f_\theta(\mathbf{X})$
 esté cerca de $Y$ (para regresión)&lt;/li&gt;
&lt;li&gt;$\theta$ es el conjunto de parámetros del modelo matemático&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Implementar una función de costo (error) $\ell$
 a minimizar&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Cuanto más se equivoque el modelo, mayor será el costo&lt;/li&gt;
&lt;li&gt;En general, se desea tener un costo pequeño&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Encontrar los parámetros  $\theta^*$ 
 de manera que  $\ell(f_{\theta^*}(\mathbf{X}_i), Y_i)$ 
 sea pequeño&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
$$\theta^* = \underset{\theta}{\arg\min}\sum_{i}\ell(f_{\theta}(\mathbf{X}_i), Y_i)$$&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Probar $f_{\theta^*}$
 en nuevos datos con una métrica de evaluación adecuada&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;aprendizaje&#34;&gt;Aprendizaje&lt;/h2&gt;
&lt;p&gt;Hay varios conceptos en el aprendizaje:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Datos etiquetados&lt;/strong&gt;: Regresión o Clasificación&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extracción de características&lt;/strong&gt;: Tono, Intensidad, Tempo o Edad, Salario, Género, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modelo $f_\theta$&lt;/strong&gt;: SVM, Regresión Logística, Bosque Aleatorio, CNN&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Función de costo&lt;/strong&gt; a optimizar: Pérdida de Bisagra, Pérdida de Entropía Cruzada, Pérdida Logística, Pérdida Cuadrada, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algoritmo de optimización&lt;/strong&gt;: Adam, SGD, BFGS, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Métrica de evaluación&lt;/strong&gt;: Recall, Precisión, Mínimos Cuadrados, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;funcion-de-costo&#34;&gt;Funcion de costo&lt;/h3&gt;
&lt;p&gt;Para cuantificar las errores del modelo en la optimizacion, se necesita una funcion de costo que llamamos  $\ell$ 
. Ella representa si el modelo esta dando las buenas respuestas $y$
 segundo una entrada $X$
. Este funcion penaliza el modelo cuando comete errores, y lo que queremos hacer es optimizar los pesos del modelo, para obtener una valor minimum de este costo, lo que significaria menos errores, entonces mejor modelo.&lt;/p&gt;
&lt;p&gt;Hay que minimizar esta función sobre el conjunto de entrenamiento (riesgo empírico) para encontrar parámetros del modelo satisfactorios:&lt;/p&gt;
$$ f_{\hat{\theta}} =\underset{f_\theta, \theta \in \Theta}{\arg\min} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$$&lt;p&gt;Los parametros van a cambiar para tener un valor minimum de costo:
















&lt;figure  id=&#34;figure-los-pesos-cambian-poco-a-poco-para-llegar-a-los-que-van-a-dar-un-valor-de-costo-minimum-sobre-el-ensemble-de-entrenamiento&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;convergence_algo_optim&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu3361748985186866440.webp 400w,
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu3884812293168319734.webp 760w,
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu2615079541688126126.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/convergence_algo_optim_hu3361748985186866440.webp&#34;
               width=&#34;566&#34;
               height=&#34;572&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los pesos cambian poco a poco para llegar a los que van a dar un valor de costo minimum sobre el ensemble de entrenamiento
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La funcion de costo expresa el error desde una perspectiva &lt;strong&gt;numérica&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Transmite al algoritmo de aprendizaje lo que es importante y tiene sentido para la tarea&lt;/li&gt;
&lt;li&gt;Debe ser una función que se pueda optimizar eficientemente (convexa). &lt;strong&gt;La función  $\ell^{0/1} = \mathbf{1}_{f(\mathbf{X}) = Y}$ 
 no es utilizable&lt;/strong&gt; (ni siquiera continua).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;complejidad-de-los-modelos-y-sobresoto-aprendizaje&#34;&gt;Complejidad de los modelos y sobre/soto-aprendizaje&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; $\mathcal{F} = \{ f: \text{ funciones medibles } \mathcal{X} \text{&amp;rarr;} \mathcal{Y}\}$ 
&lt;/li&gt;
&lt;li&gt;Mejor solución  $f^* = \arg\min_{f \in \mathcal{F}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Clase de funciones  $\mathcal{S} \subset \mathcal{F}$ 
 utilizadas como modelos&lt;/li&gt;
&lt;li&gt;Objetivo ideal en $\mathcal{S}$:  $f^*_\mathcal{S} = \arg\min_{f \in \mathcal{S}}\mathcal{R}(f)$ 
&lt;/li&gt;
&lt;li&gt;Estimación obtenida en  $\mathcal{S}$ 
: se obtiene  $f_\mathcal{S}$ 
 tras un entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Se pueden encontrar dos maneras de no tener el riesgo minimum optimum:&lt;/p&gt;
$$ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f^*) = \textcolor{red}{\underbrace{ \mathcal{R}(f_\mathcal{S}^*) - \mathcal{R}(f^*) }_{\text{error de aproximacion}}} +  \textcolor{blue}{\underbrace{ \mathcal{R}(\hat{f_\mathcal{S}}) - \mathcal{R}(f_\mathcal{S}^*) }_{\text{error de estimacion}}}$$&lt;p&gt;















&lt;figure  id=&#34;figure-los-2-tipos-de-errores-el-error-de-aproximacion-viene-de-la-eleccion-de-los-modelos-que-se-utilizan-y-el-error-de-estimacion-viene-de-un-mal-entrenamiento-del-modelo&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;approx_estim_errors&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/approx_estim_errors_hu1287908794753332032.webp 400w,
               /minerias/4_intro_sl/figures/approx_estim_errors_hu13343448165022106133.webp 760w,
               /minerias/4_intro_sl/figures/approx_estim_errors_hu4804674474612499519.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/approx_estim_errors_hu1287908794753332032.webp&#34;
               width=&#34;508&#34;
               height=&#34;435&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los 2 tipos de errores: el error de aproximacion viene de la eleccion de los modelos que se utilizan, y el error de estimacion viene de un mal entrenamiento del modelo
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El error de aproximación puede ser grande si el modelo $\mathcal{S}$ no es adaptado, y el error de estimación puede ser grande si el modelo es complejo.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-un-ejemplo-simple-seria-un-polinomio-de-grado-p-que-quiere-estimar-un-polinomio-de-grado-n-con-ruido-si-p-es-mas-grande-o-mas-pequeno-que-n-no-es-adaptado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;underfitting&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/ex_over-underfitting_hu10525897534329112004.webp 400w,
               /minerias/4_intro_sl/figures/ex_over-underfitting_hu2460076892096630892.webp 760w,
               /minerias/4_intro_sl/figures/ex_over-underfitting_hu9118631690023105709.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/ex_over-underfitting_hu10525897534329112004.webp&#34;
               width=&#34;760&#34;
               height=&#34;264&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Un ejemplo simple seria un polinomio de grado P que quiere estimar un polinomio de grado N con ruido. Si P es mas grande o mas pequeno que N, no es adaptado.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Soto-aprentizaje&lt;/strong&gt;: Si no hay demasiado parametros, es imposible de estimar bien la curva,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sobre-aprentizaje&lt;/strong&gt;: Si hay demasiado parametros va a enfocar en memorizar el ruido del ensemble de entrenamiento&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;regularizacion-y-parsimonia&#34;&gt;Regularizacion y parsimonia&lt;/h3&gt;
&lt;p&gt;Una solucion para combatir el problema de no generalizacion es la regularizacion, que permite de agregar una penalización en relación con la complejidad del modelo:&lt;/p&gt;

$$\arg\min_{f_\theta, \theta \in \Theta} \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) ) + pen(\theta)$$


&lt;p&gt;Hay varias posibilidades de penalizacion, generalmente se usa la norma de los pesos del modelo. La intuicion detras de eso es que disminuir la norma del modelo o su número de coeficientes, número de ramas del grafo (poda).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AIC:

$pen(\theta) = \lambda ||\theta||_0$


&lt;em&gt;(no convexa, parsimoniosa, poco utilizada)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ridge:

$pen(\theta) = \lambda ||\theta||_2$ 


&lt;em&gt;(convexa, no parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Lasso:

$pen(\theta) = \lambda ||\theta||_1$ 


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Elastic Net:

$pen(\theta) = \lambda_1 ||\theta||_1 + \lambda_2 ||\theta||_2$


&lt;em&gt;(convexa, parsimoniosa)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El  $\lambda$ 
 es un nuevo hiperparametro del modelo.&lt;/p&gt;
&lt;p&gt;El lasso induce la parcimonia. Aca se pueden ver para  $n=\{0,1,2\}$ 
, las bolas  $$\mathcal{B}^n = \{x / x \in \mathbb{R}^d \text{ and } ||x||_n &lt; 1\}$$ 
:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;norms&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/norms_hu13040269207447033896.webp 400w,
               /minerias/4_intro_sl/figures/norms_hu2484826675081491891.webp 760w,
               /minerias/4_intro_sl/figures/norms_hu549636321832970956.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/norms_hu13040269207447033896.webp&#34;
               width=&#34;760&#34;
               height=&#34;319&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En dimensiones grandes, la mayoría de  $\mathcal{B}^1$ 
 se concentra en los ejes: &lt;strong&gt;esto equivale a tener valores nulos para otros ejes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Sparsityl1&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/Sparsityl1_hu15118340912051855990.webp 400w,
               /minerias/4_intro_sl/figures/Sparsityl1_hu2311885472990105800.webp 760w,
               /minerias/4_intro_sl/figures/Sparsityl1_hu6053000379869258004.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/Sparsityl1_hu15118340912051855990.webp&#34;
               width=&#34;760&#34;
               height=&#34;308&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;optimizacion-loss-landscape&#34;&gt;Optimizacion, &lt;em&gt;loss landscape&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;La optimizacion de la funcion de costo sobre el ensemble de entrenamiento ( $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
) se puede hacer de manera analitica en casos simples, o de manera iterativa. La fase de optimizacion va a &lt;strong&gt;hacer converger los parametros&lt;/strong&gt; para encontrar los que van a dar un &lt;strong&gt;costo minimum en el conjunto de datos de entrenamiento&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;convergence_algo_optim&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu3361748985186866440.webp 400w,
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu3884812293168319734.webp 760w,
               /minerias/4_intro_sl/figures/convergence_algo_optim_hu2615079541688126126.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/convergence_algo_optim_hu3361748985186866440.webp&#34;
               width=&#34;566&#34;
               height=&#34;572&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En este ejemplo se puede ver los parametros $a,b$
 del modelo  $y = a\mathbf{X}+b$ 
 cambiar por cada iteracion, para tener un valor del error (sse; suma residual de cuadrados) que esta diminuando:
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;regression_gif&#34;
           src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/regression_gif.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Porque la valor del costo empirico  $ \frac{1}{n}\sum_{i=1}^n \ell(Y_i, f_\theta(\mathbf{X}_i) )$ 
 es un nombre real positivo, podemos representarlo en un eje, y los parametros con unos otros ejes. Eso se llama el &lt;em&gt;loss landscape&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-la-loss-landscape-parece-a-un-paisaje-con-desnivel-con-el-objetivo-de-encontrar-el-lugar-con-menos-altura&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;LossAlps&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/LossAlps_hu15806339177112344745.webp 400w,
               /minerias/4_intro_sl/figures/LossAlps_hu9473298813042643160.webp 760w,
               /minerias/4_intro_sl/figures/LossAlps_hu7762387643821809596.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/LossAlps_hu15806339177112344745.webp&#34;
               width=&#34;760&#34;
               height=&#34;498&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La &lt;em&gt;loss landscape&lt;/em&gt; parece a un paisaje con desnivel, con el objetivo de encontrar el lugar con menos altura.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El objetivo del algoritmo de optimizacion es de encontrar la &amp;ldquo;ruta&amp;rdquo; para conducir en una &amp;ldquo;valle&amp;rdquo;, que representa un minimum local o global. Este ollo significa que los parametros sean los que dan un error pequeña.&lt;/p&gt;
&lt;h3 id=&#34;gradiente-descendiente&#34;&gt;Gradiente descendiente&lt;/h3&gt;
&lt;p&gt;El gradiente de una función  $\nabla_xf(x)=(\frac{\partial f}{\partial x_i})_{i=1..n}$ 
 es su derivativa según cada dimensión. Es una &lt;strong&gt;aproximación lineal de la función al nivel local&lt;/strong&gt;. Este indica la direccion donde aumenta una funcion:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;gradient_curve1D&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/gradient_curve1D_hu10032121186573210922.webp 400w,
               /minerias/4_intro_sl/figures/gradient_curve1D_hu12987032236723031820.webp 760w,
               /minerias/4_intro_sl/figures/gradient_curve1D_hu8038234561045909677.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/gradient_curve1D_hu10032121186573210922.webp&#34;
               width=&#34;760&#34;
               height=&#34;415&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Por eso, se puede utilizar el gradiente de la funccion de costo para minimizar el costo. Después de cada cálculo de la función de costo  $\ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$ 
, se calcula el gradiente de esta función para actualizar los parámetros $\theta$
:&lt;/p&gt;
$$	\theta \leftarrow \theta - \alpha*\nabla_\theta \ell(Y_i, f_\theta(\mathbf{X}_i) ; \theta)$$&lt;p&gt;La tasa de aprendizaje  $\alpha$ 
 en la ecuacion precedente representa la cantidad de acutalizacion de los parametros. Es importante porque va a influir sobre la convergencia.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-la-tasa-de-aprendizaje-es-crucial-para-el-exito-de-la-fase-de-entrenamiento&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;learningrates&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/learningrates_hu14981630448780851733.webp 400w,
               /minerias/4_intro_sl/figures/learningrates_hu5823504888508698054.webp 760w,
               /minerias/4_intro_sl/figures/learningrates_hu11541866586040789986.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/learningrates_hu14981630448780851733.webp&#34;
               width=&#34;459&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La tasa de aprendizaje es crucial para el exito de la fase de entrenamiento.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En en &lt;em&gt;loss landscape&lt;/em&gt;, se puede representar el modelo durante la optimizacion como un vector moviendo en cada iteracion. Con este vision, la tasa de aprendizaje define mas o menos la &amp;ldquo;velocidad&amp;rdquo; de como se mueve este punto. Por eso, es simple de entender que a veces tiene que ser mas grande y otra veces mas pequeño, por ejemplo para pasar topografias particular del &lt;em&gt;landscape&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hay varios algoritmos de tipo gradiente descendiente para converger, con una mejora aproximacion de la tasa de aprendizaje, o la utilizacion de un momentum para ayudar el modelo
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;OtherOptimizers&#34;
           src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/OtherOptimizers.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;metricas&#34;&gt;Metricas&lt;/h2&gt;
&lt;h3 id=&#34;tipos-de-errores&#34;&gt;Tipos de errores&lt;/h3&gt;
&lt;p&gt;Un clasificador binario debe detectar un evento. A cada prediccion puede tener una prediciccion verdadera o falsa: eso son los True/False Positives/Negatives: True Positive (TP), False Postiive (FP), False Negative (FN), True Negative (TN). Se pueden encontrar dos tipos de errores:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;error&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/error_types_hu818360641458426582.webp 400w,
               /minerias/4_intro_sl/figures/error_types_hu1763655108969077203.webp 760w,
               /minerias/4_intro_sl/figures/error_types_hu8812666687817491822.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/error_types_hu818360641458426582.webp&#34;
               width=&#34;760&#34;
               height=&#34;573&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Con eso se puede crear una matriz de confusion.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-la-matriz-de-confusion-tiene-las-etiquetas-en-un-eje-y-las-predicciones-en-el-otro-se-puede-representar-los-truefalse-postivenegative&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;confusion_matrix&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/confusion_matrix_hu17666632291397020102.webp 400w,
               /minerias/4_intro_sl/figures/confusion_matrix_hu6029062098124639712.webp 760w,
               /minerias/4_intro_sl/figures/confusion_matrix_hu4969514950180408067.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/confusion_matrix_hu17666632291397020102.webp&#34;
               width=&#34;596&#34;
               height=&#34;415&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La matriz de confusion tiene las etiquetas en un eje, y las predicciones en el otro. Se puede representar los True/False Postive/Negative.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Las matrices de confusion pueden abarcar mas de 2 clases:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Se puede cada vez volver a una binaria:
















&lt;figure  id=&#34;figure-la-matriz-de-confusion-tiene-las-etiquetas-en-un-eje-y-las-predicciones-en-el-otro-una-matriz-diagonal-significa-predicciones-perfectas&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;conf_mat_multi&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/conf_mat_multi_hu1455269705468184231.webp 400w,
               /minerias/4_intro_sl/figures/conf_mat_multi_hu7808360544726272494.webp 760w,
               /minerias/4_intro_sl/figures/conf_mat_multi_hu8975441635701126753.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/conf_mat_multi_hu1455269705468184231.webp&#34;
               width=&#34;474&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La matriz de confusion tiene las etiquetas en un eje, y las predicciones en el otro. Una matriz diagonal significa predicciones perfectas.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tipos-de-metricas-y-costo&#34;&gt;Tipos de metricas y costo&lt;/h3&gt;
&lt;p&gt;Usando los True/False Positives/Negatives, se puede calcular varias metricas segundo el tipo de applicacion. Tenemos: el accuracy al nivel general, y el recall, la precision y el F-score al nivel de las clases. Mas informacion &lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;por alla&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cada tipo de error puede tener un costo differente segundo si es importante o no en la applicacion del sistema. Por eso se puede utilizar un matriz de costo, y calcular un costo global del sistema.&lt;/p&gt;
&lt;h3 id=&#34;aggregacion&#34;&gt;Aggregacion&lt;/h3&gt;
&lt;p&gt;Se puede agregar las metricas que son al nivel de clase para obtener un valor general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Macro-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Micro-averaging&lt;/strong&gt;: crear matriz de confusion binaria para cada clase, combinar las matrices y luego evaluar&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weighted-averaging&lt;/strong&gt;: computar métrica para cada clase y luego promediar usando pesos segun el importancia de la clase (nombre de ejemplos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a simple example in python obtained by the &lt;code&gt;sklearn.metrics.classification_report&lt;/code&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;class 0&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;class 2&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;classification_report&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;roc--auc&#34;&gt;ROC &amp;amp; AUC&lt;/h3&gt;
&lt;p&gt;Al momento de la inferencia, nuestro clasificador binario va a dar una probabilidad que un ejemplo sea de la clase positiva. Generalmente, si es superior a $\tau = 0.5$
, significa que el ejemplo es de la clase positiva. Sin embargo, se puede jugar con este umbral $\tau$
.&lt;/p&gt;
&lt;p&gt;El ROC (Receiver Operating Characteristic) es une curva representando la performancia de un clasificador en varias situaciones y que se crea variando el umbral. Para varios valores de el umbra, se calcula la fraccion de verdaderos positivos de los positivos frente a la fraccion de falsos positivos de los negativos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ROC&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/ROC_hu10607619452381477277.webp 400w,
               /minerias/4_intro_sl/figures/ROC_hu3908369485916332207.webp 760w,
               /minerias/4_intro_sl/figures/ROC_hu7705981326579221202.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/ROC_hu10607619452381477277.webp&#34;
               width=&#34;760&#34;
               height=&#34;491&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El Area Under Curve (AUC) permite de obtener una unica valor representando la calidad de la curve. Mas grande significa mejor.&lt;/p&gt;
&lt;h3 id=&#34;regresion&#34;&gt;Regresion&lt;/h3&gt;
&lt;p&gt;Para una regresion, se utilizan metricas que que evaluan las distancias, y si el modelo representa bien la varianza de los datos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Varios calculos de errores&lt;/li&gt;
&lt;li&gt;Coeficiente de determinacion $R^2$
 representa la proporcion de la varianza explicada&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tecnicas-de-evaluacion&#34;&gt;Tecnicas de evaluacion&lt;/h2&gt;
&lt;h3 id=&#34;validacion-cruzada-cross-validation&#34;&gt;Validacion Cruzada (Cross-Validation)&lt;/h3&gt;
&lt;p&gt;Para obtener una mejora estimacion de las performancias del modelo. Para estar seguro de testear sobre cada datos, se puede hacer $V$
 experiencias, cortando el dataset en $V$
 partes, entrenar sobre $V-1$
 y testear sobre $1$
. Es un tipo de bootstrapping con los datos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-separar-en-3-parte-y-hacer-3-entrenamiento-permite-de-hacer-el-test-sobre-todo-el-ensemble-y-tener-una-mejora-estimacion-de-las-performancias-del-modelo&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;cross-val_final&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/cross-val_final_hu16748371362900430386.webp 400w,
               /minerias/4_intro_sl/figures/cross-val_final_hu11491613420033986625.webp 760w,
               /minerias/4_intro_sl/figures/cross-val_final_hu4516641686194685870.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/cross-val_final_hu16748371362900430386.webp&#34;
               width=&#34;760&#34;
               height=&#34;212&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Separar en 3 parte, y hacer 3 entrenamiento permite de hacer el test sobre todo el ensemble y tener una mejora estimacion de las performancias del modelo.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Eso sirve para obtener los hiperparametros optimum, antes de entrenar el model final sobre todos los datos, y estimar las performancias sobre el ensemble de test.&lt;/p&gt;
&lt;h3 id=&#34;conjuntos-de-validacion-y-prueba-holdout&#34;&gt;Conjuntos de validacion y prueba (Holdout)&lt;/h3&gt;
&lt;p&gt;Si es imposible de hacer una validacion cruzada (porque el entrenaimento es largo), se puede crear un set de entrenamiento, de validacion, y de test.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-usar-un-ensemble-de-validacion-permite-de-encontrar-los-hiperparametros-sin-usar-el-test-y-sin-usar-de-validacion-cruzada&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;train_val_test&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/train_val_test_hu13319893936448070572.webp 400w,
               /minerias/4_intro_sl/figures/train_val_test_hu11490916923281978634.webp 760w,
               /minerias/4_intro_sl/figures/train_val_test_hu7094084863204531350.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/train_val_test_hu13319893936448070572.webp&#34;
               width=&#34;760&#34;
               height=&#34;279&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Usar un ensemble de validacion permite de encontrar los hiperparametros sin usar el test, y sin usar de validacion cruzada.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;tamaño-de-la-particion&#34;&gt;Tamaño de la particion&lt;/h3&gt;
&lt;p&gt;Un modelo que es buena usando pocos datos es interesante porque a veces obtener etiquetas puede ser costozo. Generalmente las &lt;strong&gt;perfomancias son mas variables y mas bajas con pocos datos de entrenamiento&lt;/strong&gt;, pero la &lt;strong&gt;evaluacion es mas confiable con mas datos de test&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;learning_curve&#34; srcset=&#34;
               /minerias/4_intro_sl/figures/learning_curve_hu14578813128231917822.webp 400w,
               /minerias/4_intro_sl/figures/learning_curve_hu7895650233809507061.webp 760w,
               /minerias/4_intro_sl/figures/learning_curve_hu16581810329368848848.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/4_intro_sl/figures/learning_curve_hu14578813128231917822.webp&#34;
               width=&#34;760&#34;
               height=&#34;637&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Biases and Fairness</title>
      <link>http://localhost:1313/minerias/5_biases/</link>
      <pubDate>Tue, 26 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/5_biases/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_biasespdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Biases.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Modelos Lineales</title>
      <link>http://localhost:1313/minerias/6_modelos_lin/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/6_modelos_lin/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_modelos_linpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Modelos_Lin.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;p&gt;Esta clase profundiza en los &lt;strong&gt;modelos de clasificación lineal&lt;/strong&gt;, mostrando cómo un &lt;strong&gt;hiperplano&lt;/strong&gt; puede separar ejemplos en un espacio de características. Además, se introduce la &lt;strong&gt;regresión lineal&lt;/strong&gt; como un caso particular de modelos lineales (cuando la variable a predecir es continua) y la regresión logística (para valores binarios o multi-clase). Finalmente, veremos una &lt;strong&gt;introducción al uso de scikit-learn&lt;/strong&gt;, la librería de Python muy popular para Machine Learning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: Puedes enfatizar en las propiedades de la linealidad, la noción de hiperplano, la forma de pasar a modelos más complejos con un truco de aumento del espacio, y rematar con ejemplos en scikit-learn. Invita a la discusión de por qué a veces preferimos estos modelos lineales frente a otros más complejos.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;modelos-de-clasificación&#34;&gt;Modelos de Clasificación&lt;/h2&gt;
&lt;h3 id=&#34;clasificador-lineal&#34;&gt;Clasificador lineal&lt;/h3&gt;
&lt;p&gt;El objetivo de un &lt;strong&gt;clasificador lineal&lt;/strong&gt; es separar el espacio de atributos con un &lt;strong&gt;hiperplano&lt;/strong&gt; de manera que queden los ejemplos de una clase a un lado y los de la(s) otra(s) clase(s) al otro lado.&lt;/p&gt;
&lt;h4 id=&#34;problemas-lineales-vs-no-lineales&#34;&gt;Problemas lineales vs. no lineales&lt;/h4&gt;
&lt;p&gt;En la práctica, muchas fronteras de decisión no son lineales. Sin embargo, &lt;strong&gt;aún podemos aplicar modelos lineales&lt;/strong&gt; si creamos características más elaboradas o hacemos transformaciones adecuadas.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplos-de-problemas-lineales-y-no-lineales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Clasificación lineal y no lineal&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/linear_vs_nonlinear_problems_hu7353041200993864283.webp 400w,
               /minerias/6_modelos_lin/figures/linear_vs_nonlinear_problems_hu17403413559867806974.webp 760w,
               /minerias/6_modelos_lin/figures/linear_vs_nonlinear_problems_hu18016639814542214630.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/linear_vs_nonlinear_problems_hu7353041200993864283.webp&#34;
               width=&#34;709&#34;
               height=&#34;297&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplos de problemas lineales y no lineales
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Una función $f : \mathcal{X}\rightarrow \mathcal{Y}$
 es lineal si $f(\lambda \mathbf{x} + \mathbf{x}&#39;) = \lambda\,f(\mathbf{x}) + f(\mathbf{x}&#39;)$
.&lt;/li&gt;
&lt;li&gt;Un caso típico es $f(\mathbf{x}) = \theta^T \mathbf{x} = \sum_i \theta_i\,x_i$
.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;hiperplano-geometría&#34;&gt;Hiperplano (geometría)&lt;/h4&gt;
$$
w_1\,x_1 \;+\; w_2\,x_2 \;+\;\dots\;+\; w_d\,x_d \;+\; w_0 \;=\; 0.
$$&lt;p&gt;&lt;strong&gt;Interpretación&lt;/strong&gt;: el signo de $w_1 x_1 + \dots + w_d x_d + w_0$
 indica de qué lado del hiperplano se encuentra $\mathbf{x}$
.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-en-2d-es-una-recta-en-3d-es-un-plano&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Representación de un hiperplano en 2D y 3D&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/affine_hu14528710492767468521.webp 400w,
               /minerias/6_modelos_lin/figures/affine_hu15839829693383708641.webp 760w,
               /minerias/6_modelos_lin/figures/affine_hu14583323748885349885.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/affine_hu14528710492767468521.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      En 2D es una recta, en 3D es un plano
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: Recordar que, en 2D, se llama recta; en 3D, se llama plano; y en más dimensiones, se sigue llamando hiperplano.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-la-normal-define-la-orientación-del-hiperplano&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Representación del plano mediante la normal&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/plan_normale_hu15260604472700012555.webp 400w,
               /minerias/6_modelos_lin/figures/plan_normale_hu6943290207337226051.webp 760w,
               /minerias/6_modelos_lin/figures/plan_normale_hu9804219792372522524.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/plan_normale_hu15260604472700012555.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La normal define la orientación del hiperplano
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En este diagrama vemos cómo la &lt;strong&gt;normal&lt;/strong&gt; ($\vec{n}$
) al hiperplano define su orientación. El término $w_0$
 (o sesgo) desplaza el plano.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;producto-escalar-y-parte-afín&#34;&gt;Producto escalar y parte afín&lt;/h4&gt;
&lt;p&gt;Si &lt;strong&gt;aumentamos el espacio&lt;/strong&gt; añadiendo un 1 a nuestro vector de características:
$
\mathbf{x} \;=\;
\begin{pmatrix}
x_1\\
\vdots\\
x_d\\
1
\end{pmatrix},\quad
\theta \;=\;
\begin{pmatrix}
\theta_1\\
\vdots\\
\theta_d\\
\theta_0
\end{pmatrix},
$

entonces $\theta^T \mathbf{x} = w_0 + \sum_i w_i\,x_i$
. Esto permite manejar en un mismo marco la parte “afín” del hiperplano.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Representación general de un hiperplano afín&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/geom2D-plan_hu3378174178292732278.webp 400w,
               /minerias/6_modelos_lin/figures/geom2D-plan_hu1534089568248987208.webp 760w,
               /minerias/6_modelos_lin/figures/geom2D-plan_hu17731144349146713873.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/geom2D-plan_hu3378174178292732278.webp&#34;
               width=&#34;482&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;resumen&#34;&gt;Resumen&lt;/h4&gt;
&lt;p&gt;El &lt;strong&gt;clasificador lineal&lt;/strong&gt; más sencillo se define como:&lt;/p&gt;
$$
f_{\mathbf{W}, b}(\mathbf{x}) \;=\;
\begin{cases}
+1, &amp; \text{si } (\mathbf{W}^T\,\mathbf{x} + b)\;\ge\;0, \\
-1, &amp; \text{en caso contrario}.
\end{cases}
$$&lt;p&gt;O, en problemas multiclase, usamos la misma idea (hiperplano para cada clase) y elegimos la que tenga la salida más alta (argmax).&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-podemos-ver-la-separación-con-una-superficie-lineal-en-el-espacio-transformado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de separación lineal en 3D (SVM no lineal)&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/SVM_non_lin_3D_hu16934183206793690649.webp 400w,
               /minerias/6_modelos_lin/figures/SVM_non_lin_3D_hu12349553245862043253.webp 760w,
               /minerias/6_modelos_lin/figures/SVM_non_lin_3D_hu15825121860496258888.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/SVM_non_lin_3D_hu16934183206793690649.webp&#34;
               width=&#34;760&#34;
               height=&#34;703&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Podemos ver la separación con una superficie lineal en el espacio transformado.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: Recalca cómo, si no es separable linealmente, podemos usar un truco de “features” que nos lleven a un espacio donde sí lo sea.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-aquí-el-hiperplano-es-una-simple-recta-2d&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Separando puntos con un hiperplano en 2D&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/linsep_new_hu14997818569280874164.webp 400w,
               /minerias/6_modelos_lin/figures/linsep_new_hu11636099713454292706.webp 760w,
               /minerias/6_modelos_lin/figures/linsep_new_hu16742827150983731599.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/linsep_new_hu14997818569280874164.webp&#34;
               width=&#34;760&#34;
               height=&#34;364&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aquí el hiperplano es una simple recta 2D
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-un-hiperplano-en-d-dimensiones&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Representación más genérica en un espacio N-dimensional&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/Linear_Classifier_space_hu15836063779108037436.webp 400w,
               /minerias/6_modelos_lin/figures/Linear_Classifier_space_hu7924874973121578882.webp 760w,
               /minerias/6_modelos_lin/figures/Linear_Classifier_space_hu4361361228696633468.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/Linear_Classifier_space_hu15836063779108037436.webp&#34;
               width=&#34;706&#34;
               height=&#34;518&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Un hiperplano en d dimensiones
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;clasificador-lineal-multinomial&#34;&gt;Clasificador lineal multinomial&lt;/h3&gt;
&lt;p&gt;Para más de 2 clases, podemos tener una matriz $\mathbf{W}$
, donde cada fila corresponde a un posible “hiperplano” que da una puntuación a la clase. Luego:&lt;/p&gt;
$$
\hat{c}(\mathbf{x})
\;=\;
\arg\max_{c}\;\bigl(\mathbf{W}_c^T\,\mathbf{x} + b_c\bigr).
$$&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Clasificador lineal multiclase como operación matricial&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/Linear_Classification_hu12810854765152666704.webp 400w,
               /minerias/6_modelos_lin/figures/Linear_Classification_hu8176628540058915532.webp 760w,
               /minerias/6_modelos_lin/figures/Linear_Classification_hu11231406798243868969.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/Linear_Classification_hu12810854765152666704.webp&#34;
               width=&#34;760&#34;
               height=&#34;641&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: destacar que es una operación muy rápida, pues es un producto matriz-vector.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;integración-del-sesgo&#34;&gt;Integración del sesgo&lt;/h3&gt;
&lt;p&gt;En vez de $\mathbf{W}^T\mathbf{x} + b$
, se pasa a un producto escalar extendido:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-se-añade-el-1-al-vector-de-descriptores-y-el-bias-a-los-pesos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Bias Trick&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/Bias_Trick_hu7434524410363380459.webp 400w,
               /minerias/6_modelos_lin/figures/Bias_Trick_hu14795603539966083441.webp 760w,
               /minerias/6_modelos_lin/figures/Bias_Trick_hu3757125100915727117.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/Bias_Trick_hu7434524410363380459.webp&#34;
               width=&#34;760&#34;
               height=&#34;277&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Se añade el 1 al vector de descriptores y el bias a los pesos
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;aumento-del-espacio-para-no-linealidad&#34;&gt;Aumento del espacio para no linealidad&lt;/h3&gt;
&lt;p&gt;Se puede &lt;strong&gt;agregar variables no lineales&lt;/strong&gt; (por ejemplo $z = x^2 + y^2$
) para separar datos no linealmente separables en la dimensión original. La separación sigue siendo &lt;em&gt;lineal&lt;/em&gt; en el espacio de mayor dimensión.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;De 2D a 3D para volver lineal lo no lineal&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/data_2d_to_3d_hu6940047956663229860.webp 400w,
               /minerias/6_modelos_lin/figures/data_2d_to_3d_hu8538703031235704932.webp 760w,
               /minerias/6_modelos_lin/figures/data_2d_to_3d_hu13469008302112689616.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/data_2d_to_3d_hu6940047956663229860.webp&#34;
               width=&#34;760&#34;
               height=&#34;355&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;regresión-lineal&#34;&gt;Regresión Lineal&lt;/h2&gt;
&lt;p&gt;La &lt;strong&gt;Regresión Lineal&lt;/strong&gt; es un modelo lineal para predecir una variable continua $y$
. En su forma más simple en 1D:&lt;/p&gt;
$$
\hat{y} \;=\; w\,x \;+\; b.
$$&lt;p&gt;O en multidimensional:&lt;/p&gt;
$$
\hat{y} \;=\; \mathbf{W}^T\,\mathbf{x} \;+\; b.
$$&lt;p&gt;















&lt;figure  id=&#34;figure-en-lugar-de-clasificar-en-un-lado-u-otro-medimos-distancias-a-la-línea&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Regresión lineal vs. clasificación lineal&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/reg_lin_hu2428590659555122744.webp 400w,
               /minerias/6_modelos_lin/figures/reg_lin_hu1609359113409240423.webp 760w,
               /minerias/6_modelos_lin/figures/reg_lin_hu16163012276799070061.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/reg_lin_hu2428590659555122744.webp&#34;
               width=&#34;386&#34;
               height=&#34;266&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      En lugar de clasificar en un lado u otro, medimos distancias a la línea
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;métricas-en-regresión&#34;&gt;Métricas en regresión&lt;/h3&gt;
&lt;p&gt;Para estimar la calidad de la predicción:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Error Medio Absoluto: $\frac{1}{n}\sum \bigl|Y_i - f(\mathbf{X}_i)\bigr|$
&lt;/li&gt;
&lt;li&gt;Error Cuadrático Medio: $\frac{1}{n}\sum \bigl(Y_i - f(\mathbf{X}_i)\bigr)^2$
&lt;/li&gt;
&lt;li&gt;Coeficiente de determinación $R^2$
, que mide cuánta varianza de $Y$
 explica el modelo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de error cuadrático para la regresión lineal&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/reg_lineaire_loss_hu13269445812579728799.webp 400w,
               /minerias/6_modelos_lin/figures/reg_lineaire_loss_hu2278288848151013165.webp 760w,
               /minerias/6_modelos_lin/figures/reg_lineaire_loss_hu18217655019017418819.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/reg_lineaire_loss_hu13269445812579728799.webp&#34;
               width=&#34;640&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: Explica la interpretación de $R^2$
 como fracción de varianza explicada.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;regresión-de-un-plano-en-3d&#34;&gt;Regresión de un plano en 3D&lt;/h3&gt;
&lt;p&gt;Para dos atributos $x_1, x_2$
 y una salida $y$
:&lt;/p&gt;
$$
\hat{y} \;=\; w_1\,x_1 \;+\; w_2\,x_2 \;+\; b.
$$&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Un plano que ajusta los datos en 3D&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/reg_plane_hu11375769634492767005.webp 400w,
               /minerias/6_modelos_lin/figures/reg_plane_hu13355732907300323387.webp 760w,
               /minerias/6_modelos_lin/figures/reg_plane_hu17520921710119118135.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/reg_plane_hu11375769634492767005.webp&#34;
               width=&#34;697&#34;
               height=&#34;399&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;regresión-logística&#34;&gt;Regresión Logística&lt;/h2&gt;
&lt;p&gt;La &lt;strong&gt;regresión logística&lt;/strong&gt; (en su forma binaria) se utiliza para convertir una &lt;strong&gt;salida lineal&lt;/strong&gt; (o “distancia” en el espacio de características) en una &lt;strong&gt;probabilidad&lt;/strong&gt; entre 0 y 1. Además, en el caso &lt;strong&gt;multiclase&lt;/strong&gt;, se generaliza a la llamada &lt;strong&gt;función softmax&lt;/strong&gt;, la cual asigna una probabilidad a cada clase, de modo que la suma de todas las probabilidades es igual a 1.&lt;/p&gt;
&lt;h3 id=&#34;caso-binario&#34;&gt;Caso Binario&lt;/h3&gt;
&lt;p&gt;ara problemas de clasificación binaria, la &lt;strong&gt;regresión logística&lt;/strong&gt; aplica la función sigmoide o &lt;em&gt;softmax&lt;/em&gt; sobre la salida lineal. La idea principal es definir una &lt;strong&gt;función sigmoide&lt;/strong&gt; que proyecte cualquier valor real (la salida lineal $\mathbf{W}^T \mathbf{x} + b$
) a un rango de $]0, 1[$
:&lt;/p&gt;
$$
\sigma(z) \;=\;
\frac{1}{1 + e^{-z}}
$$&lt;ul&gt;
&lt;li&gt;Cuando $z \rightarrow +\infty$
, la sigmoide se acerca a 1.&lt;/li&gt;
&lt;li&gt;Cuando $z \rightarrow -\infty$
, la sigmoide se acerca a 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Función sigmoide&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/sigmo_hu14640827423314996410.webp 400w,
               /minerias/6_modelos_lin/figures/sigmo_hu14976402490176052729.webp 760w,
               /minerias/6_modelos_lin/figures/sigmo_hu4385362737546358557.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/sigmo_hu14640827423314996410.webp&#34;
               width=&#34;760&#34;
               height=&#34;497&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En &lt;strong&gt;regresión logística binaria&lt;/strong&gt; (dos clases), llamemos “1” a la clase positiva y “0” (o “-1”) a la negativa. Entonces la probabilidad de que un ejemplo $\mathbf{x}$
 sea de la clase positiva es:&lt;/p&gt;
$$
P(Y = 1 \;|\; \mathbf{x})
\;=\;
\sigma\bigl(\mathbf{W}^T\mathbf{x} + b\bigr)
\;=\;
\frac{1}{1 + e^{-(\mathbf{W}^T\mathbf{x} + b)}}.
$$&lt;p&gt;Por ende, $P(Y=0 \;|\; \mathbf{x}) = 1 - P(Y=1 \;|\; \mathbf{x})$
. Esto nos da una probabilidad. Para la clase final, elegimos la etiqueta según $P(Y=1) &gt; 0.5$
 (u otro umbral). En el caso binario equivale a comparar si la salida lineal es mayor o menor que 0:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Si $\mathbf{W}^T\mathbf{x} + b &gt; 0$
, predice “1”.&lt;/li&gt;
&lt;li&gt;Si $\mathbf{W}^T\mathbf{x} + b &lt; 0$
, predice “0” (o “-1”).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;conexión-distancia-probabilidad&#34;&gt;Conexión distancia-probabilidad&lt;/h4&gt;
&lt;p&gt;En modelos lineales clásicos (p.e., SVM), la salida $\mathbf{W}^T\mathbf{x} + b$
 indica una &lt;strong&gt;distancia&lt;/strong&gt; (o margen) respecto al hiperplano. Para convertir dicha cantidad en una &lt;strong&gt;probabilidad&lt;/strong&gt;, aplicamos la función sigmoide, que comprime valores reales (infinitos en ambos extremos) a un rango de 0 a 1.&lt;/p&gt;
&lt;h3 id=&#34;caso-multiclase-softmax&#34;&gt;Caso Multiclase: Softmax&lt;/h3&gt;
&lt;p&gt;Para &lt;strong&gt;$C$
 clases&lt;/strong&gt;, generalizamos la función logística a &lt;strong&gt;softmax&lt;/strong&gt;, asignando parámetros $\theta^{(1)}, \dots, \theta^{(C)}$
 (un vector por clase) y obteniendo probabilidades que suman 1:&lt;/p&gt;
$$
P(Y = c)
\;=\;
\frac{\exp^{&lt;\theta^{(c)} \mid \mathbf{X}&gt;}}
{\sum_{j=1}^{C}
\exp^{&lt;\theta^{(j)} \mid \mathbf{X}&gt;}}
\quad,\quad
c=1,\dots,C.
$$&lt;p&gt;donde:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta^{(c)}$
 son los parámetros asociados a la clase $c$
.&lt;/li&gt;
&lt;li&gt;La &lt;strong&gt;suma de probabilidades&lt;/strong&gt; sobre las $C$
 clases es igual a 1 (gracias al denominador).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La clase final se predice con la regla:&lt;/p&gt;
$$
\hat{c}(\mathbf{X})
\;=\;
\arg\max_{1 \le c \le C} 
\,P(Y=c).
$$&lt;p&gt;Es habitual agrupar todos los vectores $\theta^{(c)}$
 en una misma matriz:&lt;/p&gt;
$$
\theta \;=\;
\begin{pmatrix}
\vertbar &amp; \vertbar &amp; \cdots &amp; \vertbar \\
\theta^{(1)} &amp; \theta^{(2)} &amp; \dots &amp; \theta^{(C)} \\
\vertbar &amp; \vertbar &amp; \cdots &amp; \vertbar
\end{pmatrix}.
$$&lt;p&gt;&lt;em&gt;(Cada columna es un vector de parámetros para la clase correspondiente.)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;uso-de-scikit-learn&#34;&gt;Uso de Scikit-learn&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt; (&lt;code&gt;sklearn&lt;/code&gt;) es una biblioteca de Python que nos permite entrenar y probar modelos de Machine Learning con funciones normalizadas.&lt;/p&gt;
&lt;h3 id=&#34;funciones-generales&#34;&gt;Funciones generales&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-se-separan-datos-se-entrena-y-se-valida&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Flujo típico de aprendizaje supervisado con sklearn&#34; srcset=&#34;
               /minerias/6_modelos_lin/figures/supervised_scikit_learn_hu17372726708710164390.webp 400w,
               /minerias/6_modelos_lin/figures/supervised_scikit_learn_hu13489451897173136613.webp 760w,
               /minerias/6_modelos_lin/figures/supervised_scikit_learn_hu5671300666085423450.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/6_modelos_lin/figures/supervised_scikit_learn_hu17372726708710164390.webp&#34;
               width=&#34;659&#34;
               height=&#34;484&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Se separan datos, se entrena y se valida
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En &lt;code&gt;sklearn&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_iris&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SGDClassifier&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_iris&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_test&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_test_split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                        &lt;span class=&#34;n&#34;&gt;test_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SGDClassifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tol&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;(Para el orador: Subraya la facilidad de implementación y las funciones de utilidad para preprocesar, hacer feature extraction, validación cruzada, etc.)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;conjuntos-de-datos-extracción-de-características-y-preprocesamiento&#34;&gt;Conjuntos de datos, extracción de características y preprocesamiento&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Múltiples datasets de juguete: Iris, Digits, Wine, etc.&lt;/li&gt;
&lt;li&gt;Conjuntos más grandes: RCV1 (texto), Faces (imágenes), etc.&lt;/li&gt;
&lt;li&gt;Módulos como &lt;code&gt;feature_extraction&lt;/code&gt;, &lt;code&gt;feature_selection&lt;/code&gt;, &lt;code&gt;preprocessing&lt;/code&gt; para:&lt;/li&gt;
&lt;li&gt;Vectorizar texto
&lt;ul&gt;
&lt;li&gt;Seleccionar atributos relevantes&lt;/li&gt;
&lt;li&gt;Normalizar los datos&lt;/li&gt;
&lt;li&gt;Rellenar valores faltantes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: Invita a explorar la documentación sklearn.org y mostrar un ejemplo interactivo si hay tiempo.)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;validación-cruzada-y-búsqueda-de-hiperparámetros&#34;&gt;Validación cruzada y búsqueda de hiperparámetros&lt;/h3&gt;
&lt;p&gt;Incluye herramientas como:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KFold&lt;/code&gt;, &lt;code&gt;StratifiedKFold&lt;/code&gt; para dividir los datos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cross_val_score&lt;/code&gt; para evaluar un modelo en K particiones&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GridSearchCV&lt;/code&gt; o &lt;code&gt;RandomizedSearchCV&lt;/code&gt; para probar múltiples parámetros y hacer validación cruzada&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GridSearchCV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;svc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;svm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SVC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;scale&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;svc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;clf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;(Para el orador: Resaltar la importancia de la validación cruzada para evitar sobreajustes y encontrar buenos hiperparámetros.)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;conclusión&#34;&gt;Conclusión&lt;/h3&gt;
&lt;p&gt;En esta clase hemos visto:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cómo un clasificador lineal separa el espacio con un hiperplano.&lt;/li&gt;
&lt;li&gt;La regresión lineal como caso de modelo lineal para predicción de variables continuas.&lt;/li&gt;
&lt;li&gt;Cómo extender la linealidad introduciendo variables polinómicas o el truco del Bias.&lt;/li&gt;
&lt;li&gt;Un breve vistazo a la regresión logística, esencial para clasificación binaria y multi-clase con softmax.&lt;/li&gt;
&lt;li&gt;scikit-learn y sus funcionalidades para datasets, entrenamiento, validación y selección de hiperparámetros.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Para el orador: concluye enfatizando que, aunque hay modelos más complejos como redes neuronales o ensembles, entender los modelos lineales es clave para la práctica de Machine Learning y la interpretación de resultados. ¡Hasta la próxima clase!)&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Mineria de Datos</title>
      <link>http://localhost:1313/teaching/minerias/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/minerias/</guid>
      <description>&lt;h3 id=&#34;all-the-different-classes-can-be-found-hereminerias&#34;&gt;All the different classes can be found &lt;a href=&#34;../../minerias&#34;&gt;here&lt;/a&gt;!&lt;/h3&gt;
&lt;p&gt;This is the CC5205 course from the Universidad de Chile. I restructured it so that it is more adapted to nowadays techniques and more machine learning oriented, it is heavily based on &lt;a href=&#34;https://scikit-learn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a summary:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;General introduction&lt;/strong&gt;: Definitions of Data Mining, Data Science, and content of the class&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data I&lt;/strong&gt;: (Un)structured data, Representation, Normalization, Noise removal, &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Data II&lt;/strong&gt;: Basic statistics for data exploration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intro to Supervised Learning&lt;/strong&gt;: Basics of Machine Learning and supervised learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intro to Fairness and Biases&lt;/strong&gt;: How to avoid making bad models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear Models&lt;/strong&gt;: A very simple model, which is the base of deep neural networks!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Classifiers&lt;/strong&gt;: KNN, Naive Bayes, Decision Tree, Boosting, Bagging, Random Forests.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Dimensionality Reduction&lt;/strong&gt;: Principal Component Analysis, Independant Component Analysis, t-SNE, UMAP,&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Clustering methods&lt;/strong&gt;: Clustering methods and associated metrics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVM, SVR&lt;/strong&gt;: Hinge loss, Lagrangian, KKT conditions, non-linear SVM, Kernel trick, SV Regressor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Neural Nets&lt;/strong&gt;: Basics of Deep Learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to NLP&lt;/strong&gt; (Invited Speaker: Juan Jose Alegria): How to deal with natural language.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>http://localhost:1313/deep/6_regularization/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/6_regularization/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslides6_regularizationpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/6_Regularization.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Clasificadores</title>
      <link>http://localhost:1313/minerias/7_clasificadores/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/7_clasificadores/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_modelos_slpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Modelos_SL.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>SVM</title>
      <link>http://localhost:1313/minerias/8_svm/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/8_svm/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_svmpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_SVM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;h2 id=&#34;máquinas-de-vectores-de-soporte-svm&#34;&gt;Máquinas de Vectores de Soporte (SVM)&lt;/h2&gt;
&lt;p&gt;Las &lt;strong&gt;Máquinas de Vectores de Soporte&lt;/strong&gt; son modelos de clasificación (o regresión) que buscan encontrar un &lt;strong&gt;hiperplano&lt;/strong&gt; o frontera de decisión que &lt;strong&gt;maximice el margen&lt;/strong&gt; entre las clases. A continuación, veremos sus conceptos clave, la formulación lineal, el uso de kernels y, finalmente, la extensión a la regresión (SVR).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;separadores-de-gran-margen&#34;&gt;Separadores de Gran Margen&lt;/h3&gt;
&lt;p&gt;El objetivo de las SVM es encontrar un &lt;strong&gt;separador&lt;/strong&gt; (en dimensión \(d\), un hiperplano) que no solo divida correctamente las clases, sino que lo haga maximizando la distancia mínima con cualquier punto de entrenamiento.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-svm-que-logra-una-separación-no-lineal&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de frontera no lineal buscada por SVM&#34; srcset=&#34;
               /minerias/8_svm/figures/SVM_nonlin_hu10766200129307954814.webp 400w,
               /minerias/8_svm/figures/SVM_nonlin_hu940999748329515222.webp 760w,
               /minerias/8_svm/figures/SVM_nonlin_hu10800116435089468750.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/SVM_nonlin_hu10766200129307954814.webp&#34;
               width=&#34;760&#34;
               height=&#34;307&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SVM que logra una separación no lineal
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En casos lineales y perfectamente separables, puede haber varios hiperplanos que distingan las clases. La pregunta es: &lt;strong&gt;¿Cuál elegir?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-dos-posibles-hiperplanos-separadores&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distintos hiperplanos posibles&#34; srcset=&#34;
               /minerias/8_svm/figures/svm_which_hyperplan2_hu6991979370543567446.webp 400w,
               /minerias/8_svm/figures/svm_which_hyperplan2_hu671267611003501627.webp 760w,
               /minerias/8_svm/figures/svm_which_hyperplan2_hu14002891887294419535.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svm_which_hyperplan2_hu6991979370543567446.webp&#34;
               width=&#34;478&#34;
               height=&#34;410&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dos posibles hiperplanos separadores
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;La SVM opta por el que maximiza el margen, buscando así una mejor &lt;strong&gt;capacidad de generalización&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;clasificador-lineal-recordatorio&#34;&gt;Clasificador lineal: recordatorio&lt;/h3&gt;
&lt;p&gt;Un clasificador lineal en \(\mathbb{R}^d\) se define por:&lt;/p&gt;
\[
f(\mathbf{X}) = \mathrm{signo}(\mathbf{W}^\top \mathbf{X} + b).
\]&lt;p&gt;En la siguiente figura se ilustra una separación lineal (en 3D) que se proyecta a una separación curva al volver al espacio 2D:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-vista-en-3d-de-datos-originalmente-no-separables-linealmente&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Espacio 3D para separar datos no lineales&#34; srcset=&#34;
               /minerias/8_svm/figures/SVM_non_lin_3D_hu2697375277477187509.webp 400w,
               /minerias/8_svm/figures/SVM_non_lin_3D_hu14481105654133733525.webp 760w,
               /minerias/8_svm/figures/SVM_non_lin_3D_hu17332218457834828460.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/SVM_non_lin_3D_hu2697375277477187509.webp&#34;
               width=&#34;760&#34;
               height=&#34;703&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Vista en 3D de datos originalmente no separables linealmente
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;svm-lineal-márgenes-y-restricción-de-separabilidad&#34;&gt;SVM lineal: márgenes y restricción de separabilidad&lt;/h3&gt;
&lt;p&gt;El hiperplano \(\mathbf{W}^\top \mathbf{X} + b = 0\) y los planos paralelos \(\mathbf{W}^\top \mathbf{X} + b = \pm 1\) marcan el &lt;strong&gt;margen&lt;/strong&gt;. Cuanto mayor sea la distancia entre esos planos, mayor será la robustez.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-hiperplano-central-y-márgenes-en-1&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Márgenes alrededor del hiperplano&#34; srcset=&#34;
               /minerias/8_svm/figures/SVM_hu17965602084783293788.webp 400w,
               /minerias/8_svm/figures/SVM_hu114460994655993461.webp 760w,
               /minerias/8_svm/figures/SVM_hu14476676033574681305.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/SVM_hu17965602084783293788.webp&#34;
               width=&#34;760&#34;
               height=&#34;534&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Hiperplano central y márgenes en ±1
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Margen geométrico&lt;/strong&gt;:
Consideremos los planos de decisión \(\mathbf{W}^\top \mathbf{X} + b = \pm 1\). La distancia entre esos dos planos es:&lt;/p&gt;
\[
     \frac{2}{\|\mathbf{W}\|}.
   \]&lt;p&gt;Por ello, &lt;strong&gt;maximizar&lt;/strong&gt; esa distancia es &lt;strong&gt;equivalente&lt;/strong&gt; a &lt;strong&gt;minimizar&lt;/strong&gt; \(\|\mathbf{W}\|\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forma habitual en SVM&lt;/strong&gt;:&lt;br&gt;
Para comodidad numérica, se minimiza \(\tfrac{1}{2}\|\mathbf{W}\|^2\) en lugar de \(\|\mathbf{W}\|\), pero el criterio es el mismo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Para datos &lt;strong&gt;(casi) separables&lt;/strong&gt;, la SVM busca:&lt;/p&gt;
\[
\begin{aligned}
&amp; \min_{\mathbf{W}, b} \quad \frac{1}{2}\|\mathbf{W}\|^2 \\
&amp; \text{sujeto a } \quad 
Y_i \, (\mathbf{W}^\top \mathbf{X}_i + b)\; \ge 1,\quad \forall i.
\end{aligned}
\]&lt;p&gt;En suma, la restricción \(Y_i(\mathbf{W}^\top \mathbf{X}_i + b)\ge 1\) asegura que cada punto esté al menos a distancia \(1/\|\mathbf{W}\|\) del hiperplano, y al reducir \(\|\mathbf{W}\|\) aumentamos este margen.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;formulación-primal-y-dual&#34;&gt;Formulación primal y dual&lt;/h3&gt;
&lt;p&gt;Los problemas primal y dual son equivalentes en entornos convexos.&lt;/p&gt;
&lt;p&gt;El problema Primal:&lt;/p&gt;
\[
\begin{aligned}
&amp;\min \quad z = c^t x, \\
&amp;\text{subject to} \quad A x \ge b, \\
&amp;\qquad\quad\;\; x \ge 0.
\end{aligned}
\]&lt;p&gt;El problema Dual:&lt;/p&gt;
\[
\begin{aligned}
&amp;\max \quad z = b^t y, \\
&amp;\text{subject to} \quad A^t y \le c, \\
&amp;\qquad\quad\;\; y \ge 0,
\end{aligned}
\]&lt;p&gt;En la práctica se implementan algoritmos para el &lt;strong&gt;problema dual&lt;/strong&gt;, sobre todo cuando se utilizan kernels.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-un-problema-primal&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interpretación visual primal-dual (izq/primal, der/dual)&#34; srcset=&#34;
               /minerias/8_svm/figures/primal_hu5214921791707174292.webp 400w,
               /minerias/8_svm/figures/primal_hu6218951539569108450.webp 760w,
               /minerias/8_svm/figures/primal_hu6179495431827664383.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/primal_hu5214921791707174292.webp&#34;
               width=&#34;581&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Un problema Primal
    &lt;/figcaption&gt;&lt;/figure&gt;
 















&lt;figure  id=&#34;figure-un-problema-dual&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interpretación visual primal-dual (izq/primal, der/dual)&#34; srcset=&#34;
               /minerias/8_svm/figures/dual_hu9024437098989026950.webp 400w,
               /minerias/8_svm/figures/dual_hu11370458516146254517.webp 760w,
               /minerias/8_svm/figures/dual_hu15067324994502624637.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/dual_hu9024437098989026950.webp&#34;
               width=&#34;581&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Un problema Dual
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Para resolverlo, se introduce el &lt;strong&gt;Lagrangiano&lt;/strong&gt; y se pasa a la &lt;strong&gt;formulación dual&lt;/strong&gt;. De este modo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Primal&lt;/strong&gt;: parámetros \(\mathbf{W}, b\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dual&lt;/strong&gt;: multiplicadores \(\alpha_i\).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;introducción-del-lagrangiano-y-condiciones-kkt&#34;&gt;Introducción del Lagrangiano y condiciones KKT&lt;/h3&gt;
\[
\min_{\mathbf{W},b} \quad \frac{1}{2}\|\mathbf{W}\|^2 
\quad\text{sujeto a}\quad Y_i(\mathbf{W}^\top\mathbf{X}_i + b)\;\ge\;1,
\]&lt;p&gt;
se introduce el &lt;strong&gt;Lagrangiano&lt;/strong&gt;:&lt;/p&gt;
\[
\mathcal{L}(\mathbf{W}, b, \boldsymbol{\alpha}) 
\;=\; \tfrac{1}{2}\|\mathbf{W}\|^2 
\;-\; \sum_{i=1}^n \alpha_i \,\bigl(Y_i(\mathbf{W}^\top \mathbf{X}_i + b) - 1\bigr),
\]&lt;p&gt;
donde \(\alpha_i \ge 0\) son los &lt;strong&gt;multiplicadores de Lagrange&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Las &lt;strong&gt;Condiciones de Karush-Kuhn-Tucker (KKT)&lt;/strong&gt; aplicadas a este problema especifican, entre otras, que:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\nabla_{\mathbf{W}} \,\mathcal{L} = 0\) y \(\nabla_b \,\mathcal{L} = 0\) (estacionaridad).&lt;/li&gt;
&lt;li&gt;\(\alpha_i \ge 0\).&lt;/li&gt;
&lt;li&gt;\(\alpha_i \,\bigl[Y_i(\mathbf{W}^\top \mathbf{X}_i + b)-1\bigr] = 0\) (complementariedad):&lt;br&gt;
esto implica que para cada \(i\), o bien la restricción se cumple con margen (estricto) o \(\alpha_i=0\).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;resolviendo-el-problema-dual-y-aparición-de-alpha_i&#34;&gt;Resolviendo el problema Dual y aparición de \(\alpha_i\)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Construcción del Dual&lt;/strong&gt;:&lt;br&gt;
Se reemplaza \(\|\mathbf{W}\|^2\) y se resuelve en función de \(\boldsymbol{\alpha}\). El &lt;strong&gt;problema dual&lt;/strong&gt; pasa a ser:
\[
   \begin{aligned}
   &amp;\max_{\boldsymbol{\alpha}} \quad 
     \sum_{i=1}^n \alpha_i \;-\; \tfrac{1}{2}\sum_{i,j} \alpha_i \alpha_j \,Y_i Y_j \,\langle \mathbf{X}_i,\mathbf{X}_j\rangle,\\
   &amp;\text{sujeto a}\quad \alpha_i \ge 0,\;\; \sum_{i=1}^n \alpha_i Y_i = 0.
   \end{aligned}
   \]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solución para \(\mathbf{W}\)&lt;/strong&gt;:&lt;br&gt;
De las KKT, se deduce que
\[
     \mathbf{W} \;=\; \sum_{i=1}^n \alpha_i \,Y_i \,\mathbf{X}_i.
   \]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clasificador&lt;/strong&gt;:&lt;br&gt;
\[
     f(\mathbf{X}) \;=\; \mathrm{signo}\!\Bigl(\sum_{i=1}^n \alpha_i\,Y_i\,\langle\mathbf{X}_i,\mathbf{X}\rangle + b \Bigr).
   \]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notar que en este punto aún &lt;strong&gt;no&lt;/strong&gt; hemos introducido el concepto de &lt;strong&gt;soft margin&lt;/strong&gt;. Cuando todos los datos son separables y no hay ruido, cada punto cumple la restricción sin violarla. Una vez introducidos errores o ruido, pasaremos a la versión &lt;em&gt;soft margin&lt;/em&gt;.
mal-dual (izq/primal, der/dual)](figures/dual.png &amp;ldquo;Un problema Dual&amp;rdquo;)&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;vectores-de-soporte&#34;&gt;Vectores de Soporte&lt;/h3&gt;
\[
\mathbf{W} = \sum_{i=1}^{n} \alpha_i \, Y_i \, \mathbf{X}_i
\]\[
f(\mathbf{X}) = \mathrm{signo}\!\Big(\sum_{i=1}^{n} \alpha_i \,Y_i\;\langle \mathbf{X}_i,\mathbf{X}\rangle + b\Big).
\]&lt;p&gt;Solo los puntos con \(\alpha_i \neq 0\) se llaman &lt;strong&gt;vectores de soporte&lt;/strong&gt;, aquellos que “soportan” el margen.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-los-vectores-de-soporte-definen-el-hiperplano-final&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Puntos soportando el margen&#34; srcset=&#34;
               /minerias/8_svm/figures/svm_support_hu9573517152434623081.webp 400w,
               /minerias/8_svm/figures/svm_support_hu3030785379930506308.webp 760w,
               /minerias/8_svm/figures/svm_support_hu13275145611349553827.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svm_support_hu9573517152434623081.webp&#34;
               width=&#34;760&#34;
               height=&#34;667&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los vectores de soporte definen el hiperplano final
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;soft-margin-y-ruido&#34;&gt;Soft margin y ruido&lt;/h3&gt;
&lt;p&gt;Cuando hay ruido, el plan optimal no es necesariamente el mejor:
















&lt;figure  id=&#34;figure-ejemplo-de-hiperplanos-con-ruido-y-errores&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo con ruido; margen distinto según C&#34; srcset=&#34;
               /minerias/8_svm/figures/SVM_bruit_hu15490994747521239547.webp 400w,
               /minerias/8_svm/figures/SVM_bruit_hu9950590124400274101.webp 760w,
               /minerias/8_svm/figures/SVM_bruit_hu4310381433717407342.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/SVM_bruit_hu15490994747521239547.webp&#34;
               width=&#34;760&#34;
               height=&#34;641&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplo de hiperplanos con ruido y errores
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Cuando los datos no son perfectamente separables (o hay ruido), se permiten &lt;strong&gt;variables de holgura&lt;/strong&gt; \(\xi_i \ge 0\). Esto penaliza los errores o los puntos dentro del margen:&lt;/p&gt;
\[
\begin{aligned}
&amp; \min_{\mathbf{W},b} \quad \frac{1}{2}\|\mathbf{W}\|^2 + C \sum_{i}\xi_i \\
&amp; \text{sujeto a} \quad Y_i(\mathbf{W}^\top \mathbf{X}_i + b) \ge 1 - \xi_i,\quad \xi_i \ge 0.
\end{aligned}
\]&lt;p&gt;















&lt;figure  id=&#34;figure-las-variables-de-holgura-permiten-el-ruido-en-los-datos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Epsilon&#34; srcset=&#34;
               /minerias/8_svm/figures/SVM_non_lin_hu8821990965519421223.webp 400w,
               /minerias/8_svm/figures/SVM_non_lin_hu13426229250700642069.webp 760w,
               /minerias/8_svm/figures/SVM_non_lin_hu5029092690650428657.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/SVM_non_lin_hu8821990965519421223.webp&#34;
               width=&#34;356&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Las variables de holgura permiten el ruido en los datos.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;El parámetro \(C\) balancea la &lt;strong&gt;complejidad&lt;/strong&gt; vs. el &lt;strong&gt;número de errores&lt;/strong&gt;, y es un parametro de regularizacion.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;resolución-dual-con-kkt&#34;&gt;Resolución Dual con KKT&lt;/h4&gt;
&lt;p&gt;Tras introducir variables de holgura \(\xi_i \ge 0\), la &lt;strong&gt;formulación primal&lt;/strong&gt; se convierte en:&lt;/p&gt;
\[
\begin{aligned}
&amp; \min_{\mathbf{W}, b, \{\xi_i\}} \quad 
   \frac{1}{2}\|\mathbf{W}\|^2 + C\sum_{i=1}^n \xi_i, \\
&amp; \text{sujeto a} \quad
   Y_i(\mathbf{W}^\top\mathbf{X}_i + b)\;\ge\;1 - \xi_i,\quad \xi_i \ge 0.
\end{aligned}
\]&lt;p&gt;En el &lt;strong&gt;Lagrangiano&lt;/strong&gt;, aparecen ahora multiplicadores \(\alpha_i\) y \(\mu_i\) para manejar las restricciones asociadas a \(\xi_i\). Se obtienen condiciones KKT adicionales, incluyendo&lt;/p&gt;
\[
\alpha_i \,\bigl[Y_i(\mathbf{W}^\top\mathbf{X}_i + b)-1+\xi_i\bigr] \;=\; 0,\quad
\mu_i\,\xi_i \;=\; 0,\quad 
0 \;\le\;\alpha_i \;\le\; C.
\]&lt;p&gt;El problema &lt;strong&gt;dual&lt;/strong&gt; final para la SVM con soft margin vuelve a ser:&lt;/p&gt;
\[
\begin{aligned}
&amp; \max_{\boldsymbol{\alpha}} \quad 
   \sum_{i=1}^n \alpha_i \;-\; \tfrac{1}{2}\sum_{i,j} \alpha_i \alpha_j \,Y_i Y_j \,\langle \mathbf{X}_i,\mathbf{X}_j\rangle,\\
&amp; \text{sujeto a} \quad 0 \;\le\;\alpha_i \;\le\; C,\quad \sum_{i=1}^n \alpha_i Y_i = 0.
\end{aligned}
\]&lt;p&gt;Aquí, los puntos para los que \(\alpha_i\) está en el rango \((0, C)\) se convierten en los &lt;strong&gt;vectores de soporte&lt;/strong&gt;, y la resolución sigue análoga al caso separable.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;equivalente-de-regularización-hinge-loss&#34;&gt;Equivalente de regularización (Hinge loss)&lt;/h4&gt;
&lt;p&gt;Otra forma de ver la &lt;strong&gt;SVM lineal&lt;/strong&gt; es como un caso especial de &lt;strong&gt;pérdida bisagra (hinge loss)&lt;/strong&gt; con regularización en la norma de \(\mathbf{W}\).&lt;br&gt;
En el &lt;strong&gt;espacio primal&lt;/strong&gt;, la minimización se puede escribir como:&lt;/p&gt;
\[
\underset{\mathbf{W}, b}{\min} \;\; \sum_{i=1}^{n} \max\bigl(0,\,1 - Y_i\bigl(\mathbf{W}^\top \mathbf{X}_i + b\bigr)\bigr) \;+\; \lambda \,\|\mathbf{W}\|^2,
\]&lt;p&gt;donde \(\lambda\) es un parámetro de regularización relacionado inversamente con \(C\).&lt;/p&gt;
&lt;p&gt;La pérdida (bisagra) \(\max(0,\,1 - Y_i(\mathbf{W}^\top \mathbf{X}_i + b))\) fuerza cada ejemplo a estar, idealmente, al menos a 1 de distancia del hiperplano, y penaliza las violaciones a ese margen. Así, minimizar la norma de \(\mathbf{W}\) y el costo bisagra conduce al mismo criterio de “gran margen” que describimos antes.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;kernel-trick-y-espacio-aumentado&#34;&gt;Kernel Trick y espacio aumentado&lt;/h3&gt;
&lt;p&gt;La separación lineal puede no ser posible en el espacio original, pero sí en una &lt;strong&gt;dimensión mayor&lt;/strong&gt;. Aun así, no es necesario calcular explícitamente dicha transformación \(\varphi(\mathbf{X})\). El &lt;strong&gt;truco del kernel&lt;/strong&gt; nos dice que:&lt;/p&gt;
\[
k(\mathbf{X},\mathbf{X}&#39;) = \langle \varphi(\mathbf{X}), \,\varphi(\mathbf{X}&#39;) \rangle.
\]&lt;h4 id=&#34;aumento-del-espacio-idea&#34;&gt;Aumento del espacio (idea)&lt;/h4&gt;
&lt;p&gt;En el nuevo espacio, la separación lineal puede existir aunque en el original no:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-aumento-de-dimensión-para-separar-datos-no-lineales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Otra vista del aumento de dimensión&#34; srcset=&#34;
               /minerias/8_svm/figures/data_2d_to_3d_2_hu15601119294237896571.webp 400w,
               /minerias/8_svm/figures/data_2d_to_3d_2_hu12492001661735803246.webp 760w,
               /minerias/8_svm/figures/data_2d_to_3d_2_hu7038276523966741848.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/data_2d_to_3d_2_hu15601119294237896571.webp&#34;
               width=&#34;760&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aumento de dimensión para separar datos no lineales
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Cuidado con la alta dimensionalidad si \(d\) es muy grande.)&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;ejemplo-de-kernel-trick&#34;&gt;Ejemplo de Kernel Trick&lt;/h4&gt;
&lt;p&gt;Supongamos que queremos clasificar datos en \(\mathbb{R}^2\) usando un &lt;strong&gt;núcleo polinomial de grado 2&lt;/strong&gt;. Podemos definir una transformación explícita:&lt;/p&gt;
\[
\varphi: (x_1,x_2) \;\mapsto\; (\,x_1^2,\;\sqrt{2}\,x_1x_2,\;x_2^2,\;\sqrt{2}\,x_1,\;\sqrt{2}\,x_2,\;1\,).
\]&lt;p&gt;Al calcular el producto punto en este espacio, se observa que:&lt;/p&gt;
\[
\langle \varphi(\mathbf{X}),\varphi(\mathbf{X}&#39;)\rangle 
\;=\; \bigl(\langle \mathbf{X},\mathbf{X}&#39;\rangle + 1\bigr)^2
\]&lt;p&gt;De modo que el &lt;strong&gt;kernel&lt;/strong&gt; asociado es:&lt;/p&gt;
\[
k(\mathbf{X},\mathbf{X}&#39;) 
\;=\; \bigl( \mathbf{X}\cdot\mathbf{X}&#39; + 1 \bigr)^2.
\]&lt;p&gt;Esto permite a la SVM trabajar implícitamente con una dimensión más alta sin calcular \(\varphi(\mathbf{X})\) ni \(\varphi(\mathbf{X}&#39;)\) de forma explícita.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-truco-del-kernel-permite-separar-datos-no-lineales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Otra vista del aumento de dimensión&#34; srcset=&#34;
               /minerias/8_svm/figures/kernel_SVM_hu6299395877450182764.webp 400w,
               /minerias/8_svm/figures/kernel_SVM_hu11090577963053166792.webp 760w,
               /minerias/8_svm/figures/kernel_SVM_hu7485170395093064274.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/kernel_SVM_hu6299395877450182764.webp&#34;
               width=&#34;760&#34;
               height=&#34;472&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El truco del kernel permite separar datos no lineales.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;tipos-de-kernel-comunes&#34;&gt;Tipos de kernel comunes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
\[
  k(\mathbf{X},\mathbf{X}&#39;) \;=\; \langle \mathbf{X},\;\mathbf{X}&#39; \rangle
  \]&lt;/li&gt;
&lt;li&gt;
\[
  k(\mathbf{X},\mathbf{X}&#39;) \;=\; \bigl(\langle \mathbf{X}, \mathbf{X}&#39; \rangle + c\bigr)^{d}
  \]&lt;/li&gt;
&lt;li&gt;
\[
  k(\mathbf{X},\mathbf{X}&#39;) \;=\; \exp\!\bigl(-\gamma\,\|\mathbf{X}-\mathbf{X}&#39;\|^2\bigr)
  \]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Lineal&lt;/strong&gt;&lt;br&gt;
















&lt;figure  id=&#34;figure-separación-lineal-con-kernel-lineal&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Kernel lineal&#34; srcset=&#34;
               /minerias/8_svm/figures/svm_kernel_lin_hu15985672093414640765.webp 400w,
               /minerias/8_svm/figures/svm_kernel_lin_hu5215183063117229496.webp 760w,
               /minerias/8_svm/figures/svm_kernel_lin_hu13743402759049042516.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svm_kernel_lin_hu15985672093414640765.webp&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Separación lineal con kernel lineal
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Polinomial&lt;/strong&gt;&lt;br&gt;
















&lt;figure  id=&#34;figure-separación-con-kernel-polinomial&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Kernel polinomial&#34; srcset=&#34;
               /minerias/8_svm/figures/svm_kernel_poly_hu2205477111260707794.webp 400w,
               /minerias/8_svm/figures/svm_kernel_poly_hu14329925200945324446.webp 760w,
               /minerias/8_svm/figures/svm_kernel_poly_hu12127295301654093412.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svm_kernel_poly_hu2205477111260707794.webp&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Separación con kernel polinomial
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gaussiano&lt;/strong&gt;&lt;br&gt;
















&lt;figure  id=&#34;figure-separación-con-kernel-gaussiano&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Kernel RBF&#34; srcset=&#34;
               /minerias/8_svm/figures/svm_kernel_gauss_hu2637743137941300484.webp 400w,
               /minerias/8_svm/figures/svm_kernel_gauss_hu2121526416713396169.webp 760w,
               /minerias/8_svm/figures/svm_kernel_gauss_hu5837164105067530359.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svm_kernel_gauss_hu2637743137941300484.webp&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Separación con kernel gaussiano
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conclusión-de-svm&#34;&gt;Conclusión de SVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Se basa en &lt;strong&gt;maximizar el margen&lt;/strong&gt; (reduce sobreajuste).&lt;/li&gt;
&lt;li&gt;La solución surge de un problema de &lt;strong&gt;optimización convexa&lt;/strong&gt; que se puede abordar en su versión dual.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Los vectores de soporte&lt;/strong&gt; son los únicos puntos relevantes para la frontera de decisión.&lt;/li&gt;
&lt;li&gt;Permite &lt;strong&gt;kernels&lt;/strong&gt; para manejar no linealidades.&lt;/li&gt;
&lt;li&gt;Ajustar hiperparámetros (ej. \(C\), grado del polinomio, \(\gamma\) en el gaussiano) puede ser costoso.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;support-vector-regressor-svr&#34;&gt;Support Vector Regressor (SVR)&lt;/h2&gt;
&lt;p&gt;La SVM también puede adaptarse a &lt;strong&gt;tareas de regresión&lt;/strong&gt;. En vez de separar puntos en clases, se busca que la predicción \(\hat{y}\) caiga en un &lt;strong&gt;tubo de ancho \(\epsilon\)&lt;/strong&gt; en torno al valor real \(y\). A esto se lo denomina &lt;strong&gt;pérdida \(\epsilon\)-insensible&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;principio&#34;&gt;Principio&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-svr-con-zona-sin-penalización-si-el-error--ε&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Tubo ε-insensible en SVR&#34; srcset=&#34;
               /minerias/8_svm/figures/svr_hu8880163900142633628.webp 400w,
               /minerias/8_svm/figures/svr_hu4001860459484953900.webp 760w,
               /minerias/8_svm/figures/svr_hu17612073410467756993.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svr_hu8880163900142633628.webp&#34;
               width=&#34;760&#34;
               height=&#34;306&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SVR con zona sin penalización si el error &amp;lt; ε
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Si \(|\hat{y} - y| \leq \epsilon\), no se incurre en penalización. Caso contrario, se agregan variables \(\xi_i, \xi_i^*\) que miden el exceso del error respecto de \(\epsilon\).&lt;/p&gt;
&lt;h3 id=&#34;formulación&#34;&gt;Formulación&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
\[
  \min_{\mathbf{W},b} \;\; \tfrac{1}{2}\|\mathbf{W}\|^2 + C\sum_i (\xi_i + \xi_i^*)
  \]\[
  \begin{cases}
  y_i - f(\mathbf{X}_i) \;\le\; \epsilon + \xi_i, \\
  f(\mathbf{X}_i) - y_i \;\le\; \epsilon + \xi_i^*, \\
  \xi_i,\xi_i^*\ge0.
  \end{cases}
  \]&lt;/li&gt;
&lt;li&gt;
\[
  f(\mathbf{X}) \;=\; \sum_{i=1}^{n} (\alpha_i - \alpha_i^*)\,k(\mathbf{X}_i,\mathbf{X})\;+\;b.
  \]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ejemplo&#34;&gt;Ejemplo&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-svr-ajustando-una-curva-y-mostrando-el-tubo-ε&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de ajuste con SVR&#34; srcset=&#34;
               /minerias/8_svm/figures/svr_ex_hu6641046939284454635.webp 400w,
               /minerias/8_svm/figures/svr_ex_hu10964327218363507417.webp 760w,
               /minerias/8_svm/figures/svr_ex_hu9191617817313677244.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/8_svm/figures/svr_ex_hu6641046939284454635.webp&#34;
               width=&#34;760&#34;
               height=&#34;563&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SVR ajustando una curva y mostrando el tubo ±ε
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Solo los puntos que quedan fuera del tubo (por encima de \(\epsilon\)) se convierten en vectores de soporte.&lt;/p&gt;
&lt;h4 id=&#34;resumen-svr&#34;&gt;Resumen SVR&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Extiende la SVM a &lt;strong&gt;regresión&lt;/strong&gt;, usando la idea de &lt;em&gt;máximo margen&lt;/em&gt; alrededor de la curva aprendida.&lt;/li&gt;
&lt;li&gt;El &lt;strong&gt;parámetro \(\epsilon\)&lt;/strong&gt; controla el ancho de la zona “sin costo”; \(\xi_i,\xi_i^*\) miden el exceso.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;\(C\)&lt;/strong&gt; regula la penalización por exceder \(\epsilon\).&lt;/li&gt;
&lt;li&gt;Se pueden usar &lt;strong&gt;kernels&lt;/strong&gt; para la parte no lineal, igual que en la clasificación.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;¡Eso es todo sobre &lt;strong&gt;SVM&lt;/strong&gt; y &lt;strong&gt;SVR&lt;/strong&gt;! Son métodos muy potentes y ampliamente utilizados en Machine Learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SVM&lt;/strong&gt; para clasificación binaria (y extensiones a multiclase).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVR&lt;/strong&gt; para regresión con márgenes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kernels&lt;/strong&gt; para no linealidad en ambos casos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>http://localhost:1313/minerias/9_clustering/</link>
      <pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/9_clustering/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_clusteringpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Clustering.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;h1 id=&#34;aprendizaje-no-supervisado-y-clustering&#34;&gt;Aprendizaje no supervisado y Clustering&lt;/h1&gt;
&lt;p&gt;En &lt;strong&gt;aprendizaje no supervisado&lt;/strong&gt;, no hay una variable objetivo \(Y\) etiquetada que queramos predecir. El objetivo es &lt;strong&gt;descubrir estructuras&lt;/strong&gt; en los datos, como &lt;strong&gt;grupos (clusters)&lt;/strong&gt;, &lt;strong&gt;patrones&lt;/strong&gt; o &lt;strong&gt;regularidades&lt;/strong&gt;. Esto difiere del &lt;strong&gt;aprendizaje supervisado&lt;/strong&gt;, donde sí conocemos las etiquetas \((\mathbf{X}_i, Y_i)\) y buscamos una función \(f\) que prediga \(Y\) a partir de \(\mathbf{X}\).&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-aprendizaje-non-supervisado-es-mas-complejo-y-menos-performante-que-el-supervisado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Aprendizaje supervisado vs. no supervisado&#34; srcset=&#34;
               /minerias/9_clustering/figures/tweet_socher_unsupervised_hu4823786434995542501.webp 400w,
               /minerias/9_clustering/figures/tweet_socher_unsupervised_hu3241925976670476204.webp 760w,
               /minerias/9_clustering/figures/tweet_socher_unsupervised_hu11863700504794312954.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/tweet_socher_unsupervised_hu4823786434995542501.webp&#34;
               width=&#34;760&#34;
               height=&#34;219&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El aprendizaje non supervisado es mas complejo y menos performante que el supervisado.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;clustering-principio&#34;&gt;Clustering: Principio&lt;/h2&gt;
&lt;h3 id=&#34;por-qué-clústeres&#34;&gt;¿Por qué clústeres?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Topic modeling&lt;/strong&gt;: agrupar documentos o comentarios según su tema.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Análisis de sentimientos&lt;/strong&gt;: reagrupar comentarios de usuarios para descubrir críticas comunes o elogios.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentación de clientes&lt;/strong&gt;: en marketing, agrupar clientes con comportamientos similares.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eficiencia&lt;/strong&gt;: entrenar submodelos especializados en cada clúster de datos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ambigüedad&#34;&gt;Ambigüedad&lt;/h3&gt;
&lt;p&gt;Distintas particiones pueden ser igualmente válidas:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-distintas-clusterizaciones-posibles-por-una-misma&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distintas clusterizaciones posibles&#34; srcset=&#34;
               /minerias/9_clustering/figures/different_clusterizations_2_hu17543974253917984688.webp 400w,
               /minerias/9_clustering/figures/different_clusterizations_2_hu1269938481521594311.webp 760w,
               /minerias/9_clustering/figures/different_clusterizations_2_hu15640061247686310699.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/different_clusterizations_2_hu17543974253917984688.webp&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Distintas clusterizaciones posibles por una misma
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;No siempre hay una sola respuesta a “cómo” agrupar.&lt;/p&gt;
&lt;h3 id=&#34;objetivo-intuitivo&#34;&gt;Objetivo intuitivo&lt;/h3&gt;
&lt;p&gt;Disminuir la &lt;strong&gt;distancia intra-cluster&lt;/strong&gt; (los puntos de un mismo grupo están juntos)
y aumentar la &lt;strong&gt;distancia inter-cluster&lt;/strong&gt; (grupos lejos entre sí):&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-buscamos-que-los-puntos-de-un-mismo-clúster-estén-cerca-y-los-distintos-clústeres-lejos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Inter/intra distancias&#34; srcset=&#34;
               /minerias/9_clustering/figures/cluster_inter_intra_hu15678636143123641471.webp 400w,
               /minerias/9_clustering/figures/cluster_inter_intra_hu5763737174000151162.webp 760w,
               /minerias/9_clustering/figures/cluster_inter_intra_hu17635859847885416538.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/cluster_inter_intra_hu15678636143123641471.webp&#34;
               width=&#34;760&#34;
               height=&#34;637&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Buscamos que los puntos de un mismo clúster estén cerca, y los distintos clústeres lejos.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;k-means&#34;&gt;K-means&lt;/h2&gt;
&lt;h3 id=&#34;idea-general&#34;&gt;Idea general&lt;/h3&gt;
&lt;p&gt;Separar los datos en \(K\) grupos usando “centroides” (medias de cada clúster):&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-los-centroides-se-ajustan-iterando-la-asignación-de-puntos-y-recálculo-de-medias&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;K-means ejemplo simple&#34; srcset=&#34;
               /minerias/9_clustering/figures/k_means_ex1_hu12039563699879038830.webp 400w,
               /minerias/9_clustering/figures/k_means_ex1_hu9479202703904514153.webp 760w,
               /minerias/9_clustering/figures/k_means_ex1_hu11178902821246344841.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/k_means_ex1_hu12039563699879038830.webp&#34;
               width=&#34;760&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los centroides se ajustan iterando la asignación de puntos y recálculo de medias.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;minimizar-la-sse-sum-of-squared-errors&#34;&gt;Minimizar la SSE (Sum of Squared Errors)&lt;/h3&gt;
&lt;p&gt;K-means particiona los datos \(\Xb_i\) en \(K\) clústeres \(\{\mathcal{C}_1,\dots,\mathcal{C}_K\}\) para &lt;strong&gt;minimizar&lt;/strong&gt;:&lt;/p&gt;
\[
G(\mathcal{C}_1,\dots,\mathcal{C}_K)
= \sum_{k=1}^K 
  \sum_{i\in\mathcal{C}_k} \|\Xb_i - \mu_k\|^2,
\]&lt;p&gt;donde \(\mu_k\) es el &lt;strong&gt;centroide&lt;/strong&gt; del clúster \(k\). Cada punto se asigna al \(\mu_k\) más cercano en norma Euclídea.&lt;/p&gt;
&lt;h3 id=&#34;algoritmo-iterativo&#34;&gt;Algoritmo iterativo&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Asignación&lt;/strong&gt;: Conociendo los centroides \(\mu_k\), cada punto se asigna al más cercano.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: Conociendo la asignación, se recalculan los centroides \(\mu_k\)
como la media de los puntos en cada clúster.&lt;/li&gt;
&lt;li&gt;Se repite hasta que la inercia (SSE) deje de disminuir significativamente.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Paso de asignación y actualización&#34; srcset=&#34;
               /minerias/9_clustering/figures/steps-of-kmeans_hu8097533622567889976.webp 400w,
               /minerias/9_clustering/figures/steps-of-kmeans_hu18443175150394165637.webp 760w,
               /minerias/9_clustering/figures/steps-of-kmeans_hu16240963076215426502.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/steps-of-kmeans_hu8097533622567889976.webp&#34;
               width=&#34;760&#34;
               height=&#34;246&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;dependencia-de-la-inicialización&#34;&gt;Dependencia de la inicialización&lt;/h3&gt;
&lt;p&gt;Es un problema no convexo, por lo que su solución depende de la semilla inicial:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-una-primera-inicializacion-da-un-primer-resultado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distintas inicializaciones llevan a diferentes soluciones&#34; srcset=&#34;
               /minerias/9_clustering/figures/K_mean_conv_ex_1_hu17759993386177690536.webp 400w,
               /minerias/9_clustering/figures/K_mean_conv_ex_1_hu4541070697010597352.webp 760w,
               /minerias/9_clustering/figures/K_mean_conv_ex_1_hu381108850359093404.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/K_mean_conv_ex_1_hu17759993386177690536.webp&#34;
               width=&#34;760&#34;
               height=&#34;661&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Una primera inicializacion da un primer resultado.
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-una-otra-inicializacion-da-un-otro-resultado&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Otra inicialización distinta&#34; srcset=&#34;
               /minerias/9_clustering/figures/K_mean_conv_ex_2_hu9792573445888391184.webp 400w,
               /minerias/9_clustering/figures/K_mean_conv_ex_2_hu10004764372344348931.webp 760w,
               /minerias/9_clustering/figures/K_mean_conv_ex_2_hu4315955420995951315.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/K_mean_conv_ex_2_hu9792573445888391184.webp&#34;
               width=&#34;760&#34;
               height=&#34;661&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Una otra inicializacion da un otro resultado
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solución&lt;/strong&gt;: ejecutar K-means varias veces y elegir la partición con menor SSE.&lt;/p&gt;
&lt;h3 id=&#34;elección-de-k&#34;&gt;Elección de \(K\)&lt;/h3&gt;
&lt;p&gt;El número de clústeres debe fijarse antes. Se puede explorar varios valores y usar el
&lt;strong&gt;método del codo (elbow method)&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-codo-indica-un-buen-trade-off-no-reduce-mucho-más-la-varianza-al-aumentar-k&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Elbow method en K-means&#34; srcset=&#34;
               /minerias/9_clustering/figures/kmeans_elbow_hu9023007950834737642.webp 400w,
               /minerias/9_clustering/figures/kmeans_elbow_hu13987282990509434748.webp 760w,
               /minerias/9_clustering/figures/kmeans_elbow_hu7513878213620665158.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/kmeans_elbow_hu9023007950834737642.webp&#34;
               width=&#34;389&#34;
               height=&#34;278&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El codo indica un buen trade-off: no reduce mucho más la varianza al aumentar K.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dbscan&#34;&gt;DBSCAN&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DBSCAN (Density-Based Spatial Clustering of Applications with Noise)&lt;/strong&gt; busca regiones densas y marca los puntos dispersos como ruido.&lt;/p&gt;
&lt;h3 id=&#34;ejemplo&#34;&gt;Ejemplo&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-dbscan-puede-reconocer-clusters-y-eliminar-ruido-al-mismo-tiempo&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DBSCAN puede eliminar ruido&#34;
           src=&#34;http://localhost:1313/minerias/9_clustering/figures/DBSCAN.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      DBSCAN puede reconocer clusters y eliminar ruido al mismo tiempo.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;principios&#34;&gt;Principios&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Parámetros:
&lt;ul&gt;
&lt;li&gt;\(\texttt{eps} (\varepsilon)\): radio de vecindad.&lt;/li&gt;
&lt;li&gt;\(\texttt{min\_samples}\): mínimo de puntos para que un punto sea núcleo (core).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Clasifica en &lt;strong&gt;core&lt;/strong&gt;, &lt;strong&gt;border&lt;/strong&gt;, &lt;strong&gt;noise&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt;: al menos \(\texttt{min\_samples}\) puntos en su vecindad.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Border&lt;/strong&gt;: no llega a ese umbral, pero está dentro del vecindario de un core.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Noise&lt;/strong&gt;: no pertenece a core ni al vecindario de un core.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-principio-de-dbscan-puntos-core-border-noise&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Puntos Core, Border, Noise&#34; srcset=&#34;
               /minerias/9_clustering/figures/dbscan-principle_hu1074686840124571361.webp 400w,
               /minerias/9_clustering/figures/dbscan-principle_hu8659054873631810230.webp 760w,
               /minerias/9_clustering/figures/dbscan-principle_hu8842561263444215320.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/dbscan-principle_hu1074686840124571361.webp&#34;
               width=&#34;358&#34;
               height=&#34;168&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Principio de DBSCAN: Puntos Core, Border, Noise
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;ventajas-y-desventajas&#34;&gt;Ventajas y Desventajas&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ventajas&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecta formas arbitrarias (no sólo esferas).&lt;/li&gt;
&lt;li&gt;Identifica outliers (ruido).&lt;/li&gt;
&lt;li&gt;Menos parámetros que métodos jerárquicos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Desventajas&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensible a \(\texttt{eps}\) y \(\texttt{min\_samples}\).&lt;/li&gt;
&lt;li&gt;Si la densidad varía drásticamente, un único \(\texttt{eps}\) no es apropiado.&lt;/li&gt;
&lt;li&gt;Difícil en alta dimensionalidad.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-mal-ajuste-con-eps-grandepequeño&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/dbscan_disvantages_2.png&#34; alt=&#34;Mal ajuste con eps grande/pequeño&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Mal ajuste con eps grande/pequeño
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-ruido-o-clusters&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;figures/dbscan_disvantages_1.png&#34; alt=&#34;Ruido o clusters?&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ruido o clusters?
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;elección-de-textttminpts-y-varepsilon&#34;&gt;Elección de \(\texttt{minPts}\) y \(\varepsilon\)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;\(\texttt{minPts}\)&lt;/strong&gt;: regla de pulgar \(\texttt{minPts}\ge D+1\) (donde \(D\) = dimensión).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;\(\varepsilon\)&lt;/strong&gt;: usar gráfico de k-dist y buscar el “codo”.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;OPTICS&lt;/em&gt; es una alternativa más avanzada que ajusta la densidad de forma variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;jerárquico-bisecting-k-means&#34;&gt;Jerárquico: Bisecting K-means&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bisecting K-means&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Arranca con todos los puntos como un clúster.&lt;/li&gt;
&lt;li&gt;Aplica K-means con \(k=2\) (bisect) para dividirlo en 2.&lt;/li&gt;
&lt;li&gt;Escoge el clúster con mayor SSE y lo subdivide.&lt;/li&gt;
&lt;li&gt;Repite hasta lograr \(K\) clústeres.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Bisecting K-means diagrama&#34; srcset=&#34;
               /minerias/9_clustering/figures/bisecting_kmeans_hu5581000751151692979.webp 400w,
               /minerias/9_clustering/figures/bisecting_kmeans_hu11579197046256656614.webp 760w,
               /minerias/9_clustering/figures/bisecting_kmeans_hu3758992962579795936.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/bisecting_kmeans_hu5581000751151692979.webp&#34;
               width=&#34;760&#34;
               height=&#34;234&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ventajas&lt;/strong&gt;: combinan jerarquía y rapidez de K-means.&lt;br&gt;
&lt;strong&gt;Algoritmo&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Pseudocódigo de Bisecting K-means&#34; srcset=&#34;
               /minerias/9_clustering/figures/Bissected_KMEANS_algo_hu1222612671137320186.webp 400w,
               /minerias/9_clustering/figures/Bissected_KMEANS_algo_hu16003874094996312868.webp 760w,
               /minerias/9_clustering/figures/Bissected_KMEANS_algo_hu9289569499455429687.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/Bissected_KMEANS_algo_hu1222612671137320186.webp&#34;
               width=&#34;401&#34;
               height=&#34;705&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;jerárquico-aglomerativo-hac&#34;&gt;Jerárquico Aglomerativo (HAC)&lt;/h2&gt;
&lt;p&gt;Otro enfoque jerárquico:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cada punto inicia como un clúster propio.&lt;/li&gt;
&lt;li&gt;Se fusionan iterativamente los 2 clústeres más cercanos.&lt;/li&gt;
&lt;li&gt;Se actualiza la distancia con el nuevo clúster.&lt;/li&gt;
&lt;li&gt;Hasta que quede un solo clúster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-fusión-jerárquica-paso-a-paso&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fusión jerárquica paso a paso&#34;
           src=&#34;http://localhost:1313/minerias/9_clustering/figures/Merging_Clusters.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fusión jerárquica paso a paso
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;distancia-entre-clústeres-linkage&#34;&gt;Distancia entre clústeres (Linkage)&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Criterios de linkage&#34; srcset=&#34;
               /minerias/9_clustering/figures/Cluster_sim_1_hu14981034795380668.webp 400w,
               /minerias/9_clustering/figures/Cluster_sim_1_hu13484372294006850988.webp 760w,
               /minerias/9_clustering/figures/Cluster_sim_1_hu15281884628367069891.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/Cluster_sim_1_hu14981034795380668.webp&#34;
               width=&#34;760&#34;
               height=&#34;329&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Se define la “distancia” entre dos grupos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single linkage&lt;/strong&gt;: menor distancia entre pares de puntos (sensibilidad al ruido).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete linkage&lt;/strong&gt;: mayor distancia entre pares de puntos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average linkage&lt;/strong&gt;: promedio de distancias entre pares (compromiso).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Centroid&lt;/strong&gt; linkage: distancia entre centroides.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hdbscan&#34;&gt;HDBSCAN&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;HDBSCAN&lt;/strong&gt;: extensión jerárquica de DBSCAN&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explora múltiples densidades.&lt;/li&gt;
&lt;li&gt;No fija un único \(\varepsilon\).&lt;/li&gt;
&lt;li&gt;Elige subconjuntos estables dentro de la jerarquía de densidad.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://hdbscan.readthedocs.io/en/latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentación HDBSCAN&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;métricas-de-validación&#34;&gt;Métricas de Validación&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;A diferencia del aprendizaje supervisado&lt;/strong&gt;, muchas veces no hay etiquetas para evaluar la calidad de un clustering. Dos enfoques:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Métricas externas&lt;/strong&gt; (hay etiquetas reales):
&lt;ul&gt;
&lt;li&gt;Homogeneidad, Completeness, V-measure.&lt;/li&gt;
&lt;li&gt;Rand Index, Fowlkes–Mallows.&lt;/li&gt;
&lt;li&gt;Mutual Information (MI).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Métricas internas&lt;/strong&gt; (sólo datos y clústeres):
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Silhouette&lt;/strong&gt; (valores entre -1 y 1).&lt;/li&gt;
&lt;li&gt;SSE (inercia en K-means).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resumen-rápido&#34;&gt;Resumen rápido&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Homogeneidad&lt;/strong&gt;: cada clúster contiene sólo puntos de una clase.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Completeness&lt;/strong&gt;: cada clase está contenida en un único clúster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;V-measure&lt;/strong&gt;: media armónica de las dos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fowlkes–Mallows&lt;/strong&gt;: ve pares de puntos coincidentes en la etiqueta y en el cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Silhouette&lt;/strong&gt;: cohesión y separación sin usar clases reales.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;otros-métodos-de-clustering&#34;&gt;Otros métodos de clustering&lt;/h2&gt;
&lt;p&gt;Existen varias alternativas según forma de los clústeres, ruido, densidad variable, etc.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;clustering_examples.png&#34; alt=&#34;Galería de métodos de clustering&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Tabla de referencia (Scikit-learn):&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Method Name&lt;/th&gt;
          &lt;th&gt;Parameters&lt;/th&gt;
          &lt;th&gt;Scalability&lt;/th&gt;
          &lt;th&gt;Use Case&lt;/th&gt;
          &lt;th&gt;Geometry (Metric Used)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#k-means&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;K-Means&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Number of clusters&lt;/td&gt;
          &lt;td&gt;Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt; with MiniBatch code&lt;/td&gt;
          &lt;td&gt;General-purpose, even cluster size, flat geometry, not too many clusters, inductive&lt;/td&gt;
          &lt;td&gt;Distances between points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Affinity Propagation&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Damping, sample preference&lt;/td&gt;
          &lt;td&gt;Not scalable with &lt;code&gt;n_samples&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Many clusters, uneven cluster size, non-flat geometry, inductive&lt;/td&gt;
          &lt;td&gt;Graph distance (e.g., nearest-neighbor graph)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#mean-shift&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Mean-Shift&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Bandwidth&lt;/td&gt;
          &lt;td&gt;Not scalable with &lt;code&gt;n_samples&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Many clusters, uneven cluster size, non-flat geometry, inductive&lt;/td&gt;
          &lt;td&gt;Distances between points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Spectral Clustering&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Number of clusters&lt;/td&gt;
          &lt;td&gt;Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Few clusters, even cluster size, non-flat geometry, transductive&lt;/td&gt;
          &lt;td&gt;Graph distance (e.g., nearest-neighbor graph)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Ward Hierarchical Clustering&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Number of clusters or distance threshold&lt;/td&gt;
          &lt;td&gt;Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Many clusters, possibly connectivity constraints, transductive&lt;/td&gt;
          &lt;td&gt;Distances between points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Agglomerative Clustering&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Number of clusters or distance threshold, linkage type, distance&lt;/td&gt;
          &lt;td&gt;Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Many clusters, possibly connectivity constraints, non-Euclidean distances, transductive&lt;/td&gt;
          &lt;td&gt;Any pairwise distance&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#dbscan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;DBSCAN&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Neighborhood size&lt;/td&gt;
          &lt;td&gt;Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Non-flat geometry, uneven cluster sizes, outlier removal, transductive&lt;/td&gt;
          &lt;td&gt;Distances between nearest points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#hdbscan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;HDBSCAN&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Minimum cluster membership, minimum point neighbors&lt;/td&gt;
          &lt;td&gt;Large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Non-flat geometry, uneven cluster sizes, outlier removal, transductive, hierarchical, variable cluster density&lt;/td&gt;
          &lt;td&gt;Distances between nearest points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#optics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;OPTICS&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Minimum cluster membership&lt;/td&gt;
          &lt;td&gt;Very large &lt;code&gt;n_samples&lt;/code&gt;, large &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Non-flat geometry, uneven cluster sizes, variable cluster density, outlier removal, transductive&lt;/td&gt;
          &lt;td&gt;Distances between points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#gaussian-mixture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Gaussian Mixtures&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Many&lt;/td&gt;
          &lt;td&gt;Not scalable&lt;/td&gt;
          &lt;td&gt;Flat geometry, good for density estimation, inductive&lt;/td&gt;
          &lt;td&gt;Mahalanobis distances to centers&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#birch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;BIRCH&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Branching factor, threshold, optional global clusterer&lt;/td&gt;
          &lt;td&gt;Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Large dataset, outlier removal, data reduction, inductive&lt;/td&gt;
          &lt;td&gt;Euclidean distance between points&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#bisecting-k-means&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bisecting K-Means&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Number of clusters&lt;/td&gt;
          &lt;td&gt;Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;General-purpose, even cluster size, flat geometry, no empty clusters, inductive, hierarchical&lt;/td&gt;
          &lt;td&gt;Distances between points&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Más info en: &lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;clustering scikit-learn docs&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;use-case-bertopic&#34;&gt;Use-case: BERTopic&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERTopic&lt;/strong&gt; es una librería que combina embeddings (BERT) + clustering para &lt;strong&gt;topic modeling&lt;/strong&gt; sobre textos.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;BERTopic Ejemplo&#34; srcset=&#34;
               /minerias/9_clustering/figures/BERTopic_pres_hu18231522808155045213.webp 400w,
               /minerias/9_clustering/figures/BERTopic_pres_hu8636971556350744288.webp 760w,
               /minerias/9_clustering/figures/BERTopic_pres_hu2358273787969741926.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/9_clustering/figures/BERTopic_pres_hu18231522808155045213.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Puede:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agrupar oraciones/documentos en “temas” usando embeddings.&lt;/li&gt;
&lt;li&gt;Visualizar la &lt;strong&gt;evolución&lt;/strong&gt; de temas en el tiempo (dynamic topic modeling).&lt;/li&gt;
&lt;li&gt;Hacer &lt;strong&gt;clustering jerárquico&lt;/strong&gt; en temas descubiertos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://maartengr.github.io/BERTopic/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERTopic docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-permite-un-tracking-dinámico-de-la-evolución-de-topics-ademas-se-puede-ver-que-las-palabras-las-mas-importante-para-cada-topic-evoluan-en-el-tiempo&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Visualización de temas en el tiempo&#34;
           src=&#34;http://localhost:1313/minerias/9_clustering/figures/BERTopic_time.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Permite un tracking dinámico de la evolución de topics. Ademas se puede ver que las palabras las mas importante para cada topic evoluan en el tiempo.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision Architectures</title>
      <link>http://localhost:1313/deep/9_cnn_architectures/</link>
      <pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/9_cnn_architectures/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslides9_cnn_architecturespdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/9_CNN_Architectures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>(TODO) Dimensionality Reduction</title>
      <link>http://localhost:1313/minerias/10_reduccion_atributos/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/10_reduccion_atributos/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_disminucion_dimensionespdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Disminucion_Dimensiones.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reducción-de-dimensionalidad&#34;&gt;Reducción de Dimensionalidad&lt;/h2&gt;
&lt;p&gt;Las &lt;strong&gt;técnicas de reducción de dimensión&lt;/strong&gt; buscan simplificar la representación de los datos, sea eliminando atributos innecesarios (selección de características) o encontrando una proyección más compacta que mantenga la mayor parte de la información. A continuación, veremos métodos tanto &lt;strong&gt;supervisados&lt;/strong&gt; (wrappers, filters) como &lt;strong&gt;no supervisados&lt;/strong&gt; (PCA, entre otros).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivaciones-para-una-dimensión-más-baja&#34;&gt;Motivaciones para una Dimensión más Baja&lt;/h2&gt;
&lt;h3 id=&#34;selección-y-reducción-de-atributos&#34;&gt;Selección y Reducción de Atributos&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Motivación General&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En muchos problemas de aprendizaje, hay &lt;strong&gt;atributos irrelevantes&lt;/strong&gt; o &lt;strong&gt;redundantes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Pueden perjudicar el desempeño de un clasificador o incrementar el costo de entrenamiento.&lt;/li&gt;
&lt;li&gt;Dos enfoques principales:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Selección de atributos (supervisado)&lt;/strong&gt;: escoger un &lt;strong&gt;subconjunto&lt;/strong&gt; relevante para la tarea (clasificación, regresión&amp;hellip;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducción de dimensionalidad (no supervisado)&lt;/strong&gt;: encontrar una &lt;strong&gt;proyección&lt;/strong&gt; de menor dimensión que concentre la información de los datos.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;razones-para-seleccionar-atributos&#34;&gt;Razones para Seleccionar Atributos&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Árboles de decisión&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Aunque tratan de escoger atributos relevantes, si hay muchos atributos basura, puede aparecer sobreajuste (“aprender” ruido con árboles muy profundos).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KNN&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Se ve muy afectado por atributos irrelevantes, pues todas las dimensiones contribuyen igual al &lt;strong&gt;cálculo de distancias&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naïve Bayes&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Robusto a atributos irrelevantes (los ignora en la probabilidad a posteriori), pero sufre con atributos muy &lt;strong&gt;correlacionados&lt;/strong&gt; (redundantes).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;En general, se quiere &lt;strong&gt;mitigar la curse of dimensionality&lt;/strong&gt; (maldición de la dimensionalidad).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;selección-de-atributos&#34;&gt;Selección de Atributos&lt;/h2&gt;
&lt;h3 id=&#34;filtros-vs-envoltura&#34;&gt;Filtros vs. Envoltura&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Feature-based (Filtros)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evalúan atributos por &lt;strong&gt;propiedades de los datos&lt;/strong&gt; (p.ej. varianza, correlación, información mutua).&lt;/li&gt;
&lt;li&gt;Independientes del clasificador.&lt;/li&gt;
&lt;li&gt;Más rápidos y con menos riesgo de sobreajuste, pero &lt;strong&gt;no captan&lt;/strong&gt; interacciones complejas con la tarea predictiva.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Model-based (Wrappers)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entrenan un &lt;strong&gt;clasificador&lt;/strong&gt; sobre cada subconjunto para medir qué tan bueno es.&lt;/li&gt;
&lt;li&gt;Objetivo: maximizar la capacidad predictiva del modelo específico.&lt;/li&gt;
&lt;li&gt;Pueden ser computacionalmente costosos y propensos a sobreajuste, pero suelen encontrar subconjuntos de atributos &lt;strong&gt;más específicos a la tarea&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;métodos-feature-based-univariate&#34;&gt;Métodos Feature-based (Univariate)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Independientes de un modelo&lt;/strong&gt; (univariate feature selection).&lt;/li&gt;
&lt;li&gt;Usan &lt;strong&gt;métricas&lt;/strong&gt; generales (entropía, Information Gain, correlación, \(\chi^2\)&amp;hellip;).&lt;/li&gt;
&lt;li&gt;Ejemplos:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Information Gain&lt;/strong&gt; (basado en entropía).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mutual Information&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correlation-based Feature Selection (CFS)&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low variance&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Son rápidos y simples, pero &lt;strong&gt;no consideran&lt;/strong&gt; cómo cada atributo interactúa con un clasificador concreto.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Mutual Information y \(\chi^2\) son útiles cuando los datos son dispersos (sparse).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h4 id=&#34;correlation-based-feature-selection-cfs&#34;&gt;Correlation-based Feature Selection (CFS)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Se evalúa un subconjunto de atributos según correlaciones en los datos:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Alta correlación&lt;/strong&gt; con la clase.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Baja correlación&lt;/strong&gt; entre atributos (evitar redundancia).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Se suele usar &lt;em&gt;symmetric uncertainty&lt;/em&gt; para medir correlaciones entre atributos categóricos:&lt;/li&gt;
&lt;/ul&gt;
\[
\text{SymmUnc}(A,B) 
= \frac{2 \times (H(A) - H(A|B))}{H(A)+H(B)} 
= \frac{2 \times IG(A,B)}{H(A)+H(B)},
\]&lt;p&gt;
donde \(H(\cdot)\) es la entropía, e \(IG(A,B)\) la ganancia de información.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Búsqueda de subconjuntos&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El total de subconjuntos es \(\mathcal{O}(2^n)\).&lt;/li&gt;
&lt;li&gt;Se aplican heurísticas &lt;strong&gt;greedy&lt;/strong&gt; (Forward selection, Backward elimination) o más avanzadas (Best-first search, Beam search, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model-based-scheme-specific&#34;&gt;Model-based (Scheme-Specific)&lt;/h3&gt;
&lt;h4 id=&#34;selección-basada-en-importancia-de-atributos&#34;&gt;Selección basada en importancia de atributos&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tras entrenar un &lt;strong&gt;modelo&lt;/strong&gt; (ej. árbol, regresión lineal, random forest), se obtiene la &lt;strong&gt;importancia&lt;/strong&gt; de cada atributo (coeficientes o feature importances).&lt;/li&gt;
&lt;li&gt;Se descartan los que &lt;strong&gt;no&lt;/strong&gt; superan cierto umbral.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SelectFromModel de scikit-learn&lt;/a&gt; permite automatizarlo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regularización \(\ell_1\) (Lasso)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Induce &lt;strong&gt;parcimonia&lt;/strong&gt;: algunos coeficientes se anulan.&lt;/li&gt;
&lt;li&gt;Ideal para selección en modelos lineales (regresión, logística).&lt;/li&gt;
&lt;li&gt;Penaliza \(\sum |w_j|\), forzando algunos \(w_j\) a 0.&lt;/li&gt;
&lt;li&gt;Integra fácilmente con &lt;code&gt;SelectFromModel&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;selección-por-wrapper-performance-based&#34;&gt;Selección por Wrapper (Performance-based)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Se prueban subconjuntos entrenando un modelo y midiendo su &lt;strong&gt;performance&lt;/strong&gt; (accuracy, F1, AUC&amp;hellip;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Muy costoso&lt;/strong&gt;: reentrenar el modelo para cada subconjunto potencial.&lt;/li&gt;
&lt;li&gt;A menudo se hace &lt;strong&gt;greedy&lt;/strong&gt; (Forward o Backward).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive Feature Elimination (RFE)&lt;/strong&gt;:
&lt;ol&gt;
&lt;li&gt;Entrena estimador, extrae la &lt;strong&gt;importancia&lt;/strong&gt; de atributos,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Descarta&lt;/strong&gt; los menos importantes,&lt;/li&gt;
&lt;li&gt;Repite con el subconjunto reducido hasta lograr la cantidad de atributos deseada.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reducción-de-dimensión&#34;&gt;Reducción de Dimensión&lt;/h2&gt;
&lt;p&gt;Más allá de “quitar” atributos, a veces &lt;strong&gt;transformamos&lt;/strong&gt; los datos a una dimensión menor con un método no supervisado.&lt;/p&gt;
&lt;h3 id=&#34;intereses-de-la-reducción-de-dimensión&#34;&gt;Intereses de la Reducción de Dimensión&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simplificar los datos&lt;/strong&gt;: Menos ruido, mejor generalización.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducir costo computacional&lt;/strong&gt;: Entrenar y predecir con menos dimensiones es más rápido.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualización&lt;/strong&gt;: Proyecciones 2D o 3D para entender la estructura de los datos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evitar la curse of dimensionality&lt;/strong&gt;: Métodos basados en distancia sufren en alta dimensión.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compresión&lt;/strong&gt;: Reducir espacio de almacenamiento.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;análisis-de-componentes-principales-pca&#34;&gt;Análisis de Componentes Principales (PCA)&lt;/h2&gt;
&lt;p&gt;PCA es un método &lt;strong&gt;lineal&lt;/strong&gt; que halla vectores propios de la matriz de &lt;strong&gt;covarianza&lt;/strong&gt;. Permitten de representar la mayor parte de los datos, con menos vectores en la base del espacio.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-cada-dato-se-representa-como-una-suma-ponderada-de-vectores-de-una-nueva-base-que-permite-de-representar-bien-la-mayor-parte-de-los-datos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de eigenfaces (PCA en imágenes)&#34; srcset=&#34;
               /minerias/10_reduccion_atributos/figures/pca_matrices_val_hu6389701791494221410.webp 400w,
               /minerias/10_reduccion_atributos/figures/pca_matrices_val_hu16190810966266397190.webp 760w,
               /minerias/10_reduccion_atributos/figures/pca_matrices_val_hu4178062851371759896.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/pca_matrices_val_hu6389701791494221410.webp&#34;
               width=&#34;760&#34;
               height=&#34;417&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cada dato se representa como una suma ponderada de vectores de una nueva base, que permite de representar bien la mayor parte de los datos.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-eigenfaces-como-vectores-base-de-pca-obtenidos-con-pca-en-un-dataset-de-rostros&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Eigenfaces obtenidos con PCA en un dataset de rostros&#34; srcset=&#34;
               /minerias/10_reduccion_atributos/figures/eigenfaces2_hu15919972794918670722.webp 400w,
               /minerias/10_reduccion_atributos/figures/eigenfaces2_hu16321244179592396584.webp 760w,
               /minerias/10_reduccion_atributos/figures/eigenfaces2_hu15806552021475915375.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/eigenfaces2_hu15919972794918670722.webp&#34;
               width=&#34;576&#34;
               height=&#34;237&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Eigenfaces como vectores base de PCA obtenidos con PCA en un dataset de rostros.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;por-qué-pca&#34;&gt;¿Por qué PCA?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reduce dimensión&lt;/strong&gt; para análisis, visualización o compresión.&lt;/li&gt;
&lt;li&gt;Encuentra un &lt;strong&gt;sistema de coordenadas&lt;/strong&gt; donde los ejes (componentes principales) corresponden a las direcciones de &lt;strong&gt;mayor varianza&lt;/strong&gt; en los datos.&lt;/li&gt;
&lt;li&gt;Base calculada con las &lt;strong&gt;autovectores&lt;/strong&gt; de la covarianza.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ejemplo-e-ilustración&#34;&gt;Ejemplo e Ilustración&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PCA ve la redundancia (varias variables correlacionadas) y la condensa en &lt;strong&gt;componentes&lt;/strong&gt; que explican la mayor parte de la varianza.&lt;/li&gt;
&lt;li&gt;Datos \(\mathbf{X}_0\) &lt;strong&gt;centrados&lt;/strong&gt; (o estandarizados) se proyectan en el subespacio donde la &lt;strong&gt;varianza&lt;/strong&gt; proyectada es máxima.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplo-pca-de-2d-a-1d&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo PCA 2D -&amp;gt; 1D&#34;
           src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/EX_PCA.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplo PCA de 2D a 1D.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Se pueden visualizar transformaciones o la reconstrucción aproximada:&lt;/p&gt;
&lt;h2 id=&#34;data---normalized---transformed---reconstructedfigurespca_data_original_normalized_transformed_reconstructedgif-la-data-inicial-normalizada-transformada-y-reconstruida&#34;&gt;















&lt;figure  id=&#34;figure-la-data-inicial-normalizada-transformada-y-reconstruida&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data -&amp;gt; normalized -&amp;gt; transformed -&amp;gt; reconstructed&#34;
           src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/PCA_data_original_normalized_transformed_reconstructed.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      La data inicial, normalizada, transformada y reconstruida
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;fundamento-matemático&#34;&gt;Fundamento Matemático&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Normalizamos \(\mathbf{X}\) a \(\mathbf{X}_0\) (restando la media, opcionalmente dividiendo por la desviación estándar).&lt;/li&gt;
&lt;li&gt;Calculamos \(\Sigma = \mathbf{X}_0^\top \mathbf{X}_0\) (matriz de covarianza).&lt;/li&gt;
&lt;li&gt;Hallamos &lt;strong&gt;vectores propios&lt;/strong&gt; \(u_k\) y valores propios \(\lambda_k\) de \(\Sigma\).&lt;/li&gt;
&lt;li&gt;Elegimos los \(L\) vectores con mayores \(\lambda_k\) para capturar, p. ej., 99% de la varianza.&lt;/li&gt;
&lt;li&gt;Proyectamos los datos en esos vectores: se obtiene la &lt;strong&gt;representación&lt;/strong&gt; de \(\mathbf{X}_0\) en dimensión \(L\).&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;PCA&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pasos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Centrar&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datos&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Calcular&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;covarianza&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Eigen&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decomposition&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Elegir&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;top&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;L&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;componentes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Proyectar&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Reconstruir&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;ejemplos-visuales&#34;&gt;Ejemplos Visuales&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo con 32 imágenes y Reconstrucción con 4 componentes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ejemplo-de-datos-originales-y-reconstrucción-con-solo-4-componentes-principales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;32 imágenes antes de la reducción de dimensión (PCA)&#34;
           src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/pca_raw.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ejemplo de datos originales, y reconstrucción con solo 4 componentes principales.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;En imágenes de caras (eigenfaces):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-los-vectores-base-se-llaman-eigenfaces-en-este-caso&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de eigenfaces (PCA en imágenes)&#34; srcset=&#34;
               /minerias/10_reduccion_atributos/figures/eigenfaces_hu13679287234197417025.webp 400w,
               /minerias/10_reduccion_atributos/figures/eigenfaces_hu17278100951785769264.webp 760w,
               /minerias/10_reduccion_atributos/figures/eigenfaces_hu2553934778585337579.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/10_reduccion_atributos/figures/eigenfaces_hu13679287234197417025.webp&#34;
               width=&#34;760&#34;
               height=&#34;475&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los vectores base se llaman eigenfaces en este caso.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;otros-algoritmos&#34;&gt;Otros Algoritmos&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multidimensional Scaling (MDS)&lt;/strong&gt;: Preserva distancias entre puntos en la proyección.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;t-SNE&lt;/strong&gt;: Modela la cercanía de puntos en alta dimensión a una proyección 2D:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NEaUSP4YerM&#34; title=&#34;Video sobre t-SNE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video explicativo t-SNE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICA (Independent Component Analysis)&lt;/strong&gt;: Busca componentes estadísticamente independientes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UMAP&lt;/strong&gt;: Se basa en geometría riemanniana y topología algebraica:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nq6iPZVUxZU&#34; title=&#34;Video sobre UMAP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Más detalles en video UMAP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Autoencoder&lt;/strong&gt;: Red neuronal no supervisada para comprimir (encoder) y reconstruir (decoder), usando la capa latente como reducción de dimensión.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusiones&#34;&gt;Conclusiones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Selección de atributos&lt;/strong&gt; (filtrada o basada en un modelo) reduce complejidad y puede mejorar rendimiento, sobre todo en métodos sensibles a ruido y correlaciones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducción de dimensionalidad&lt;/strong&gt; (PCA, t-SNE, UMAP, etc.) proyecta los datos a un espacio de menor dimensión, lo que facilita la visualización, disminuye ruido y puede acelerar el entrenamiento.&lt;/li&gt;
&lt;li&gt;Hay que equilibrar la &lt;strong&gt;simplicidad&lt;/strong&gt; lograda y la &lt;strong&gt;pérdida de información&lt;/strong&gt; al desechar o proyectar atributos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Transfer Learning</title>
      <link>http://localhost:1313/deep/10_transferlearning/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/10_transferlearning/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslides10_transferlearningpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/10_TransferLearning.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>http://localhost:1313/deep/11_computervision/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/11_computervision/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslides11_computervisionpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/11_ComputerVision.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Redes Neuronales</title>
      <link>http://localhost:1313/minerias/11_nn/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/11_nn/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esdm_nnetpdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_NNet.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;perceptrón&#34;&gt;Perceptrón&lt;/h2&gt;
&lt;p&gt;Las &lt;strong&gt;Redes Neuronales&lt;/strong&gt; se inspiran ligeramente en la biología de las neuronas, tratando de simular cómo la señal fluye entre capas de neuronas. El &lt;strong&gt;Perceptrón&lt;/strong&gt; es la unidad básica que originó muchos avances en este campo.&lt;/p&gt;
&lt;h3 id=&#34;por-qué-este-nombre&#34;&gt;¿Por qué este nombre?&lt;/h3&gt;
&lt;p&gt;En biología, una neurona:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recibe&lt;/strong&gt; neurotransmisores por las dendritas, provenientes de sinapsis de otras neuronas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Se activa&lt;/strong&gt; al superar un cierto umbral de estimulación.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emite&lt;/strong&gt; a su vez señal eléctrica por el axón, y libera neurotransmisores en sus sinapsis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-las-dendritas-reciben-información-si-supera-un-umbral-la-neurona-dispara-por-el-axón&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Esquema de neurona biológica&#34; srcset=&#34;
               /minerias/11_nn/figures/neurone_bio_hu5042147682634696871.webp 400w,
               /minerias/11_nn/figures/neurone_bio_hu8780734665714482128.webp 760w,
               /minerias/11_nn/figures/neurone_bio_hu11577556031772479973.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/neurone_bio_hu5042147682634696871.webp&#34;
               width=&#34;760&#34;
               height=&#34;560&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Las dendritas reciben información; si supera un umbral, la neurona dispara por el axón.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En un &lt;strong&gt;perceptrón&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Las entradas (análogo de dendritas) se combinan con ciertos pesos (\(w_i\)) y pasan por una función de activación que decide si hay salida (dispara) o no.&lt;/li&gt;
&lt;li&gt;Si supera un &lt;strong&gt;umbral&lt;/strong&gt;, la “neurona” artificial se activa.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;perceptrón-presentación&#34;&gt;Perceptrón: Presentación&lt;/h3&gt;
&lt;p&gt;El perceptrón más simple (con dos entradas \(x_1\) y \(x_2\)) se representa como:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Un &lt;strong&gt;suma ponderada&lt;/strong&gt; \(\sum_j x_j w_j - b\)&lt;/li&gt;
&lt;li&gt;Una &lt;strong&gt;función de activación&lt;/strong&gt; &lt;code&gt;f&lt;/code&gt; que decide la salida \(y\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;En la versión binaria más elemental:&lt;/p&gt;
\[
y = f\bigl(x_1 w_1 + x_2 w_2 - b\bigr) = f\bigl(W^TX- b\bigr)
\]&lt;p&gt;Donde \(f\) es (en teoría) la función &lt;strong&gt;Heaviside&lt;/strong&gt;:&lt;/p&gt;
\[
f(z) = 
\begin{cases}
1, &amp; z \ge 0\\
0, &amp; z &lt; 0
\end{cases}
\]&lt;p&gt;Si \(x_1 w_1 + x_2 w_2 \) supera el umbral \(b\), la salida es \(1\).&lt;/p&gt;
&lt;p&gt;Algunas representaciones gráficas:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-perceptron-con-la-capa-de-entrada-x-la-funcion-de-activacion-f-la-salida-y&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Perceptrón con umbral binario&#34;
           src=&#34;http://localhost:1313/minerias/11_nn/figures/Binary_valued_threshold.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El perceptron, con la capa de entrada \(X\), la funcion de activacion \(f\), la salida \(y\)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;perceptrón-multicapa-mlp&#34;&gt;Perceptrón Multicapa (MLP)&lt;/h2&gt;
&lt;p&gt;Para resolver tareas más complejas, se agregan &lt;strong&gt;capas&lt;/strong&gt; de neuronas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Capa de entrada&lt;/strong&gt;: Recibe las características (\(x\) o \(\mathbf{a}^{(1)}\)).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capas ocultas&lt;/strong&gt;: Transforman la información de forma intermedia.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capa de salida&lt;/strong&gt;: Emite la predicción final (clase, valor, etc.).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ejemplo-de-un-mlp&#34;&gt;Ejemplo de un MLP&lt;/h3&gt;
&lt;p&gt;La señal va propagándose hacia adelante, capa por capa (“feed-forward”):
















&lt;figure  id=&#34;figure-propagación-en-un-mlp-con-2-capas-de-neuronas-intermedias&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;MLP de 2 capas ocultas&#34;
           src=&#34;http://localhost:1313/minerias/11_nn/figures/MLP2_L.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Propagación en un MLP, con 2 capas de neuronas intermedias
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ecuaciones-y-composición&#34;&gt;Ecuaciones y Composición&lt;/h3&gt;
&lt;h4 id=&#34;principio-de-una-capa&#34;&gt;Principio de una capa&lt;/h4&gt;
&lt;p&gt;Si la capa \(\ell\) tiene \(n_\ell\) neuronas, y recibimos como entrada \(\mathbf{a}^{(\ell)}\), la salida \(\mathbf{a}^{(\ell+1)}\) se obtiene aplicando:&lt;/p&gt;
\[
\mathbf{a}^{(\ell+1)} 
= f\bigl( \mathbf{W}^{(\ell,\ell+1)} \; \mathbf{a}^{(\ell)} \bigr)
\]&lt;p&gt;donde \(f\) es la función de activación elemento a elemento, y \(\mathbf{W}^{(\ell,\ell+1)}\) es la matriz de pesos entre la capa \(\ell\) y la capa \(\ell+1\).&lt;/p&gt;
&lt;p&gt;Por ejemplo, para una sola neurona \(j\) en la capa \(\ell+1\):&lt;/p&gt;
\[
a_j^{(\ell+1)} 
= f\Bigl(\sum_i w_{ji}^{(\ell,\ell+1)} \; a_i^{(\ell)}\Bigr)
\]&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ejemplo de una capa con 4 neuronas de salida&#34; srcset=&#34;
               /minerias/11_nn/figures/MLP2_L0_hu9982466221484515949.webp 400w,
               /minerias/11_nn/figures/MLP2_L0_hu13782756864887087107.webp 760w,
               /minerias/11_nn/figures/MLP2_L0_hu17769283940668246020.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/MLP2_L0_hu9982466221484515949.webp&#34;
               width=&#34;544&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;mlp-composición-de-capas&#34;&gt;MLP: composición de capas&lt;/h4&gt;
&lt;p&gt;Un MLP con \(L\) capas puede verse como una &lt;strong&gt;composición de funciones&lt;/strong&gt;:&lt;/p&gt;
\[
\mathbf{a}^{(2)} = h^{(1,2)}(\mathbf{a}^{(1)}), 
\quad
\mathbf{a}^{(3)} = h^{(2,3)}(\mathbf{a}^{(2)}) 
\quad \ldots \quad
\mathbf{a}^{(L)} = h^{(L-1,L)}(\mathbf{a}^{(L-1)}).
\]&lt;p&gt;Por tanto:&lt;/p&gt;
\[
\mathbf{a}^{(L)} 
= h^{(L-1,L)} \circ \cdots \circ h^{(1,2)}\;(\mathbf{a}^{(1)}).
\]&lt;p&gt;Esta \(\mathbf{a}^{(L)}\) es la &lt;strong&gt;salida final&lt;/strong&gt; del MLP.&lt;/p&gt;
&lt;p&gt;Se puede escribir una funcion del MLP:&lt;/p&gt;
\[
MLP = h^{(L-1, L)} \circ h^{(L-2, L-1)} \circ ... \circ h^{(1,2)} \text{ \textbf{tal que} } MLP(\mathbf{a}^{(1)}) = \mathbf{a}^{(L)}
\]&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Conclusión&lt;/strong&gt;: un MLP es una &lt;strong&gt;función no lineal&lt;/strong&gt; compuesta. A mayor número de capas (profundidad), mayor &lt;strong&gt;capacidad&lt;/strong&gt; de representar funciones complejas.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;funciones-de-activación&#34;&gt;Funciones de Activación&lt;/h2&gt;
&lt;p&gt;La función de activación en cada neurona &lt;strong&gt;rompe la linealidad&lt;/strong&gt;. Sin ella, el modelo sería solo una combinación lineal de los datos. Algunas funciones típicas:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;Nombre&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Gráfico&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Ecuación&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Valores en&lt;/strong&gt; \(\pm\infty\)&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Valores derivada en&lt;/strong&gt; \(\pm\infty\)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Heaviside&lt;/strong&gt; (teórica)&lt;/td&gt;
          &lt;td&gt;[No derivable]&lt;/td&gt;
          &lt;td&gt;\(f(z)=\mathbf{1}_{z\ge0}\)&lt;/td&gt;
          &lt;td&gt;0,1&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sigmoide&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;sigmoide&#34; srcset=&#34;
               /minerias/11_nn/figures/sigmoide_hu2258526441697663153.webp 400w,
               /minerias/11_nn/figures/sigmoide_hu11249631941563051249.webp 760w,
               /minerias/11_nn/figures/sigmoide_hu6025699800413662374.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/sigmoide_hu2258526441697663153.webp&#34;
               width=&#34;250&#34;
               height=&#34;106&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;\(f(z)=\frac{1}{1+e^{-z}}\)&lt;/td&gt;
          &lt;td&gt;0; 1&lt;/td&gt;
          &lt;td&gt;0; 0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Tanh&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;tanh&#34; srcset=&#34;
               /minerias/11_nn/figures/tanh_hu2161236630147589151.webp 400w,
               /minerias/11_nn/figures/tanh_hu13683484027632460599.webp 760w,
               /minerias/11_nn/figures/tanh_hu7537390151399161997.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/tanh_hu2161236630147589151.webp&#34;
               width=&#34;250&#34;
               height=&#34;106&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;\(f(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\)&lt;/td&gt;
          &lt;td&gt;-1; 1&lt;/td&gt;
          &lt;td&gt;0; 0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;ReLU&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;relu&#34; srcset=&#34;
               /minerias/11_nn/figures/relu_hu14682383464111917821.webp 400w,
               /minerias/11_nn/figures/relu_hu247199606108359105.webp 760w,
               /minerias/11_nn/figures/relu_hu1501887440186754615.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/relu_hu14682383464111917821.webp&#34;
               width=&#34;250&#34;
               height=&#34;106&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;\(f(z)=\max(0,z)\)&lt;/td&gt;
          &lt;td&gt;0; \(z\)&lt;/td&gt;
          &lt;td&gt;0; 1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;ELU&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;elu&#34; srcset=&#34;
               /minerias/11_nn/figures/elu_hu3293995043550659080.webp 400w,
               /minerias/11_nn/figures/elu_hu16152205053403592329.webp 760w,
               /minerias/11_nn/figures/elu_hu15040008162705597673.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/elu_hu3293995043550659080.webp&#34;
               width=&#34;250&#34;
               height=&#34;100&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
          &lt;td&gt;\(f(z)=\begin{cases}\alpha(e^z-1)&amp;z&lt;0\\ z&amp;z\ge0\end{cases}\)&lt;/td&gt;
          &lt;td&gt;\(-\alpha\); \(z\)&lt;/td&gt;
          &lt;td&gt;0; 1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Heaviside&lt;/strong&gt; (original) no es derivable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sigmoide / Tanh&lt;/strong&gt;: buenas para salidas en [0,1] o [-1,1], pero pueden saturarse.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ReLU&lt;/strong&gt;: muy popular en redes profundas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ELU&lt;/strong&gt;: variante suavizada por debajo de 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recuerdos-de-optimización-y-entrenamiento&#34;&gt;Recuerdos de Optimización y Entrenamiento&lt;/h2&gt;
&lt;p&gt;Para entrenar la red, definimos una &lt;strong&gt;función de costo&lt;/strong&gt; (\(\ell\)), por ejemplo el error entre salidas reales y predichas. Queremos &lt;strong&gt;minimizar&lt;/strong&gt; esa función respecto de los &lt;strong&gt;pesos&lt;/strong&gt; de la red.&lt;/p&gt;
&lt;h3 id=&#34;descenso-del-gradiente&#34;&gt;Descenso del Gradiente&lt;/h3&gt;
&lt;p&gt;Si \(\theta\) representa todos los pesos:&lt;/p&gt;
\[
\theta \leftarrow \theta - \alpha \; \nabla_\theta \ell(\theta)
\]&lt;p&gt;donde \(\alpha\) es la &lt;strong&gt;tasa de aprendizaje&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-visualización-conceptual&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Convergencia en un valle del “paisaje de costos”&#34; srcset=&#34;
               /minerias/11_nn/figures/LossAlps_hu15806339177112344745.webp 400w,
               /minerias/11_nn/figures/LossAlps_hu9473298813042643160.webp 760w,
               /minerias/11_nn/figures/LossAlps_hu7762387643821809596.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/LossAlps_hu15806339177112344745.webp&#34;
               width=&#34;760&#34;
               height=&#34;498&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visualización conceptual
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Se pueden usar mini-batches (stochastic gradient descent - SGD) para computar gradientes parciales y hacer actualizaciones más frecuentes.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;backpropagation-el-núcleo-del-entrenamiento&#34;&gt;Backpropagation: el núcleo del entrenamiento&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt; (o retropropagación del error) es el algoritmo que:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Calcula&lt;/strong&gt; la salida de la red (forward pass).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evalúa&lt;/strong&gt; el error (función de costo).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propaga&lt;/strong&gt; gradientes hacia atrás (desde la salida a cada capa) usando la &lt;strong&gt;regla de la cadena&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actualiza&lt;/strong&gt; cada peso según su gradiente parcial.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Explicación más detallada:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tras un &lt;strong&gt;forward pass&lt;/strong&gt;, conocemos la salida \(\mathbf{a}^{(L)}\).&lt;/li&gt;
&lt;li&gt;Comparamos con la etiqueta real y obtenemos el &lt;strong&gt;error&lt;/strong&gt; \(\ell\).&lt;/li&gt;
&lt;li&gt;Para cada capa \(\ell\), la &lt;strong&gt;derivada&lt;/strong&gt; de \(\ell\) respecto a \(\mathbf{W}^{(\ell,\ell+1)}\) se computa recursivamente, empezando por la capa de salida y yendo hacia la capa de entrada.&lt;/li&gt;
&lt;li&gt;Esto es posible gracias a que
\[
  \frac{\partial \ell}{\partial \mathbf{W}^{(\ell,\ell+1)}} 
  = \frac{\partial \ell}{\partial \mathbf{a}^{(\ell+1)}} 
  \cdot \frac{\partial \mathbf{a}^{(\ell+1)}}{\partial \mathbf{z}^{(\ell+1)}} 
  \cdot \frac{\partial \mathbf{z}^{(\ell+1)}}{\partial \mathbf{W}^{(\ell,\ell+1)}}.
  \]&lt;/li&gt;
&lt;li&gt;Se multiplican derivadas (regla de la cadena) y se obtiene la dirección de ajuste de cada peso.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-el-error-fluye-hacia-atrás-para-ajustar-cada-capa&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Representación esquemática de backpropagation (imagen ilustrativa)&#34; srcset=&#34;
               /minerias/11_nn/figures/backprop_hu5437070840985716372.webp 400w,
               /minerias/11_nn/figures/backprop_hu525999881374688638.webp 760w,
               /minerias/11_nn/figures/backprop_hu15532578210586162933.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/backprop_hu5437070840985716372.webp&#34;
               width=&#34;760&#34;
               height=&#34;543&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      El error fluye hacia atrás para ajustar cada capa
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En la práctica, librerías como &lt;strong&gt;PyTorch&lt;/strong&gt; o &lt;strong&gt;TensorFlow&lt;/strong&gt; hacen esto automáticamente mediante &lt;strong&gt;autograd&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representaciones&#34;&gt;Representaciones&lt;/h2&gt;
&lt;p&gt;En un MLP, cada capa oculta \(\mathbf{a}^{(\ell)}\) forma una &lt;strong&gt;representación&lt;/strong&gt; distinta de los datos. Al componer varias capas, se construyen representaciones de nivel creciente de abstracción.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-cada-capa-genera-un-nuevo-vector-de-características-activaciones&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;MLP con 2 capas ocultas y salidas intermedias (activaciones)&#34; srcset=&#34;
               /minerias/11_nn/figures/MLP2_L6_hu2654510542182663165.webp 400w,
               /minerias/11_nn/figures/MLP2_L6_hu9619913887787023670.webp 760w,
               /minerias/11_nn/figures/MLP2_L6_hu6647341152618120042.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/MLP2_L6_hu2654510542182663165.webp&#34;
               width=&#34;760&#34;
               height=&#34;594&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cada capa genera un nuevo vector de características (activaciones)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En imágenes: capas iniciales detectan contornos, capas intermedias detectan partes (nariz, ojo&amp;hellip;), capas finales reconocen objetos concretos.&lt;/li&gt;
&lt;li&gt;En texto: capas iniciales identifican gramática, capas intermedias reconocen entidades, capas profundas comprenden significados complejos (sentimientos, intenciones…).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;composicionalidad-de-las-representaciones&#34;&gt;Composicionalidad de las representaciones&lt;/h3&gt;
&lt;p&gt;En CNN u otras redes, se pueden visualizar “filtros” que activan sobre patrones (bordes, texturas, etc.).&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-se-compone-con-los-filtros-mas-basicos-los-filtros-mas-complejos-una-cara-se-compone-de-2-ojos-una-nariz-y-una-boca&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Visualización de filtros (composición de características)&#34; srcset=&#34;
               /minerias/11_nn/figures/filters_visualization_hu17234472207453702754.webp 400w,
               /minerias/11_nn/figures/filters_visualization_hu9084227478072464886.webp 760w,
               /minerias/11_nn/figures/filters_visualization_hu7053406489742120465.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/filters_visualization_hu17234472207453702754.webp&#34;
               width=&#34;679&#34;
               height=&#34;672&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Se compone con los filtros mas basicos los filtros mas complejos: una cara se compone de 2 ojos una nariz y una boca.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;visibilidad-de-filtros&#34;&gt;Visibilidad de filtros&lt;/h3&gt;
&lt;p&gt;Una video muy interesante de la &lt;a href=&#34;https://www.youtube.com/watch?v=AgkfIQ4IGaM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Visualization Toolbox&lt;/a&gt; para ver en tiempo real lo que activa los diferentes neuronas, las imagenes del dataset que les activan lo mas, y imagenes artificiales optimizada para activar los filtros.&lt;/p&gt;
&lt;p&gt;La visualizacion de las imagenes que activan el mas los filtros de un AlexNet en &lt;a href=&#34;https://distill.pub/2017/feature-visualization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distill.pub: Feature Visualization&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-los-primeros-filtros-son-lo-mas-simple-como-detecor-de-edge-y-se-van-complejando-tipo-texturas-y-despues-objetos&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Visualización de filtros (composición de características)&#34; srcset=&#34;
               /minerias/11_nn/figures/feature_vis_hu12627367647364061408.webp 400w,
               /minerias/11_nn/figures/feature_vis_hu3957077363820538824.webp 760w,
               /minerias/11_nn/figures/feature_vis_hu7687384663625210479.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/feature_vis_hu12627367647364061408.webp&#34;
               width=&#34;760&#34;
               height=&#34;284&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Los primeros filtros son lo mas simple como detecor de edge, y se van complejando tipo texturas y despues objetos.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representation-learning-for-transfer&#34;&gt;Representation Learning for Transfer&lt;/h2&gt;
&lt;h3 id=&#34;aprendizaje-de-representaciones&#34;&gt;Aprendizaje de Representaciones&lt;/h3&gt;
&lt;p&gt;El &lt;strong&gt;aprendizaje de representaciones&lt;/strong&gt; (Representation Learning) se basa en que la red neuronal aprende, de forma automática, &lt;strong&gt;características&lt;/strong&gt; o &lt;strong&gt;features&lt;/strong&gt; útiles directamente desde los datos brutos (imágenes, texto, etc.). Estas capas ocultas (o &lt;em&gt;embeddings&lt;/em&gt;) pueden:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generalizar&lt;/strong&gt; mejor que las características diseñadas a mano (hand-crafted features).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adaptarse&lt;/strong&gt; a distintas tareas si se transfieren.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Los intereses son varios, por ejemplo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cada capa proporciona una representación de los datos de entrada con dimensiones más bajas.&lt;/li&gt;
&lt;li&gt;Esas representaciones provenientes únicamente de los datos brutos a veces superan conjuntos de descriptores clásicos \(\rightarrow\) &lt;a href=&#34;https://arxiv.org/pdf/1609.08675&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube-8M&lt;/a&gt;: Representaciones con LogReg superaban a todos los clasificadores de vanguardia en muchas tareas.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-las-representaciones-en-altas-dimensiones-se-proyectan-a-2d-y-muestran-agrupaciones-naturales&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;t-SNE de representaciones de AlexNet&#34; srcset=&#34;
               /minerias/11_nn/figures/tsne_hu6012296096494109504.webp 400w,
               /minerias/11_nn/figures/tsne_hu6407126937998179609.webp 760w,
               /minerias/11_nn/figures/tsne_hu17633236819676664136.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/tsne_hu6012296096494109504.webp&#34;
               width=&#34;760&#34;
               height=&#34;283&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Las representaciones en altas dimensiones se proyectan a 2D y muestran agrupaciones naturales.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Ejemplo: después de entrenar en ImageNet, las capas profundas “entienden” rasgos básicos de las imágenes. Si queremos clasificar perros vs. gatos, basta con re-entrenar poco.)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;transferencia&#34;&gt;Transferencia&lt;/h3&gt;
&lt;p&gt;Una red entrenada en una tarea (p. ej. reconocimiento de objetos en imágenes) puede servir como base para otras tareas (p. ej. detectar nuevos tipos de objetos). Se aprovecha el &lt;strong&gt;transfer learning&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Entrenamos una red (p. ej. CNN) en una gran base (ImageNet).&lt;/li&gt;
&lt;li&gt;Se “toman” sus capas iniciales como &lt;strong&gt;extractor de características&lt;/strong&gt; (al estar entrenadas en millones de imágenes, captan contornos y patrones generales).&lt;/li&gt;
&lt;li&gt;Para una nueva tarea con pocos datos, se conectan la capa final (llamado head) nueva o se re-entrena ligeramente (fine-tuning).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;De este modo, la &lt;strong&gt;representación&lt;/strong&gt; (las activaciones intermedias) es &lt;strong&gt;reutilizada&lt;/strong&gt;. Esto ahorra tiempo y datos:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;El MLP o CNN grande provee features genéricas.&lt;/li&gt;
&lt;li&gt;Sólo ajustamos parcialmente la red (o las últimas capas) a la tarea específica.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frameworks-populares&#34;&gt;Frameworks Populares&lt;/h2&gt;
&lt;h3 id=&#34;keras&#34;&gt;Keras&lt;/h3&gt;
&lt;p&gt;Biblioteca de Python de alto nivel para Redes Neuronales, sobre TensorFlow u otros backends.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Keras logo&#34; srcset=&#34;
               /minerias/11_nn/figures/keras_hu1035629169656843260.webp 400w,
               /minerias/11_nn/figures/keras_hu8780139607811972134.webp 760w,
               /minerias/11_nn/figures/keras_hu15459690173681857519.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/keras_hu1035629169656843260.webp&#34;
               width=&#34;760&#34;
               height=&#34;220&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ejemplo CNN preentrenada (VGG16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tutorial RNN-LSTM seq2seq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Embeddings de palabras preentrenados&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pytorch&#34;&gt;PyTorch&lt;/h3&gt;
&lt;p&gt;Biblioteca de Python centrada en el cálculo automático de gradientes y en el aprendizaje profundo.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;PyTorch logo&#34; srcset=&#34;
               /minerias/11_nn/figures/pytorch_hu8893412317164389539.webp 400w,
               /minerias/11_nn/figures/pytorch_hu16231463881976280037.webp 760w,
               /minerias/11_nn/figures/pytorch_hu894001013880019535.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/pytorch_hu8893412317164389539.webp&#34;
               width=&#34;512&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tutorial Transfer Learning con ResNet18&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech recognition con Wav2Vec2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RNN-GRU seq2seq para traducción&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;huggingface-transformers&#34;&gt;HuggingFace Transformers&lt;/h3&gt;
&lt;p&gt;Biblioteca en Python para modelos tipo &lt;strong&gt;Transformers&lt;/strong&gt; (BERT, GPT, etc.).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;HuggingFace logo&#34; srcset=&#34;
               /minerias/11_nn/figures/hf_logo_hu4463973729321469342.webp 400w,
               /minerias/11_nn/figures/hf_logo_hu16750015608575083964.webp 760w,
               /minerias/11_nn/figures/hf_logo_hu855115890972270235.webp 1200w&#34;
               src=&#34;http://localhost:1313/minerias/11_nn/figures/hf_logo_hu4463973729321469342.webp&#34;
               width=&#34;760&#34;
               height=&#34;175&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Soporta modelos &lt;strong&gt;preentrenados&lt;/strong&gt; en texto, visión, audio&amp;hellip;&lt;/li&gt;
&lt;li&gt;Otras librerías: &lt;code&gt;Diffusers&lt;/code&gt; (imágenes generativas), &lt;code&gt;Datasets&lt;/code&gt;, &lt;code&gt;Accelerate&lt;/code&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;see-you-in-the-classroom&#34;&gt;See you in the classroom!&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Intro a la NLP</title>
      <link>http://localhost:1313/minerias/12_nlp/</link>
      <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/minerias/12_nlp/</guid>
      <description>&lt;p&gt;This is a class from an invited speaker, &lt;a href=&#34;https://cl.linkedin.com/in/juanjo-alegria&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juan Jose Alegria&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc5205-mineria-datos-contentrawrefsheadsmainslides_esc12_introduccion_nlppdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/c12_introduccio%cc%81n_nlp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Generative LLMs</title>
      <link>http://localhost:1313/deep/n_generative_llms/</link>
      <pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/n_generative_llms/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslidesn_generative_llmspdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/N_Generative_LLMs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Large Multimodal Models</title>
      <link>http://localhost:1313/deep/n_multimodal_models/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/deep/n_multimodal_models/</guid>
      <description>&lt;h2 id=&#34;the-slides-are-available-herehttpsgithubcomvalbarrierecc6204-deep-learningrawrefsheadsmainslidesn_multimodal_modelspdf&#34;&gt;The slides are available &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/raw/refs/heads/main/Slides/N_Multimodal_Models.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>http://localhost:1313/teaching/deep/</link>
      <pubDate>Sat, 24 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/deep/</guid>
      <description>&lt;h3 id=&#34;all-the-different-classes-can-be-found-heredeep-index&#34;&gt;All the different classes can be found &lt;a href=&#34;../../deep-index&#34;&gt;here&lt;/a&gt;!&lt;/h3&gt;
&lt;p&gt;This is the CC66204 course from the Universidad de Chile. Based on the class of &lt;a href=&#34;https://github.com/ivansipiran&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ivan Sipiran&lt;/a&gt;, I added details in each of the classes, allowing to understand on how we get to the recent large multimodal models. The github is &lt;a href=&#34;https://github.com/valbarriere/CC6204-Deep-Learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;br&gt;
Here&amp;rsquo;s a summary:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;General introduction&lt;/strong&gt;: Overview of the class, reminders from Machine Learning,&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Basics&lt;/strong&gt;: Perceptron, Vanilla Gradient Descent, MLP, Backprop,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Losses and Activations&lt;/strong&gt;: General losses, Softmax, CE, Activation functions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Initialization and Optimization&lt;/strong&gt;: Weights initialization, Complex gradient descents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization&lt;/strong&gt;: Penalization, Dropout, Data augmentation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Convolutional Layer&lt;/strong&gt;: Convolution, Padding, Pooling, LeNet&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision Architectures&lt;/strong&gt;: ImageNet, Revolution of depth, Classical classifiers architectures&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transfer Learning&lt;/strong&gt;: Motivation, Principle, Types of TL, Weights unfreezing, Pre-training datasets, SoTA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object Detection&lt;/strong&gt;: Principle, IoU and mAP, Classical Object Detection, Segmentation and Mask-RCNN, SoTA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Recurrent Layer&lt;/strong&gt;: Sequential Modeling, RNN, LSTM, GRU,&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Attention&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[TODO] Transformers&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative Large Language Models&lt;/strong&gt;: Language Modeling and Temperature, Abilities and In-Context-Learning, Tokenization, Instructions, Alignments, Reasonings, Training and Evaluating in Practice, LLMs as Agents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large Multimodal Models&lt;/strong&gt;: Multimodality, Fusion, Original tasks and datasets, Early multimodal transformers, CLIP and text2image Diffusion, Frozen encoders, BLIP 1/2/3 and LMM Assistants, Open-source training datasets, LMM evaluation, Video, Multimodal Tokenization&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Apprentissage Machine</title>
      <link>http://localhost:1313/teaching/fouilledonnees/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/fouilledonnees/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
