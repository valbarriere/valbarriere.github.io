---
title: Clasificadores
summary: KNN, Naive Bayes, Decision Tree, Boosting, Bagging, Random Forests
date: 2024-03-23
type: docs
math: true
tags:
  - Machine Learning
  - Course
  - Spanish
---

## The slides are available [here](https://github.com/valbarriere/CC5205-Mineria-Datos-Content/raw/refs/heads/main/slides_es/DM_Modelos_SL.pdf)!

---

## K-Vecinos M치s Cercanos (K-NN)

### Descripci칩n General

K-Vecinos M치s Cercanos (K-NN) es un algoritmo de aprendizaje basado en instancias que clasifica un nuevo punto seg칰n la mayor칤a de etiquetas entre sus K ejemplos de entrenamiento m치s cercanos en el espacio de caracter칤sticas. No construye un modelo expl칤cito.

![Ejemplo de K-NN](figures/knn_exemple.png "Ejemplo de K-NN que ilustra los l칤mites de decisi칩n y vecinos")

### Regla de Clasificaci칩n

Dado un punto \( \mathbf{X} \), el clasificador K-NN realiza lo siguiente:

- Sea \( S = \{(\mathbf{X}_i, y_i)\}_{i=1}^n \) el conjunto de entrenamiento
- Definir una m칠trica de distancia \( d(\cdot, \cdot) \)
- Ordenar los puntos de entrenamiento por distancia a \( \mathbf{X} \)
- Tomar los primeros \( K \) elementos: \( S^K_\mathbf{X} = \{\mathbf{X}_{\sigma(1)}, \dots, \mathbf{X}_{\sigma(K)}\} \)
- Contar la ocurrencia de cada etiqueta en este subconjunto

\[
h_{KNN}(\mathbf{X}) = \arg\max_y \frac{N^K_y(\mathbf{X})}{K}
\]

Donde \( N^K_y(\mathbf{X}) \) es el n칰mero de vecinos con etiqueta \( y \).

![Algoritmo KNN](figures/knn_algo_schema.png "K-NN: Esquema de los pasos del algoritmo")

### Elecci칩n de K

El hiperpar치metro \( K \) controla el equilibrio sesgo-varianza:

- **\( K \) peque침o** (ej. 1 o 3): Bajo sesgo, alta varianza; muy sensible al ruido.
- **\( K \) grande** (ej. 20+): Alto sesgo, baja varianza; suaviza la frontera de decisi칩n.

![Efecto de K](figures/K-NN.png "Efecto de K en los l칤mites de decisi칩n")

---

### M칠tricas de Distancia

La elecci칩n de la m칠trica de distancia puede afectar significativamente el rendimiento de K-NN. M칠tricas comunes incluyen:

\[
d(x, y) = \left( \sum_i |x_i - y_i|^p \right)^{1/p}
\]

- **Distancia Manhattan**: \( p=1 \)
- **Distancia Euclidiana**: \( p=2 \)
- **Distancia Chebyshev**: \( p = \infty \)
- **Distancia Mahalanobis**: Tiene en cuenta la covarianza de las caracter칤sticas:
  \[
  d(x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}
  \]

**Importante**: Normaliza tus caracter칤sticas. De lo contrario, las caracter칤sticas con rangos num칠ricos grandes pueden dominar el c치lculo de distancia.

M치s info: [M칠tricas de distancia en Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html)

---

### Escalado y Normalizaci칩n de Caracter칤sticas

Antes de usar K-NN, aplica un preprocesamiento adecuado:

- **Estandarizaci칩n**: Distribuci칩n gaussiana con media cero y varianza unitaria
  \[
  x' = \frac{x - \mu_x}{\sigma_x}
  \]

- **Escalado Min-Max**: Escalar a \([0,1]\)
  \[
  x' = \frac{x - \min(x)}{\max(x) - \min(x)}
  \]

- **Normalizaci칩n L2**: Vector unitario (se usa cuando la orientaci칩n es m치s importante que la magnitud)

M치s sobre preprocesamiento: [documentaci칩n de scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html)

---

### Fortalezas y Limitaciones

- **Ventajas**:
  - Simple e intuitivo
  - No requiere entrenamiento (aprendizaje perezoso)
  - Se adapta a fronteras no lineales

- **Limitaciones**:
  - Costoso computacionalmente en el momento de prueba
  - Afectado por la **maldici칩n de la dimensionalidad**
  - Sensible a caracter칤sticas irrelevantes

---

## Naive Bayes

### Regla de Bayes

El teorema de Bayes relaciona la probabilidad posterior de una clase con su probabilidad previa y su verosimilitud:

\[
P(c|o) = \frac{P(o|c) \cdot P(c)}{P(o)}
\]

Donde:

- \( P(c) \): probabilidad previa de la clase \( c \)
- \( P(o|c) \): verosimilitud de la observaci칩n dada la clase
- \( P(o) \): evidencia (puede ignorarse para clasificaci칩n)
- \( P(c|o) \): probabilidad posterior

### Suposici칩n Naive

Naive Bayes asume que las caracter칤sticas son **condicionalmente independientes** dada la clase:

\[
P(o|c) = \prod_{i=1}^d P(x_i | c)
\]

Por lo tanto:

\[
\hat{c} = \arg\max_c P(c) \cdot \prod_{i=1}^d P(x_i | c)
\]

Esta suposici칩n rara vez se cumple en datos reales, pero el algoritmo a menudo funciona bien a pesar de ello.

---

### Estimaci칩n de Par치metros

Para caracter칤sticas categ칩ricas:

\[
P(x_i = v | c) = \frac{\text{count}(x_i = v, c)}{\text{count}(c)}
\]

#### Suavizado de Laplace

Si una palabra/caracter칤stica no se observa en la clase \( c \), entonces \( P(x_i | c) = 0 \), lo que anula el producto. **El suavizado de Laplace** corrige esto:

\[
P(x_i = v | c) = \frac{\text{count}(x_i = v, c) + \alpha}{\text{count}(c) + \alpha \cdot n}
\]

- \( \alpha \): par치metro de suavizado (usualmente 1)
- \( n \): n칰mero de posibles valores de \( x_i \)

---

### Caracter칤sticas Continuas: Naive Bayes Gaussiano

Si las caracter칤sticas son continuas, se asume que siguen una distribuci칩n gaussiana por clase:

\[
P(x_i | y) = \frac{1}{\sqrt{2\pi \sigma^2_y}} \exp\left( -\frac{(x_i - \mu_y)^2}{2\sigma^2_y} \right)
\]

Estimar \( \mu_y \) y \( \sigma_y \) a partir de los datos de entrenamiento usando MLE.

---

### Pros y Contras

- **Pros**:
  - Entrenamiento muy r치pido
  - Robusto a caracter칤sticas irrelevantes
  - Funciona bien con datos dispersos y de alta dimensi칩n (ej. clasificaci칩n de texto)

- **Contras**:
  - Suposici칩n fuerte de independencia rara vez se cumple
  - Las estimaciones de probabilidad no son bien calibradas

---

## 츼rboles de Decisi칩n

### Descripci칩n General

Los 츼rboles de Decisi칩n dividen el espacio de caracter칤sticas usando hiperplanos alineados con los ejes para particionar los datos en regiones homog칠neas seg칰n la etiqueta de clase.

![Divisi칩n alineada a los ejes](figures/DT_hp.png "Divisiones seg칰n los ejes de caracter칤sticas")

Cada nodo interno divide por una caracter칤stica. Cada hoja representa una clase.

![Estructura del 츼rbol](figures/DT_arbre.png "츼rbol de decisi칩n con nodos y etiquetas")

- Usan divisiones del tipo \( x^j = \tau \)
- Representan el proceso de decisi칩n en un 치rbol binario
- Cada nodo corresponde a una divisi칩n

\[
h_{tree}(\mathbf{X}) = \sum_{l=1}^{L} \alpha_l \mathbb{I}_{\mathcal{C}_l}(\mathbf{X})
\]

Donde cada \( \mathcal{C}_l \) es una regi칩n definida por un conjunto de divisiones, y \( \alpha_l \) es la etiqueta para esa regi칩n.

---

### Construcci칩n del 츼rbol

En cada nodo, el 치rbol:

1. Selecciona una caracter칤stica \( x^j \) y un umbral \( \tau \)
2. Divide los datos en:
   - Izquierda: \( \{x \in \mathcal{S} \mid x^j \leq \tau\} \)
   - Derecha: \( \{x \in \mathcal{S} \mid x^j > \tau\} \)

Se elige la mejor divisi칩n minimizando la **impureza**.

---

### Medidas de Impureza

Dado un subconjunto \( \mathcal{S} \), se definen proporciones de clase \( p_c \). Criterios comunes:

- **Entrop칤a**:
  \[
  H(\mathcal{S}) = -\sum_c p_c \log p_c
  \]

- **칈ndice Gini**:
  \[
  H(\mathcal{S}) = \sum_c p_c(1 - p_c)
  \]

- **Error de Clasificaci칩n**:
  \[
  H(\mathcal{S}) = 1 - \max_c p_c
  \]

![Medidas de impureza](figures/impurity.png "Comparaci칩n de medidas de impureza")

---

### Ganancia de Informaci칩n

Elegir la caracter칤stica \( j \) y umbral \( \tau \) que minimicen:

\[
L(t_{j,\tau}, \mathcal{S}) = \frac{|\mathcal{D}|}{n} H(\mathcal{D}) + \frac{|\mathcal{I}|}{n} H(\mathcal{I})
\]

Donde \( \mathcal{D} \) e \( \mathcal{I} \) son las divisiones izquierda y derecha.

La **Ganancia de Informaci칩n** es la reducci칩n de entrop칤a tras la divisi칩n.

![Ganancia de Informaci칩n](figures/tree_features.png "Impacto de la divisi칩n en la ganancia de informaci칩n")

---

### Criterios de Parada

Un 치rbol deja de crecer cuando:

- Se alcanza la profundidad m치xima
- Las hojas tienen menos de un n칰mero m칤nimo de muestras
- Todas las muestras en un nodo pertenecen a la misma clase

Cuidado: permitir crecimiento ilimitado lleva a sobreajuste.

---

## M칠todos de Ensamble 

### Bagging (Bootstrap Aggregation)

Bagging es un m칠todo de ensamble donde:

- Se entrenan m칰ltiples modelos en diferentes muestras bootstrap
- Sus salidas se agregan (voto mayoritario o promedio)

\[
f_{ens}(\mathbf{X}) = \frac{1}{T} \sum_{t=1}^T f_t(\mathbf{X})
\]

Reduce la **varianza**, haciendo el modelo m치s estable.

![Bagging](figures/bagging.png "Ilustraci칩n del procedimiento de Bagging")

Ecuaci칩n del MSE:

\[
MSE(f_{ens}) = \frac{1}{T^2} \mathbb{E}\left[ \left( \sum_t \epsilon_t(x) \right)^2 \right]
\]

Si \( \epsilon_t \) son no correlacionados, la varianza se reduce.

---

### Boosting

Boosting entrena modelos secuencialmente, corrigiendo los errores del anterior:

1. Inicializar pesos iguales sobre los datos  
2. Entrenar modelo \( h_t \)  
3. Aumentar pesos en ejemplos **mal clasificados**  
4. Agregar predicciones con votos ponderados

Algoritmos comunes: AdaBoost, Gradient Boosting

![Boosting](figures/boosting.png "Boosting enfoc치ndose en ejemplos dif칤ciles")

---

### Ejemplo de Boosting

Se entrenan clasificadores binarios y se agregan:

\[
F_n(x) = \text{sign}(H_{n-1}(x) + h_n(x))
\]

Cada aprendiz se enfoca m치s en puntos mal clasificados por el anterior.

![Animaci칩n Boosting](figures/boosting.gif "Ejemplo de iteraciones de Adaboost")

游댕 [M치s sobre Boosting](https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/)  
游댕 [Adaboost de la clase de Jiri Matas](https://www.robots.ox.ac.uk/~az/lectures/cv/adaboost_matas.pdf)

---

## Bosques Aleatorios (Random Forest)

### Motivaci칩n

Los 치rboles de decisi칩n tienen **alta varianza**. Un peque침o cambio en los datos puede generar 치rboles muy diferentes.

**Random Forests** reducen la varianza promediando m칰ltiples 치rboles entrenados en subconjuntos distintos de datos y caracter칤sticas.

---

### Algoritmo

1. Para cada 치rbol \( t \in 1..T \):
   - Tomar una muestra bootstrap \( \mathcal{S}^{(t)} \)
   - Elegir aleatoriamente \( k \ll F \) caracter칤sticas en cada nodo
   - Entrenar un 치rbol \( h^{(t)} \)
2. Predicci칩n final:
   \[
   h^{(T)}(x) = \frac{1}{T} \sum_t h^{(t)}(x)
   \]

![Diagrama Random Forest](figures/RF_val.png "Agregaci칩n de 치rboles en un Random Forest")

---

### Fronteras de Decisi칩n

![KNN vs RF](figures/RF_KNN.png "Comparaci칩n entre las fronteras de decisi칩n de K-NN y Random Forest")

---

### Regresi칩n con Random Forest

Para regresi칩n, se reemplaza la impureza por reducci칩n de varianza:

\[
L(t_{j, \tau}, \mathcal{S}) = VAR(\mathcal{S}) - \frac{|\mathcal{D}|}{n} VAR(\mathcal{D}) - \frac{|\mathcal{I}|}{n} VAR(\mathcal{I})
\]

Donde:

\[
VAR(\mathcal{S}) = \frac{1}{|\mathcal{S}|} \sum_{(x^i, y^i) \in \mathcal{S}} (y^i - \bar{y})^2
\]

---

## Tablas Resumen

### Fortalezas y Debilidades

![Comparaci칩n de clasificadores](figures/table_pros_cons.png "Ventajas y desventajas de los principales clasificadores")

- K-NN: F치cil de usar, pero lento y sensible a caracter칤sticas irrelevantes  
- Naive Bayes: R치pido, funciona bien con texto  
- 츼rboles de Decisi칩n: Interpretables, propensos a sobreajuste  
- Random Forests: Potentes y estables, pero menos interpretables  

游댕 [Resumen de algoritmos de SAP ML](https://cai.tools.sap/blog/machine-learning-algorithms/2/)  
游댕 [Hoja de c치lculo comparativa](https://docs.google.com/spreadsheets/d/1BETYWHJwA65TjrJ-mGwQ8lSpgk3V1cq3GOYRwbm1WDU/edit?usp=sharing)

---

### Gu칤a de Selecci칩n de Modelos

Puedes consultar el flujo de decisi칩n de scikit-learn:

![Mapa de Scikit-learn](../1_intro/figures/ml_map.png "Gu칤a r치pida de selecci칩n de modelos de Scikit-learn")

---

# See you in the classroom!

